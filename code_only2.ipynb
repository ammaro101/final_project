{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22168f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Project prototype (implementation)\n",
    "## Install Dependencies and import libraries\n",
    "\n",
    "# pip install pandas numpy yfinance pandas-ta scikit-learn tensorflow\n",
    "\n",
    "# https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "# import yfinance, our data source\n",
    "import yfinance as yf\n",
    "\n",
    "# https://pypi.org/project/pandas-ta/ (\"\"\"An easy to use Python 3 Pandas Extension with 130+ Technical Analysis Indicators. Can be called from a Pandas DataFrame or standalone\"\"\")\n",
    "# import pandas-ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import from tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Input, GRU\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fbc43",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cf16548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from directory\n"
     ]
    }
   ],
   "source": [
    "# insert the stock symbols into a list\n",
    "symbols_list = ['PFE', 'ROP', 'XYL', 'CPAY', 'INCY']\n",
    "\n",
    "# define a function to load the data from source (yfinance API), and save it as a csv to local storage\n",
    "def loadData(symbols=symbols_list, period='10y', interval='1wk'):\n",
    "    \n",
    "    try:\n",
    "        # load the the dataframe from the csv file if it already exist\n",
    "        df = pd.read_csv('stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "        \n",
    "        print(\"Data loaded from directory\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        # print a message stating the data does not already exists and need to be downloaded from yfinance\n",
    "        print(\"There is no stocks_data.csv. Data will be downloaded from yfinance.\")\n",
    "        \n",
    "        # download the data from source and store it in the stock_data variable which will hold the data as a pandas dataframe\n",
    "        stocks_data =  yf.download(symbols, period=period, interval=interval)\n",
    "\n",
    "        # reshape the dataframe as a multi-level index dataframe\n",
    "        stocks_data = stocks_data.stack()\n",
    "\n",
    "        # source: https://www.statology.org/pandas-change-column-names-to-lowercase/\n",
    "        # convert column names to lowercase\n",
    "        stocks_data.columns = stocks_data.columns.str.lower()\n",
    "\n",
    "        # save the dataframe to a csv file (Save the data to a CSV so we don't have to make any extra unnecessary requests to the API every time we reload the notebook)\n",
    "        stocks_data.to_csv('stocks_data.csv', index=True)\n",
    "\n",
    "        # load the the dataframe from the csv file\n",
    "        df = pd.read_csv('stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "\n",
    "    finally: \n",
    "        # create a dict to store the dataframe of each unique symbol where keys are symbol, values are dataframes\n",
    "        df_dict = {}\n",
    "\n",
    "        # iterate over the symbols\n",
    "        for symbol in symbols:\n",
    "\n",
    "            # source of inspiration https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html [11]\n",
    "            # extract the specific stock data at the 'Ticker' level of this multi index dataframe and save it as a dataframe\n",
    "            symbol_df = df.xs(symbol, axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "            # store the datafram into the df_dict\n",
    "            df_dict[symbol] = symbol_df\n",
    "\n",
    "        # return the dictionary\n",
    "        return df_dict\n",
    "\n",
    "\n",
    "dfs = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "910aa98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08-04</th>\n",
       "      <td>18.252020</td>\n",
       "      <td>26.888046</td>\n",
       "      <td>26.982922</td>\n",
       "      <td>26.442125</td>\n",
       "      <td>26.850096</td>\n",
       "      <td>88542113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-11</th>\n",
       "      <td>18.445242</td>\n",
       "      <td>27.172676</td>\n",
       "      <td>27.419355</td>\n",
       "      <td>26.593927</td>\n",
       "      <td>27.068312</td>\n",
       "      <td>107166188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-18</th>\n",
       "      <td>18.625566</td>\n",
       "      <td>27.438330</td>\n",
       "      <td>27.542694</td>\n",
       "      <td>27.220114</td>\n",
       "      <td>27.277040</td>\n",
       "      <td>102843207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-25</th>\n",
       "      <td>18.928270</td>\n",
       "      <td>27.884251</td>\n",
       "      <td>28.149904</td>\n",
       "      <td>27.428843</td>\n",
       "      <td>27.447819</td>\n",
       "      <td>99802628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>19.095715</td>\n",
       "      <td>28.130930</td>\n",
       "      <td>28.140417</td>\n",
       "      <td>27.666035</td>\n",
       "      <td>27.713472</td>\n",
       "      <td>86197384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>28.517534</td>\n",
       "      <td>28.920000</td>\n",
       "      <td>29.230000</td>\n",
       "      <td>27.299999</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>168155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-15</th>\n",
       "      <td>29.552921</td>\n",
       "      <td>29.969999</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>29.030001</td>\n",
       "      <td>180142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-22</th>\n",
       "      <td>30.341789</td>\n",
       "      <td>30.770000</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>29.309999</td>\n",
       "      <td>30.110001</td>\n",
       "      <td>179544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>30.430000</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>31.540001</td>\n",
       "      <td>29.780001</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>254667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-05</th>\n",
       "      <td>29.605000</td>\n",
       "      <td>29.605000</td>\n",
       "      <td>30.049999</td>\n",
       "      <td>28.850000</td>\n",
       "      <td>29.090000</td>\n",
       "      <td>52348852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj close      close       high        low       open     volume\n",
       "Date                                                                        \n",
       "2014-08-04  18.252020  26.888046  26.982922  26.442125  26.850096   88542113\n",
       "2014-08-11  18.445242  27.172676  27.419355  26.593927  27.068312  107166188\n",
       "2014-08-18  18.625566  27.438330  27.542694  27.220114  27.277040  102843207\n",
       "2014-08-25  18.928270  27.884251  28.149904  27.428843  27.447819   99802628\n",
       "2014-09-01  19.095715  28.130930  28.140417  27.666035  27.713472   86197384\n",
       "...               ...        ...        ...        ...        ...        ...\n",
       "2024-07-08  28.517534  28.920000  29.230000  27.299999  28.049999  168155400\n",
       "2024-07-15  29.552921  29.969999  30.690001  28.830000  29.030001  180142400\n",
       "2024-07-22  30.341789  30.770000  30.930000  29.309999  30.110001  179544200\n",
       "2024-07-29  30.430000  30.430000  31.540001  29.780001  30.690001  254667700\n",
       "2024-08-05  29.605000  29.605000  30.049999  28.850000  29.090000   52348852\n",
       "\n",
       "[523 rows x 6 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[symbols_list[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08027413",
   "metadata": {},
   "source": [
    "# Add Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0a5a8d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>next_close</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08-04</th>\n",
       "      <td>18.252020</td>\n",
       "      <td>26.888046</td>\n",
       "      <td>26.982922</td>\n",
       "      <td>26.442125</td>\n",
       "      <td>26.850096</td>\n",
       "      <td>88542113</td>\n",
       "      <td>27.172676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-11</th>\n",
       "      <td>18.445242</td>\n",
       "      <td>27.172676</td>\n",
       "      <td>27.419355</td>\n",
       "      <td>26.593927</td>\n",
       "      <td>27.068312</td>\n",
       "      <td>107166188</td>\n",
       "      <td>27.438330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-18</th>\n",
       "      <td>18.625566</td>\n",
       "      <td>27.438330</td>\n",
       "      <td>27.542694</td>\n",
       "      <td>27.220114</td>\n",
       "      <td>27.277040</td>\n",
       "      <td>102843207</td>\n",
       "      <td>27.884251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-25</th>\n",
       "      <td>18.928270</td>\n",
       "      <td>27.884251</td>\n",
       "      <td>28.149904</td>\n",
       "      <td>27.428843</td>\n",
       "      <td>27.447819</td>\n",
       "      <td>99802628</td>\n",
       "      <td>28.130930</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>19.095715</td>\n",
       "      <td>28.130930</td>\n",
       "      <td>28.140417</td>\n",
       "      <td>27.666035</td>\n",
       "      <td>27.713472</td>\n",
       "      <td>86197384</td>\n",
       "      <td>27.922201</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>27.659641</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>28.629999</td>\n",
       "      <td>27.620001</td>\n",
       "      <td>27.950001</td>\n",
       "      <td>80647600</td>\n",
       "      <td>28.920000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>28.517534</td>\n",
       "      <td>28.920000</td>\n",
       "      <td>29.230000</td>\n",
       "      <td>27.299999</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>168155400</td>\n",
       "      <td>29.969999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-15</th>\n",
       "      <td>29.552921</td>\n",
       "      <td>29.969999</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>29.030001</td>\n",
       "      <td>180142400</td>\n",
       "      <td>30.770000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-22</th>\n",
       "      <td>30.341789</td>\n",
       "      <td>30.770000</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>29.309999</td>\n",
       "      <td>30.110001</td>\n",
       "      <td>179544200</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>30.430000</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>31.540001</td>\n",
       "      <td>29.780001</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>254667700</td>\n",
       "      <td>29.605000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj close      close       high        low       open     volume  \\\n",
       "Date                                                                           \n",
       "2014-08-04  18.252020  26.888046  26.982922  26.442125  26.850096   88542113   \n",
       "2014-08-11  18.445242  27.172676  27.419355  26.593927  27.068312  107166188   \n",
       "2014-08-18  18.625566  27.438330  27.542694  27.220114  27.277040  102843207   \n",
       "2014-08-25  18.928270  27.884251  28.149904  27.428843  27.447819   99802628   \n",
       "2014-09-01  19.095715  28.130930  28.140417  27.666035  27.713472   86197384   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "2024-07-01  27.659641  28.049999  28.629999  27.620001  27.950001   80647600   \n",
       "2024-07-08  28.517534  28.920000  29.230000  27.299999  28.049999  168155400   \n",
       "2024-07-15  29.552921  29.969999  30.690001  28.830000  29.030001  180142400   \n",
       "2024-07-22  30.341789  30.770000  30.930000  29.309999  30.110001  179544200   \n",
       "2024-07-29  30.430000  30.430000  31.540001  29.780001  30.690001  254667700   \n",
       "\n",
       "            next_close  trend  \n",
       "Date                           \n",
       "2014-08-04   27.172676      1  \n",
       "2014-08-11   27.438330      1  \n",
       "2014-08-18   27.884251      1  \n",
       "2014-08-25   28.130930      1  \n",
       "2014-09-01   27.922201      0  \n",
       "...                ...    ...  \n",
       "2024-07-01   28.920000      1  \n",
       "2024-07-08   29.969999      1  \n",
       "2024-07-15   30.770000      1  \n",
       "2024-07-22   30.430000      0  \n",
       "2024-07-29   29.605000      0  \n",
       "\n",
       "[518 rows x 8 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function that takes a dataframe and create 'next_close' column based on its 'close' column\n",
    "def get_next_close(_df):\n",
    "    \n",
    "    # create the 'next_close' column to be equal to the next closing price\n",
    "    # this can be accomplished easily by shifting the close column backward by 1\n",
    "    return _df['close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['next_close'] > row['close']:\n",
    "        return 1\n",
    "    elif row['next_close'] < row['close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "# create a function that add the target columns to the dataframe\n",
    "def add_targets(_df):\n",
    "    \n",
    "    # add the next_close column to the dataframe\n",
    "    _df['next_close'] = get_next_close(_df)\n",
    "    \n",
    "    # add the trend column to the dataframe\n",
    "    _df['trend'] = _df.apply(assign_trend, axis=1)\n",
    "    \n",
    "    # drop the NaN values\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    # fix the 'trend' data type to be int\n",
    "    _df = _df.astype({'trend': int})\n",
    "    \n",
    "    return _df\n",
    "\n",
    "df = add_targets(dfs[symbols_list[0]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6eb2d",
   "metadata": {},
   "source": [
    "# Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ac283a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas TA - Technical Analysis Indicators - v0.3.14b0\n",
      "Total Indicators & Utilities: 205\n",
      "Abbreviations:\n",
      "    aberration, above, above_value, accbands, ad, adosc, adx, alma, amat, ao, aobv, apo, aroon, atr, bbands, below, below_value, bias, bop, brar, cci, cdl_pattern, cdl_z, cfo, cg, chop, cksp, cmf, cmo, coppock, cross, cross_value, cti, decay, decreasing, dema, dm, donchian, dpo, ebsw, efi, ema, entropy, eom, er, eri, fisher, fwma, ha, hilo, hl2, hlc3, hma, hwc, hwma, ichimoku, increasing, inertia, jma, kama, kc, kdj, kst, kurtosis, kvo, linreg, log_return, long_run, macd, mad, massi, mcgd, median, mfi, midpoint, midprice, mom, natr, nvi, obv, ohlc4, pdist, percent_return, pgo, ppo, psar, psl, pvi, pvo, pvol, pvr, pvt, pwma, qqe, qstick, quantile, rma, roc, rsi, rsx, rvgi, rvi, short_run, sinwma, skew, slope, sma, smi, squeeze, squeeze_pro, ssf, stc, stdev, stoch, stochrsi, supertrend, swma, t3, td_seq, tema, thermo, tos_stdevall, trima, trix, true_range, tsi, tsignals, ttm_trend, ui, uo, variance, vhf, vidya, vortex, vp, vwap, vwma, wcp, willr, wma, xsignals, zlma, zscore\n",
      "\n",
      "Candle Patterns:\n",
      "    2crows, 3blackcrows, 3inside, 3linestrike, 3outside, 3starsinsouth, 3whitesoldiers, abandonedbaby, advanceblock, belthold, breakaway, closingmarubozu, concealbabyswall, counterattack, darkcloudcover, doji, dojistar, dragonflydoji, engulfing, eveningdojistar, eveningstar, gapsidesidewhite, gravestonedoji, hammer, hangingman, harami, haramicross, highwave, hikkake, hikkakemod, homingpigeon, identical3crows, inneck, inside, invertedhammer, kicking, kickingbylength, ladderbottom, longleggeddoji, longline, marubozu, matchinglow, mathold, morningdojistar, morningstar, onneck, piercing, rickshawman, risefall3methods, separatinglines, shootingstar, shortline, spinningtop, stalledpattern, sticksandwich, takuri, tasukigap, thrusting, tristar, unique3river, upsidegap2crows, xsidegap3methods\n",
      "Help on NoneType object:\n",
      "\n",
      "class NoneType(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      True if self else False\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  we can easily check the available indicators in the pandas-ta library\n",
    "help(df.ta.indicators())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9401d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function donchian in module pandas_ta.volatility.donchian:\n",
      "\n",
      "donchian(high, low, lower_length=None, upper_length=None, offset=None, **kwargs)\n",
      "    Donchian Channels (DC)\n",
      "    \n",
      "    Donchian Channels are used to measure volatility, similar to\n",
      "    Bollinger Bands and Keltner Channels.\n",
      "    \n",
      "    Sources:\n",
      "        https://www.tradingview.com/wiki/Donchian_Channels_(DC)\n",
      "    \n",
      "    Calculation:\n",
      "        Default Inputs:\n",
      "            lower_length=upper_length=20\n",
      "        LOWER = low.rolling(lower_length).min()\n",
      "        UPPER = high.rolling(upper_length).max()\n",
      "        MID = 0.5 * (LOWER + UPPER)\n",
      "    \n",
      "    Args:\n",
      "        high (pd.Series): Series of 'high's\n",
      "        low (pd.Series): Series of 'low's\n",
      "        lower_length (int): The short period. Default: 20\n",
      "        upper_length (int): The short period. Default: 20\n",
      "        offset (int): How many periods to offset the result. Default: 0\n",
      "    \n",
      "    Kwargs:\n",
      "        fillna (value, optional): pd.DataFrame.fillna(value)\n",
      "        fill_method (value, optional): Type of fill method\n",
      "    \n",
      "    Returns:\n",
      "        pd.DataFrame: lower, mid, upper columns.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCL_20_20</th>\n",
       "      <th>DCM_20_20</th>\n",
       "      <th>DCU_20_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.465000</td>\n",
       "      <td>29.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.465000</td>\n",
       "      <td>29.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-15</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.945001</td>\n",
       "      <td>30.690001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-22</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>28.065001</td>\n",
       "      <td>30.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>28.370001</td>\n",
       "      <td>31.540001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DCL_20_20  DCM_20_20  DCU_20_20\n",
       "Date                                       \n",
       "2014-08-04        NaN        NaN        NaN\n",
       "2014-08-11        NaN        NaN        NaN\n",
       "2014-08-18        NaN        NaN        NaN\n",
       "2014-08-25        NaN        NaN        NaN\n",
       "2014-09-01        NaN        NaN        NaN\n",
       "...               ...        ...        ...\n",
       "2024-07-01  25.200001  27.465000  29.730000\n",
       "2024-07-08  25.200001  27.465000  29.730000\n",
       "2024-07-15  25.200001  27.945001  30.690001\n",
       "2024-07-22  25.200001  28.065001  30.930000\n",
       "2024-07-29  25.200001  28.370001  31.540001\n",
       "\n",
       "[518 rows x 3 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(ta.donchian)\n",
    "df.ta.donchian()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70aa6f",
   "metadata": {},
   "source": [
    "65 different technical indicators columns were added in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "da6d64f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_histogram</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>rsi</th>\n",
       "      <th>...</th>\n",
       "      <th>dm_positive</th>\n",
       "      <th>dm_negative</th>\n",
       "      <th>donchian_lower</th>\n",
       "      <th>donchian_mid</th>\n",
       "      <th>donchian_upper</th>\n",
       "      <th>ebsw</th>\n",
       "      <th>efi</th>\n",
       "      <th>entropy</th>\n",
       "      <th>next_close</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-05-04</th>\n",
       "      <td>22.294880</td>\n",
       "      <td>32.277039</td>\n",
       "      <td>32.741936</td>\n",
       "      <td>31.508539</td>\n",
       "      <td>32.362431</td>\n",
       "      <td>120949030</td>\n",
       "      <td>1.094356</td>\n",
       "      <td>-0.154644</td>\n",
       "      <td>1.249000</td>\n",
       "      <td>56.919457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220596</td>\n",
       "      <td>0.173414</td>\n",
       "      <td>29.193548</td>\n",
       "      <td>31.451613</td>\n",
       "      <td>33.709679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.860276e+06</td>\n",
       "      <td>3.348804</td>\n",
       "      <td>32.248577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-11</th>\n",
       "      <td>22.459530</td>\n",
       "      <td>32.248577</td>\n",
       "      <td>32.504745</td>\n",
       "      <td>31.764706</td>\n",
       "      <td>32.343452</td>\n",
       "      <td>108459973</td>\n",
       "      <td>0.996643</td>\n",
       "      <td>-0.201886</td>\n",
       "      <td>1.198529</td>\n",
       "      <td>56.667510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203982</td>\n",
       "      <td>0.160354</td>\n",
       "      <td>29.421251</td>\n",
       "      <td>31.565465</td>\n",
       "      <td>33.709679</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>-5.464084e+06</td>\n",
       "      <td>3.341200</td>\n",
       "      <td>32.523720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-18</th>\n",
       "      <td>22.651154</td>\n",
       "      <td>32.523720</td>\n",
       "      <td>32.722961</td>\n",
       "      <td>32.106262</td>\n",
       "      <td>32.239090</td>\n",
       "      <td>96837832</td>\n",
       "      <td>0.930679</td>\n",
       "      <td>-0.214280</td>\n",
       "      <td>1.144959</td>\n",
       "      <td>58.576403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205050</td>\n",
       "      <td>0.148324</td>\n",
       "      <td>29.421251</td>\n",
       "      <td>31.565465</td>\n",
       "      <td>33.709679</td>\n",
       "      <td>0.796640</td>\n",
       "      <td>-8.771833e+05</td>\n",
       "      <td>3.336386</td>\n",
       "      <td>32.969639</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-25</th>\n",
       "      <td>22.961714</td>\n",
       "      <td>32.969639</td>\n",
       "      <td>33.197342</td>\n",
       "      <td>32.286530</td>\n",
       "      <td>32.428844</td>\n",
       "      <td>112572997</td>\n",
       "      <td>0.903963</td>\n",
       "      <td>-0.192797</td>\n",
       "      <td>1.096760</td>\n",
       "      <td>61.533927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225183</td>\n",
       "      <td>0.137236</td>\n",
       "      <td>29.430740</td>\n",
       "      <td>31.570210</td>\n",
       "      <td>33.709679</td>\n",
       "      <td>0.983651</td>\n",
       "      <td>6.419335e+06</td>\n",
       "      <td>3.333379</td>\n",
       "      <td>32.343452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-01</th>\n",
       "      <td>22.525608</td>\n",
       "      <td>32.343452</td>\n",
       "      <td>33.130932</td>\n",
       "      <td>32.191650</td>\n",
       "      <td>32.988613</td>\n",
       "      <td>112931778</td>\n",
       "      <td>0.822778</td>\n",
       "      <td>-0.219185</td>\n",
       "      <td>1.041963</td>\n",
       "      <td>55.537418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208406</td>\n",
       "      <td>0.134080</td>\n",
       "      <td>29.430740</td>\n",
       "      <td>31.570210</td>\n",
       "      <td>33.709679</td>\n",
       "      <td>0.987158</td>\n",
       "      <td>-4.600047e+06</td>\n",
       "      <td>3.326444</td>\n",
       "      <td>32.457306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>27.659641</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>28.629999</td>\n",
       "      <td>27.620001</td>\n",
       "      <td>27.950001</td>\n",
       "      <td>80647600</td>\n",
       "      <td>-0.410333</td>\n",
       "      <td>0.294996</td>\n",
       "      <td>-0.705329</td>\n",
       "      <td>47.924023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295705</td>\n",
       "      <td>0.328908</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.465000</td>\n",
       "      <td>29.730000</td>\n",
       "      <td>-0.931133</td>\n",
       "      <td>1.495683e+07</td>\n",
       "      <td>3.372494</td>\n",
       "      <td>28.920000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>28.517534</td>\n",
       "      <td>28.920000</td>\n",
       "      <td>29.230000</td>\n",
       "      <td>27.299999</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>168155400</td>\n",
       "      <td>-0.297625</td>\n",
       "      <td>0.326164</td>\n",
       "      <td>-0.623788</td>\n",
       "      <td>53.304140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317441</td>\n",
       "      <td>0.305415</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.465000</td>\n",
       "      <td>29.730000</td>\n",
       "      <td>-0.144033</td>\n",
       "      <td>3.371948e+07</td>\n",
       "      <td>3.370019</td>\n",
       "      <td>29.969999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-15</th>\n",
       "      <td>29.552921</td>\n",
       "      <td>29.969999</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>29.030001</td>\n",
       "      <td>180142400</td>\n",
       "      <td>-0.122168</td>\n",
       "      <td>0.401296</td>\n",
       "      <td>-0.523464</td>\n",
       "      <td>58.832123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399052</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.945001</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>0.630386</td>\n",
       "      <td>5.592375e+07</td>\n",
       "      <td>3.372634</td>\n",
       "      <td>30.770000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-22</th>\n",
       "      <td>30.341789</td>\n",
       "      <td>30.770000</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>29.309999</td>\n",
       "      <td>30.110001</td>\n",
       "      <td>179544200</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.483178</td>\n",
       "      <td>-0.402670</td>\n",
       "      <td>62.476919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387691</td>\n",
       "      <td>0.263342</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>28.065001</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>0.888022</td>\n",
       "      <td>6.845401e+07</td>\n",
       "      <td>3.375619</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>30.430000</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>31.540001</td>\n",
       "      <td>29.780001</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>254667700</td>\n",
       "      <td>0.211260</td>\n",
       "      <td>0.491144</td>\n",
       "      <td>-0.279884</td>\n",
       "      <td>60.043839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403571</td>\n",
       "      <td>0.244532</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>28.370001</td>\n",
       "      <td>31.540001</td>\n",
       "      <td>0.986655</td>\n",
       "      <td>4.630529e+07</td>\n",
       "      <td>3.374534</td>\n",
       "      <td>29.605000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj close      close       high        low       open     volume  \\\n",
       "Date                                                                           \n",
       "2015-05-04  22.294880  32.277039  32.741936  31.508539  32.362431  120949030   \n",
       "2015-05-11  22.459530  32.248577  32.504745  31.764706  32.343452  108459973   \n",
       "2015-05-18  22.651154  32.523720  32.722961  32.106262  32.239090   96837832   \n",
       "2015-05-25  22.961714  32.969639  33.197342  32.286530  32.428844  112572997   \n",
       "2015-06-01  22.525608  32.343452  33.130932  32.191650  32.988613  112931778   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "2024-07-01  27.659641  28.049999  28.629999  27.620001  27.950001   80647600   \n",
       "2024-07-08  28.517534  28.920000  29.230000  27.299999  28.049999  168155400   \n",
       "2024-07-15  29.552921  29.969999  30.690001  28.830000  29.030001  180142400   \n",
       "2024-07-22  30.341789  30.770000  30.930000  29.309999  30.110001  179544200   \n",
       "2024-07-29  30.430000  30.430000  31.540001  29.780001  30.690001  254667700   \n",
       "\n",
       "                macd  macd_histogram  macd_signal        rsi  ...  \\\n",
       "Date                                                          ...   \n",
       "2015-05-04  1.094356       -0.154644     1.249000  56.919457  ...   \n",
       "2015-05-11  0.996643       -0.201886     1.198529  56.667510  ...   \n",
       "2015-05-18  0.930679       -0.214280     1.144959  58.576403  ...   \n",
       "2015-05-25  0.903963       -0.192797     1.096760  61.533927  ...   \n",
       "2015-06-01  0.822778       -0.219185     1.041963  55.537418  ...   \n",
       "...              ...             ...          ...        ...  ...   \n",
       "2024-07-01 -0.410333        0.294996    -0.705329  47.924023  ...   \n",
       "2024-07-08 -0.297625        0.326164    -0.623788  53.304140  ...   \n",
       "2024-07-15 -0.122168        0.401296    -0.523464  58.832123  ...   \n",
       "2024-07-22  0.080508        0.483178    -0.402670  62.476919  ...   \n",
       "2024-07-29  0.211260        0.491144    -0.279884  60.043839  ...   \n",
       "\n",
       "            dm_positive  dm_negative  donchian_lower  donchian_mid  \\\n",
       "Date                                                                 \n",
       "2015-05-04     0.220596     0.173414       29.193548     31.451613   \n",
       "2015-05-11     0.203982     0.160354       29.421251     31.565465   \n",
       "2015-05-18     0.205050     0.148324       29.421251     31.565465   \n",
       "2015-05-25     0.225183     0.137236       29.430740     31.570210   \n",
       "2015-06-01     0.208406     0.134080       29.430740     31.570210   \n",
       "...                 ...          ...             ...           ...   \n",
       "2024-07-01     0.295705     0.328908       25.200001     27.465000   \n",
       "2024-07-08     0.317441     0.305415       25.200001     27.465000   \n",
       "2024-07-15     0.399052     0.283600       25.200001     27.945001   \n",
       "2024-07-22     0.387691     0.263342       25.200001     28.065001   \n",
       "2024-07-29     0.403571     0.244532       25.200001     28.370001   \n",
       "\n",
       "            donchian_upper      ebsw           efi   entropy  next_close  \\\n",
       "Date                                                                       \n",
       "2015-05-04       33.709679  0.000000 -5.860276e+06  3.348804   32.248577   \n",
       "2015-05-11       33.709679  0.577350 -5.464084e+06  3.341200   32.523720   \n",
       "2015-05-18       33.709679  0.796640 -8.771833e+05  3.336386   32.969639   \n",
       "2015-05-25       33.709679  0.983651  6.419335e+06  3.333379   32.343452   \n",
       "2015-06-01       33.709679  0.987158 -4.600047e+06  3.326444   32.457306   \n",
       "...                    ...       ...           ...       ...         ...   \n",
       "2024-07-01       29.730000 -0.931133  1.495683e+07  3.372494   28.920000   \n",
       "2024-07-08       29.730000 -0.144033  3.371948e+07  3.370019   29.969999   \n",
       "2024-07-15       30.690001  0.630386  5.592375e+07  3.372634   30.770000   \n",
       "2024-07-22       30.930000  0.888022  6.845401e+07  3.375619   30.430000   \n",
       "2024-07-29       31.540001  0.986655  4.630529e+07  3.374534   29.605000   \n",
       "\n",
       "            trend  \n",
       "Date               \n",
       "2015-05-04      0  \n",
       "2015-05-11      1  \n",
       "2015-05-18      1  \n",
       "2015-05-25      0  \n",
       "2015-06-01      1  \n",
       "...           ...  \n",
       "2024-07-01      1  \n",
       "2024-07-08      1  \n",
       "2024-07-15      1  \n",
       "2024-07-22      0  \n",
       "2024-07-29      0  \n",
       "\n",
       "[479 rows x 74 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def add_technical_indicators(_df):\n",
    "    # apply macd on the close column in a df and add it to the dataframe    \n",
    "    macd = ta.macd(_df['close'])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, 'macd', macd.iloc[:,0])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(7, 'macd_histogram', macd.iloc[:,1])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(8, 'macd_signal', macd.iloc[:,2])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df['close'])\n",
    "    _df.insert(9, 'rsi', rsi)\n",
    "\n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df['close'])\n",
    "    _df.insert(10, 'sma', sma)\n",
    "\n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df['close'])\n",
    "    _df.insert(11, 'ema', ema)\n",
    "\n",
    "    ######## repeat the same proccess for all the technical indicators we want to include ##########\n",
    "    # aberration: A volatility indicator\n",
    "    aberration = ta.aberration(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(12, 'aberration_zg', aberration.iloc[:,0])\n",
    "    _df.insert(13, 'aberration_sg', aberration.iloc[:,1])\n",
    "    _df.insert(14, 'aberration_xg', aberration.iloc[:,2])\n",
    "    _df.insert(15, 'aberration_atr', aberration.iloc[:,3])\n",
    "    \n",
    "    # bbands: A popular volatility indicator by John Bollinger.\n",
    "    bbands = ta.bbands(_df['close'])\n",
    "    _df.insert(16, 'bbands_lower', bbands.iloc[:,0])\n",
    "    _df.insert(17, 'bbands_mid', bbands.iloc[:,1])\n",
    "    _df.insert(18, 'bbands_upper', bbands.iloc[:,2])\n",
    "    _df.insert(19, 'bbands_bandwidth', bbands.iloc[:,3])\n",
    "    _df.insert(20, 'bbands_percent', bbands.iloc[:,4])\n",
    "    \n",
    "    # adx:  Average Directional Movement is meant to quantify trend strength by measuring the amount of movement in a single direction.    \n",
    "    adx = ta.adx(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(21, 'adx_adx', adx.iloc[:,0])\n",
    "    _df.insert(22, 'adx_dmp', adx.iloc[:,1])\n",
    "    _df.insert(23, 'adx_dmn', adx.iloc[:,2])\n",
    "\n",
    "    # atr: Averge True Range is used to measure volatility, especially volatility caused by gaps or limit moves.\n",
    "    atr = ta.atr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(24, 'atr', atr)\n",
    "    \n",
    "    # stoch: The Stochastic Oscillator (STOCH) was developed by George Lane in the 1950's. He believed this indicator was a good way to measure momentum because changes in momentum precede changes in price.\n",
    "    stoch = ta.stoch(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(25, 'stoch_k', stoch.iloc[:,0])\n",
    "    _df.insert(26, 'stoch_d', stoch.iloc[:,1])\n",
    "    \n",
    "    # obv: On Balance Volume is a cumulative indicator to measure buying and selling pressure.\n",
    "    obv = ta.obv(_df['close'], _df['volume'])\n",
    "    _df.insert(27, 'obv', obv)\n",
    "    \n",
    "    # Supertrend: is an overlap indicator. It is used to help identify trend direction, setting stop loss, identify support and resistance, and/or generate buy & sell signals.\n",
    "    supertrend = ta.supertrend(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(28, 'supertrend_trend', supertrend.iloc[:,0])\n",
    "    _df.insert(29, 'supertrend_direction', supertrend.iloc[:,1])\n",
    "    \n",
    "    # dema: The Double Exponential Moving Average attempts to a smoother average with less lag than the normal Exponential Moving Average (EMA).\n",
    "    dema = ta.dema(_df['close'])\n",
    "    _df.insert(30, 'dema', dema)\n",
    "    \n",
    "    # tema: A less laggy Exponential Moving Average.\n",
    "    tema = ta.tema(_df['close'])\n",
    "    _df.insert(31, 'tema', tema)\n",
    "\n",
    "    # roc: Rate of Change is an indicator is also referred to as Momentum. It is a pure momentum oscillator that measures the percent change in price with the previous price 'n' (or length) periods ago.\n",
    "    roc = ta.roc(_df['close'])\n",
    "    _df.insert(32, 'roc', roc)\n",
    "    \n",
    "    # mom: Momentum is an indicator used to measure a security's speed (or strength) of movement.  Or simply the change in price.\n",
    "    mom = ta.mom(_df['close'])\n",
    "    _df.insert(33, 'mom', mom)\n",
    "    \n",
    "    # cci: Commodity Channel Index is a momentum oscillator used to primarily identify overbought and oversold levels relative to a mean.\n",
    "    cci = ta.cci(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(34, 'cci', cci)\n",
    "    \n",
    "    # aroon: attempts to identify if a security is trending and how strong.\n",
    "    aroon = ta.aroon(_df['high'], _df['low'])\n",
    "    _df.insert(35, 'aroon_up', aroon.iloc[:,0])\n",
    "    _df.insert(36, 'aroon_down', aroon.iloc[:,1])\n",
    "    _df.insert(37, 'aroon_osc', aroon.iloc[:,2])\n",
    "    \n",
    "    # natr: Normalized Average True Range attempt to normalize the average true range.\n",
    "    natr = ta.natr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(38, 'natr', natr)\n",
    "    \n",
    "    # William's Percent R is a momentum oscillator similar to the RSI that attempts to identify overbought and oversold conditions.\n",
    "    willr = ta.willr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(39, 'willr', willr)\n",
    "    \n",
    "    # vortex: Two oscillators that capture positive and negative trend movement.\n",
    "    vortex = ta.vortex(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(40, 'vortex_vip', vortex.iloc[:,0])\n",
    "    _df.insert(41, 'vortex_vim', vortex.iloc[:,1])\n",
    "        \n",
    "    # kama: Developed by Perry Kaufman, Kaufman's Adaptive Moving Average (KAMA) is a moving average designed to account for market noise or volatility. KAMA will closely follow prices when the price swings are relatively small and the noise is low. KAMA will adjust when the price swings widen and follow prices from a greater distance. This trend-following indicator can be used to identify the overall trend, time turning points and filter price movements.\n",
    "    kama = ta.kama(_df['close'])\n",
    "    _df.insert(42, 'kama', kama)\n",
    "                       \n",
    "    # trix: is a momentum oscillator to identify divergences.\n",
    "    trix = ta.trix(_df['close'])\n",
    "    _df.insert(43, 'trix', trix.iloc[:,0])\n",
    "    _df.insert(44, 'trixs', trix.iloc[:,1])\n",
    "                       \n",
    "    # hlc3: the average of high, low, and close prices\n",
    "    hlc3 = ta.hlc3(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(45, 'hlc3', hlc3)\n",
    "\n",
    "    # ohlc4: the average of open, high, low, and close prices\n",
    "    ohlc4 = ta.ohlc4(_df['open'], _df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(46, 'ohlc4', ohlc4)\n",
    "    \n",
    "    # hma: The Hull Exponential Moving Average attempts to reduce or remove lag in moving averages.\n",
    "    hma = ta.hma(_df['close'])\n",
    "    _df.insert(47, 'hma', hma)\n",
    "\n",
    "    # vwma: Volume Weighted Moving Average.\n",
    "    vwma = ta.vwma(_df['close'], _df['volume'])\n",
    "    _df.insert(48, 'vwma', vwma)\n",
    "    \n",
    "    # accbands: Acceleration Bands created by Price Headley plots upper and lower envelope bands around a simple moving average.\n",
    "    accbands = ta.accbands(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(49, 'accbands_lower', accbands.iloc[:,0])\n",
    "    _df.insert(50, 'accbands_mid', accbands.iloc[:,1])\n",
    "    _df.insert(51, 'accbands_upper', accbands.iloc[:,2])\n",
    "    \n",
    "    # adosc: Accumulation/Distribution Oscillator indicator utilizes Accumulation/Distribution and treats it similarily to MACD or APO.\n",
    "    adosc = ta.adosc(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(52, 'adosc', adosc)\n",
    "    \n",
    "    # alma: The ALMA moving average uses the curve of the Normal (Gauss) distribution, which can be shifted from 0 to 1. This allows regulating the smoothness and high sensitivity of the indicator. Sigma is another parameter that is responsible for the shape of the curve coefficients. This moving average reduces lag of the data in conjunction with smoothing to reduce noise.\n",
    "    alma = ta.alma(_df['close'])\n",
    "    _df.insert(53, 'alma', alma)\n",
    "    \n",
    "    # apo: The Absolute Price Oscillator is an indicator used to measure a security's momentum.  It is simply the difference of two Exponential Moving Averages (EMA) of two different periods. Note: APO and MACD lines are equivalent.\n",
    "    apo = ta.apo(_df['close'])\n",
    "    _df.insert(54, 'apo', apo)\n",
    "    \n",
    "    # cfo: The Forecast Oscillator calculates the percentage difference between the actualprice and the Time Series Forecast (the endpoint of a linear regression line).\n",
    "    cfo = ta.cfo(_df['close'])\n",
    "    _df.insert(55, 'cfo', cfo)\n",
    "    \n",
    "    # cg: The Center of Gravity Indicator by John Ehlers attempts to identify turning points while exhibiting zero lag and smoothing.\n",
    "    cg = ta.cg(_df['close'])\n",
    "    _df.insert(56, 'cg', cg)    \n",
    "    \n",
    "    # chop: The Choppiness Index was created by Australian commodity trader E.W. Dreiss and is designed to determine if the market is choppy (trading sideways) or not choppy (trading within a trend in either direction). Values closer to 100 implies the underlying is choppier whereas values closer to 0 implies the underlying is trending.\n",
    "    chop = ta.chop(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(57, 'chop', chop)\n",
    "    \n",
    "    # cmf: Chailin Money Flow measures the amount of money flow volume over a specific period in conjunction with Accumulation/Distribution.\n",
    "    cmf = ta.cmf(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(58, 'cmf', cmf)\n",
    "    \n",
    "    # cmo: Attempts to capture the momentum of an asset with overbought at 50 and oversold at -50.\n",
    "    cmo = ta.cmo(_df['close'])\n",
    "    _df.insert(59, 'cmo', cmo)\n",
    "    \n",
    "    # coppock: Coppock Curve (originally called the \"Trendex Model\") is a momentum indicator is designed for use on a monthly time scale.  Although designed for monthly use, a daily calculation over the same period can be made, converting the periods to 294-day and 231-day rate of changes, and a 210-day weighted moving average.\n",
    "    coppock = ta.coppock(_df['close'])\n",
    "    _df.insert(60, 'coppock', coppock)\n",
    "    \n",
    "    # cti: The Correlation Trend Indicator is an oscillator created by John Ehler in 2020. It assigns a value depending on how close prices in that range are to following a positively- or negatively-sloping straight line. Values range from -1 to 1. This is a wrapper for ta.linreg(close, r=True).\n",
    "    cti = ta.cti(_df['close'])\n",
    "    _df.insert(61, 'cti', cti)\n",
    "    \n",
    "    # decay: Creates a decay moving forward from prior signals like crosses. The default is \"linear\". Exponential is optional as \"exponential\" or \"exp\".\n",
    "    decay = ta.decay(_df['close'])\n",
    "    _df.insert(62, 'decay', decay)\n",
    "    \n",
    "    # decreasing: Returns True if the series is decreasing over a period, False otherwise. If the kwarg 'strict' is True, it returns True if it is continuously decreasing over the period. When using the kwarg 'asint', then it returns 1 for True or 0 for False.\n",
    "    decreasing = ta.decreasing(_df['close'])\n",
    "    _df.insert(63, 'decreasing', decreasing)\n",
    "    \n",
    "    # dm: The Directional Movement was developed by J. Welles Wilder in 1978 attempts to determine which direction the price of an asset is moving. It compares prior highs and lows to yield to two series +DM and -DM.\n",
    "    dm = ta.dm(_df['high'], _df['low'])\n",
    "    _df.insert(64, 'dm_positive', dm.iloc[:,0])\n",
    "    _df.insert(65, 'dm_negative', dm.iloc[:,1])\n",
    "\n",
    "    # donchian: Donchian Channels are used to measure volatility, similar to Bollinger Bands and Keltner Channels.\n",
    "    donchian = ta.donchian(_df['high'], _df['low'])\n",
    "    _df.insert(66, 'donchian_lower', donchian.iloc[:,0])\n",
    "    _df.insert(67, 'donchian_mid', donchian.iloc[:,1])\n",
    "    _df.insert(68, 'donchian_upper', donchian.iloc[:,2])\n",
    "    \n",
    "    # ebsw: This indicator measures market cycles and uses a low pass filter to remove noise. Its output is bound signal between -1 and 1 and the maximum length of a detected trend is limited by its length input.\n",
    "    ebsw = ta.ebsw(_df['close'])\n",
    "    _df.insert(69, 'ebsw', ebsw)\n",
    "    \n",
    "    # efi: Elder's Force Index measures the power behind a price movement using price and volume as well as potential reversals and price corrections.\n",
    "    efi = ta.efi(_df['close'], _df['volume'])\n",
    "    _df.insert(70, 'efi', efi)\n",
    "    \n",
    "    # entropy: Introduced by Claude Shannon in 1948, entropy measures the unpredictability of the data, or equivalently, of its average information. A die has higher entropy (p=1/6) versus a coin (p=1/2).\n",
    "    entropy = ta.entropy(_df['close'])\n",
    "    _df.insert(71, 'entropy', entropy)\n",
    "\n",
    "    #### we can add more technical indicators if we want using the same process ####\n",
    "    \n",
    "    # remove the NaN values and return the new dataframe\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "# call the function on the selected dataframe\n",
    "df = add_technical_indicators(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea32a55",
   "metadata": {},
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8c599f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((331, 6, 72), (142, 6, 72))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source of isnpiration: https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4 [13]\n",
    "# create a function to reshape X and y into sequences of x timesteps\n",
    "def create_seqs(features, target, timesteps):\n",
    "    # create 2 empty lists to store the newly shaped features and target lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate over the features\n",
    "    for i in range(len(features) - timesteps):\n",
    "        # create indexes of the start and end of each sequence\n",
    "        seq_s = i\n",
    "        seq_e = i + timesteps\n",
    "        \n",
    "        # the ith sequence will be a slice of the features between the indexes, create it and add it to X\n",
    "        xi = features[seq_s : seq_e]\n",
    "        X.append(xi)\n",
    "        \n",
    "        # do the same for the target and add it to y\n",
    "        yi = target[seq_e]\n",
    "        y.append(yi)\n",
    "    \n",
    "    # return the X and y as numpy arraies\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# create a function to convert a dataframe into training and test sets\n",
    "def create_train_test_sets(_df, target=\"classification\", timesteps=6):\n",
    "\n",
    "    # reset the index\n",
    "    _df.reset_index(inplace = True)\n",
    "    \n",
    "    # drop the Date column as it's not necessary for now\n",
    "    _df.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "    # the features list\n",
    "    X = _df.iloc[:, :-2]\n",
    "    \n",
    "    # the target \n",
    "    if (target == \"classification\"):\n",
    "        # trend is the target for classification\n",
    "        y = _df.iloc[:, -1]\n",
    "    else:\n",
    "        # next_close is the target for regression\n",
    "        y = _df.iloc[:, -2]\n",
    "\n",
    "    # create sequences\n",
    "    X_seq, y_seq = create_seqs(X, y, timesteps)\n",
    "    \n",
    "    # source of inspiration: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical [14]\n",
    "    # use to_categorical from tf to converts the target (trend) to binary class matrix\n",
    "    if (target == \"classification\"):\n",
    "        y_seq = to_categorical(y_seq)\n",
    "\n",
    "    # devide the data into a training set and a test set in 70-30 ratio\n",
    "    training_ratio = int(len(X_seq) * 0.7)\n",
    "    X_train, X_test = X_seq[:training_ratio], X_seq[training_ratio:]\n",
    "    y_train, y_test = y_seq[:training_ratio], y_seq[training_ratio:]\n",
    "    \n",
    "    # return the sets and the last_date\n",
    "    return X_train, X_test, y_train, y_test, last_date\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test, last_date = create_train_test_sets(df, \"classification\", 6)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8add0",
   "metadata": {},
   "source": [
    "# Scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87485a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a MinMaxScaler instance for a range between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# pass the features to the scaler\n",
    "scaled_X1 = scaler.fit_transform(X1)\n",
    "\n",
    "# scaled_X1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baa116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee127b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6a881ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammaroAsus\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 0s - 5ms/step - accuracy: 0.5423 - loss: 0.7908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7908073663711548, 0.5422534942626953]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create and train the baseline classification model\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_model(timesteps=6):\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(SimpleRNN(64, input_shape=(timesteps, X_train.shape[2]), return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_model()\n",
    "\n",
    "# get the model weights before training\n",
    "# model1.get_weights()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "## Model evaluation and prototype conclusion\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f9f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef6c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda4d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9239e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Project prototype (implementation)\n",
    "\n",
    "## Install Dependencies and import libraries\n",
    "\n",
    "# pip install pandas numpy yfinance pandas-ta scikit-learn tensorflow\n",
    "\n",
    "# https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "# import yfinance, our data source\n",
    "import yfinance as yf\n",
    "\n",
    "# https://pypi.org/project/pandas-ta/ (\"\"\"An easy to use Python 3 Pandas Extension with 130+ Technical Analysis Indicators. Can be called from a Pandas DataFrame or standalone\"\"\")\n",
    "# import pandas-ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import from tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Input, GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# insert the stock symbols into a list\n",
    "symbols_list = ['PFE', 'ROP', 'XYL', 'CPAY', 'INCY']\n",
    "\n",
    "# we will take the weekly data for the last 10 years\n",
    "# data_weekly = yf.download(symbols_list, period='10y', interval='1wk')\n",
    "\n",
    "## Format the data and save it as a CSV file\n",
    "\n",
    "# source of inspiration: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html[10]\n",
    "# Return a reshaped DataFrame having a multi-level inde\n",
    "# stacked_data_weekly = data_weekly.stack()\n",
    "# stacked_data_weekly\n",
    "\n",
    "# Save the data to a CSV so we don't have to make any extra unnecessary requests to the API every time we reload the notebook\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "# stacked_data_weekly.to_csv('stacked_data_weekly_1.csv', index=True)\n",
    "\n",
    "# load the the dataframe from the csv file\n",
    "df = pd.read_csv('stacked_data_weekly_1.csv').set_index([\"Date\", \"Ticker\"])\n",
    "\n",
    "# df.head(5)\n",
    "\n",
    "## Perform simple exploritory data analysis\n",
    "\n",
    "# how many null values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# the data shape\n",
    "df.shape\n",
    "\n",
    "# data basic stats\n",
    "df.describe()\n",
    "\n",
    "## Devide the data into five dataframes, one for each stock\n",
    "\n",
    "# source of inspiration https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html [11]\n",
    "# select specific stock data at the 'Ticker' level of this multi index dataframe\n",
    "df1 = df.xs('PFE', axis=0, level='Ticker', drop_level=True)\n",
    "df2 = df.xs('ROP', axis=0, level='Ticker', drop_level=True)\n",
    "df3 = df.xs('XYL', axis=0, level='Ticker', drop_level=True)\n",
    "df4 = df.xs('CPAY', axis=0, level='Ticker', drop_level=True)\n",
    "df5 = df.xs('INCY', axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "# disply the first dataframe\n",
    "df1.head(5)\n",
    "\n",
    "# show the new df shape\n",
    "df1.shape\n",
    "\n",
    "## Create the target of the model\n",
    "\n",
    "# copy the dateframe before modification so we don't get a warning from jupyter notebook\n",
    "df1 = df1.copy()\n",
    "\n",
    "# create the 'Next' column to be equal to the next closing price\n",
    "# this can be accomplished easily by shifting the close column backward by 1\n",
    "df1[\"Next\"] = df1['Close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['Next'] > row['Close']:\n",
    "        return 1\n",
    "    elif row['Next'] < row['Close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# create the 'Trend' column to be equal to the output of the 'assign_trend' function    \n",
    "df1['Trend'] = df1.apply(assign_trend, axis=1)\n",
    "\n",
    "# check out the results\n",
    "df1.head(5)\n",
    "\n",
    "# Check if the data is balanced\n",
    "\n",
    "# let's check the occurance of each value in the Trend column\n",
    "df1['Trend'].value_counts()\n",
    "\n",
    "df1['Trend'].value_counts()[1]\n",
    "\n",
    "# percentage of 'trend up' to the whole column\n",
    "df1['Trend'].value_counts()[1]/df1.shape[0]\n",
    "\n",
    "## Create a common sense baseline\n",
    "\n",
    "# this can be accomplished easily by shifting the close column forward by 1\n",
    "common_sense = df1['Trend'].shift(1)\n",
    "\n",
    "# measure the average of when the common sense (naive) prediction will match the actual 'Trend'\n",
    "(common_sense == df1['Trend']).mean()\n",
    "\n",
    "## Include the technical indicators\n",
    "\n",
    "#  we can easily check the available indicators in the pandas-ta library\n",
    "# help(df1.ta.indicators())\n",
    "\n",
    "#  we can also learn about any specific indicator like this\n",
    "# help(ta.macd)\n",
    "\n",
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def assign_TIs(_df):\n",
    "    # apply macd on the Close column in a df and add it to the dataframe    \n",
    "    mcda = ta.macd(_df[\"Close\"])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, \"MACD\", mcda[\"MACD_12_26_9\"])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(7, \"Signal\", mcda[\"MACD_12_26_9\"])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(8, \"Histogram\", mcda[\"MACD_12_26_9\"])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df[\"Close\"])\n",
    "    _df.insert(9, \"RSI\", rsi)\n",
    "    \n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df[\"Close\"])\n",
    "    _df.insert(10, \"SMA\", sma)\n",
    "    \n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df[\"Close\"])\n",
    "    _df.insert(11, \"EMA\", ema)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "# apply the function to the dataframe\n",
    "df1 = assign_TIs(df1)\n",
    "\n",
    "# drop the NaN values\n",
    "df1.dropna(inplace=True)\n",
    "\n",
    "# fix the 'Trend' data type to be int\n",
    "df1 = df1.astype({'Trend': int})\n",
    "\n",
    "# check the dataframe\n",
    "df1.head(5)\n",
    "\n",
    "# the shape of the data now\n",
    "df1.shape\n",
    "\n",
    "##  Prepare the data for training\n",
    "\n",
    "# reset the index\n",
    "df1.reset_index(inplace = True)\n",
    "\n",
    "# drop the Date column as it's not necessary for now\n",
    "df1.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "# df1.head(5)\n",
    "\n",
    "Create the features list, for now we will use every column except the last two.\n",
    "\n",
    "# The features list\n",
    "X1 = df1.iloc[:, :-2]\n",
    "\n",
    "X1.head(2)\n",
    "\n",
    "# Create the target, which is the 'Trend' column for now.\n",
    "\n",
    "# The Target (Trend for now)\n",
    "y1 = df1.iloc[:, -1]\n",
    "\n",
    "# initialize a MinMaxScaler instance for a range between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# pass the features to the scaler\n",
    "scaled_X1 = scaler.fit_transform(X1)\n",
    "\n",
    "# scaled_X1\n",
    "\n",
    "# source of isnpiration: https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4 [13]\n",
    "# create a function to reshape X and y into sequences of x timesteps\n",
    "def create_seqs(features, target, num_rows):\n",
    "    # create 2 empty lists to store the newly shaped features and target lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate over the features\n",
    "    for i in range(len(features) - num_rows):\n",
    "        # create indexes of the start and end of each sequence\n",
    "        seq_s = i\n",
    "        seq_e = i + num_rows\n",
    "        \n",
    "        # the ith sequence will be a slice of the features between the indexes, create it and add it to X\n",
    "        xi = features[seq_s : seq_e]\n",
    "        X.append(xi)\n",
    "        \n",
    "        # do the same for the target and add it to y\n",
    "        yi = target[seq_e]\n",
    "        y.append(yi)\n",
    "    \n",
    "    # return the X and y as numpy arraies\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "timesteps = 6\n",
    "X_seq1, y_seq1 = create_seqs(scaled_X1, y1, timesteps)\n",
    "\n",
    "# check the new shapes for the features and labels sets\n",
    "X_seq1.shape, y_seq1.shape\n",
    "\n",
    "# source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical [14]\n",
    "# use to_categorical from tf to converts the target (Trend) to binary class matrix\n",
    "y_seq1 = to_categorical(y_seq1)\n",
    "\n",
    "# Devide the data into a training set and a test set in 70-30 ratio\n",
    "\n",
    "#  sets the training test ratio to be 70-30\n",
    "training_ratio = int(len(X_seq1) * 0.7)\n",
    "\n",
    "# # split the data into training and test\n",
    "X1_train, X1_test = X_seq1[:training_ratio], X_seq1[training_ratio:]\n",
    "y1_train, y1_test = y_seq1[:training_ratio], y_seq1[training_ratio:]\n",
    "\n",
    "X1_train.shape, X1_test.shape\n",
    "\n",
    "## Create and train the baseline classification model\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(SimpleRNN(64, input_shape=(timesteps, X1_train.shape[2]), return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_model()\n",
    "\n",
    "# get the model weights before training\n",
    "# model1.get_weights()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "## Model evaluation and prototype conclusion\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# get the model summary\n",
    "# model1.summary()\n",
    "\n",
    "# get the model compile configurations\n",
    "# model1.get_compile_config()\n",
    "\n",
    "# get the model configurations after training\n",
    "# model1.get_config()\n",
    "\n",
    "# get the model weights after training\n",
    "# model1.get_weights()\n",
    "\n",
    "# Get a prediction from the model given the last 6 weeks. This is to simulate how a user would get a prediction from the model. The input will be the last entry in the test set.\n",
    "\n",
    "# reshape the input so it have the shape (1, 6, 12) which what the model expect as input\n",
    "_input = X1_test[-1].reshape(1, X1_test.shape[1], X1_test.shape[2])\n",
    "pred = model1.predict(_input)\n",
    "print(f\"Trend: {np.argmax(pred)}, \",f\"Confidence: {np.max(pred)}\")\n",
    "\n",
    "### LSTM\n",
    "\n",
    "# construct LSTM the model\n",
    "def create_LSTM_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    # input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # first dense layer\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    \n",
    "    # second dense layer\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_LSTM_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### GRU\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_GRU_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    # input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # first dense layer\n",
    "    model.add(GRU(64, \n",
    "                  dropout=0.1, \n",
    "                  recurrent_dropout=0.5, \n",
    "                  return_sequences=True))\n",
    "    \n",
    "    # second dense layer\n",
    "    model.add(GRU(64,\n",
    "                  dropout=0.1, \n",
    "                  recurrent_dropout=0.5))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_GRU_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Now le'ts create a regression version of all the 3 models we have so far.\n",
    "# First let's adjust the target column of the model, for refression the column we are trying to predict is the Next colum.\n",
    "\n",
    "# The Target (Next for now)\n",
    "y1_reg = df1.iloc[:, -2]\n",
    "\n",
    "# Create sequences\n",
    "timesteps = 6\n",
    "X_seq1_reg, y_seq1_reg = create_seqs(scaled_X1, y1_reg, timesteps)\n",
    "\n",
    "# split the data into training and test 70-30 ratio\n",
    "X1_train_reg, X1_test_reg = X_seq1_reg[:training_ratio], X_seq1_reg[training_ratio:]\n",
    "y1_train_reg, y1_test_reg = y_seq1_reg[:training_ratio], y_seq1_reg[training_ratio:]\n",
    "\n",
    "### SimpleRNN regression model\n",
    "\n",
    "# construct the model\n",
    "def create_SimpleRNN_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(SimpleRNN(64, return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_SimpleRNN_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### LSTM regression model\n",
    "\n",
    "# construct the model\n",
    "def create_LSTM_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(LSTM(64, return_sequences=True))    \n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_LSTM_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=300, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### GRU regression model\n",
    "\n",
    "# construct the model\n",
    "def create_GRU_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n",
    "    model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_GRU_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the average value for the Next closing price to judge weather the mae is acceptable or not \n",
    "df1['Next'].mean()\n",
    "\n",
    "## Hyperparameters optimization\n",
    "\n",
    "# !pip install scikeras\n",
    "\n",
    "# helper function to measure how long a process would take\n",
    "from datetime import datetime\n",
    "\n",
    "def get_time():\n",
    "    return datetime.now()\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import warnings\n",
    "\n",
    "\n",
    "# get the time before starting the process\n",
    "start = get_time()\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/72392579/scikeras-randomizedsearchcv-for-best-hyper-parameters [19]\n",
    "# construct the model\n",
    "def create_model(n_hidden = 1, n_neurons = 30, learning_rate=3e-3):\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create an input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # add a static first deep layer\n",
    "    model.add(SimpleRNN(n_neurons, return_sequences=True))\n",
    "\n",
    "    # add the other model deep layers dynamically\n",
    "    for layer in range(1, n_hidden):\n",
    "        model.add(SimpleRNN(n_neurons))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # create an Adam optimizer with a variable learning rate\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# define possible parameters (this is just a test for now and we will add more parameters for the final report)\n",
    "# we need to distingiush between the model building function input and the RandomizedSearchCV input, to do that we prefix the model input with model__\n",
    "# source: https://adriangb.com/scikeras/stable/notebooks/Basic_Usage.html (7.1 Special prefixes) [20]\n",
    "params = {\n",
    "    \"model__n_hidden\": [0, 1, 2, 3, 4, 5],\n",
    "    \"model__n_neurons\": [int(x) for x in np.arange(1, 128)],\n",
    "    \"model__learning_rate\": [1e-2, 1e-3, 1e-4], \n",
    "    'epochs': [10, 20, 30],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a keras classification model wrappers from the scikeras library which allow us to utilize the hyperparameter tunning functions from the scikit-learn library\n",
    "kerasWarp = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_simpleRnn_clas = RandomizedSearchCV(estimator=kerasWarp, param_distributions=params, n_iter=10, cv=None, random_state=101)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/40105796/turn-warning-off-in-a-cell-jupyter-notebook [21]\n",
    "# prevent FitFailedWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    # fit the RandomizedSearchCV models\n",
    "    random_simpleRnn_clas.fit(X1_train, y1_train, validation_split=0.2)\n",
    "\n",
    "# print results\n",
    "print(f\"RandomizedSearchCV best parameters: {random_simpleRnn_clas.best_params_}\")\n",
    "print(f\"RandomizedSearchCV best score: {random_simpleRnn_clas.best_score_}\")\n",
    "\n",
    "\n",
    "# get the time after finishing the process\n",
    "end = get_time()\n",
    "\n",
    "# print the duration\n",
    "print(f\"process finished in: {end - start}\")\n",
    "\n",
    "# retrieve the best model and refit on the training data to get the history\n",
    "best_model = random_simpleRnn_clas.best_estimator_.model_\n",
    "\n",
    "# evaluate the best model on the test data\n",
    "best_model.evaluate(X1_test, y1_test, verbose=1)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = best_model.predict(X1_test)\n",
    "\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# get a summary of the best model\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38f804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1084c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd8e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8aef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9647e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
