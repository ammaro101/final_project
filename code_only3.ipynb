{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22168f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Project prototype (implementation)\n",
    "## Install Dependencies and import libraries\n",
    "\n",
    "# pip install pandas numpy yfinance pandas-ta scikit-learn tensorflow\n",
    "\n",
    "# https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "# import yfinance, our data source\n",
    "import yfinance as yf\n",
    "\n",
    "# https://pypi.org/project/pandas-ta/ (\"\"\"An easy to use Python 3 Pandas Extension with 130+ Technical Analysis Indicators. Can be called from a Pandas DataFrame or standalone\"\"\")\n",
    "# import pandas-ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import from tensorflow\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Input, GRU\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fbc43",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf16548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no 1wk_stocks_data.csv. Data will be downloaded from yfinance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  5 of 5 completed\n"
     ]
    }
   ],
   "source": [
    "# insert the stock symbols into a list\n",
    "symbols_list = ['PFE', 'ROP', 'XYL', 'CPAY', 'INCY']\n",
    "\n",
    "# define a function to load the data from source (yfinance API), and save it as a csv to local storage\n",
    "def loadData(symbols=symbols_list, period='10y', interval='1wk'):\n",
    "    \n",
    "    try:\n",
    "        # load the the dataframe from the csv file if it already exist\n",
    "        df = pd.read_csv(f'{interval}_stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "        \n",
    "        print(\"Data loaded from directory\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        # print a message stating the data does not already exists and need to be downloaded from yfinance\n",
    "        print(f\"There is no {interval}_stocks_data.csv. Data will be downloaded from yfinance.\")\n",
    "        \n",
    "        # download the data from source and store it in the stock_data variable which will hold the data as a pandas dataframe\n",
    "        stocks_data =  yf.download(symbols, period=period, interval=interval)\n",
    "\n",
    "        # reshape the dataframe as a multi-level index dataframe\n",
    "        stocks_data = stocks_data.stack()\n",
    "\n",
    "        # source: https://www.statology.org/pandas-change-column-names-to-lowercase/\n",
    "        # convert column names to lowercase\n",
    "        stocks_data.columns = stocks_data.columns.str.lower()\n",
    "\n",
    "        # save the dataframe to a csv file (Save the data to a CSV so we don't have to make any extra unnecessary requests to the API every time we reload the notebook)\n",
    "        stocks_data.to_csv(f'{interval}_stocks_data.csv', index=True)\n",
    "\n",
    "        # load the the dataframe from the csv file\n",
    "        df = pd.read_csv(f'{interval}_stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "\n",
    "    finally: \n",
    "        # create a dict to store the dataframe of each unique symbol where keys are symbol, values are dataframes\n",
    "        df_dict = {}\n",
    "\n",
    "        # iterate over the symbols\n",
    "        for symbol in symbols:\n",
    "\n",
    "            # source of inspiration https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html [11]\n",
    "            # extract the specific stock data at the 'Ticker' level of this multi index dataframe and save it as a dataframe\n",
    "            symbol_df = df.xs(symbol, axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "            # store the datafram into the df_dict\n",
    "            df_dict[symbol] = symbol_df\n",
    "\n",
    "        # return the dictionary\n",
    "        return df_dict\n",
    "\n",
    "\n",
    "dfs = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910aa98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08-11</th>\n",
       "      <td>18.445234</td>\n",
       "      <td>27.172676</td>\n",
       "      <td>27.419355</td>\n",
       "      <td>26.726755</td>\n",
       "      <td>26.802656</td>\n",
       "      <td>62383098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-18</th>\n",
       "      <td>18.625566</td>\n",
       "      <td>27.438330</td>\n",
       "      <td>27.542694</td>\n",
       "      <td>27.220114</td>\n",
       "      <td>27.277040</td>\n",
       "      <td>102843207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-25</th>\n",
       "      <td>18.928263</td>\n",
       "      <td>27.884251</td>\n",
       "      <td>28.149904</td>\n",
       "      <td>27.428843</td>\n",
       "      <td>27.447819</td>\n",
       "      <td>99802628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>19.095709</td>\n",
       "      <td>28.130930</td>\n",
       "      <td>28.140417</td>\n",
       "      <td>27.666035</td>\n",
       "      <td>27.713472</td>\n",
       "      <td>86197384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-08</th>\n",
       "      <td>18.954029</td>\n",
       "      <td>27.922201</td>\n",
       "      <td>28.130930</td>\n",
       "      <td>27.523720</td>\n",
       "      <td>28.064516</td>\n",
       "      <td>97524618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-15</th>\n",
       "      <td>29.552921</td>\n",
       "      <td>29.969999</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>29.030001</td>\n",
       "      <td>180142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-22</th>\n",
       "      <td>30.341789</td>\n",
       "      <td>30.770000</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>29.309999</td>\n",
       "      <td>30.110001</td>\n",
       "      <td>179544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>30.430000</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>31.540001</td>\n",
       "      <td>29.780001</td>\n",
       "      <td>30.690001</td>\n",
       "      <td>254667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-05</th>\n",
       "      <td>28.549999</td>\n",
       "      <td>28.549999</td>\n",
       "      <td>30.049999</td>\n",
       "      <td>28.450001</td>\n",
       "      <td>29.090000</td>\n",
       "      <td>157625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-12</th>\n",
       "      <td>28.850000</td>\n",
       "      <td>28.850000</td>\n",
       "      <td>28.950001</td>\n",
       "      <td>28.309999</td>\n",
       "      <td>28.580000</td>\n",
       "      <td>81639829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj close      close       high        low       open     volume\n",
       "Date                                                                        \n",
       "2014-08-11  18.445234  27.172676  27.419355  26.726755  26.802656   62383098\n",
       "2014-08-18  18.625566  27.438330  27.542694  27.220114  27.277040  102843207\n",
       "2014-08-25  18.928263  27.884251  28.149904  27.428843  27.447819   99802628\n",
       "2014-09-01  19.095709  28.130930  28.140417  27.666035  27.713472   86197384\n",
       "2014-09-08  18.954029  27.922201  28.130930  27.523720  28.064516   97524618\n",
       "...               ...        ...        ...        ...        ...        ...\n",
       "2024-07-15  29.552921  29.969999  30.690001  28.830000  29.030001  180142400\n",
       "2024-07-22  30.341789  30.770000  30.930000  29.309999  30.110001  179544200\n",
       "2024-07-29  30.430000  30.430000  31.540001  29.780001  30.690001  254667700\n",
       "2024-08-05  28.549999  28.549999  30.049999  28.450001  29.090000  157625900\n",
       "2024-08-12  28.850000  28.850000  28.950001  28.309999  28.580000   81639829\n",
       "\n",
       "[523 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[symbols_list[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08027413",
   "metadata": {},
   "source": [
    "# Add Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5a8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dataframe and create 'next_close' column based on its 'close' column\n",
    "def get_next_close(_df):\n",
    "    \n",
    "    # create the 'next_close' column to be equal to the next closing price\n",
    "    # this can be accomplished easily by shifting the close column backward by 1\n",
    "    return _df['close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['next_close'] > row['close']:\n",
    "        return 1\n",
    "    elif row['next_close'] < row['close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "# create a function that add the target columns to the dataframe\n",
    "def add_targets(_df):\n",
    "    \n",
    "    # add the next_close column to the dataframe\n",
    "    _df['next_close'] = get_next_close(_df)\n",
    "    \n",
    "    # add the trend column to the dataframe\n",
    "    _df['trend'] = _df.apply(assign_trend, axis=1)\n",
    "    \n",
    "    # drop the NaN values\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    # fix the 'trend' data type to be int\n",
    "    _df = _df.astype({'trend': int})\n",
    "    \n",
    "    return _df\n",
    "\n",
    "# add target columns to all the data frames\n",
    "# df = add_targets(dfs[symbols_list[0]])\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6eb2d",
   "metadata": {},
   "source": [
    "# Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "ac283a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas TA - Technical Analysis Indicators - v0.3.14b0\n",
      "Total Indicators & Utilities: 205\n",
      "Abbreviations:\n",
      "    aberration, above, above_value, accbands, ad, adosc, adx, alma, amat, ao, aobv, apo, aroon, atr, bbands, below, below_value, bias, bop, brar, cci, cdl_pattern, cdl_z, cfo, cg, chop, cksp, cmf, cmo, coppock, cross, cross_value, cti, decay, decreasing, dema, dm, donchian, dpo, ebsw, efi, ema, entropy, eom, er, eri, fisher, fwma, ha, hilo, hl2, hlc3, hma, hwc, hwma, ichimoku, increasing, inertia, jma, kama, kc, kdj, kst, kurtosis, kvo, linreg, log_return, long_run, macd, mad, massi, mcgd, median, mfi, midpoint, midprice, mom, natr, nvi, obv, ohlc4, pdist, percent_return, pgo, ppo, psar, psl, pvi, pvo, pvol, pvr, pvt, pwma, qqe, qstick, quantile, rma, roc, rsi, rsx, rvgi, rvi, short_run, sinwma, skew, slope, sma, smi, squeeze, squeeze_pro, ssf, stc, stdev, stoch, stochrsi, supertrend, swma, t3, td_seq, tema, thermo, tos_stdevall, trima, trix, true_range, tsi, tsignals, ttm_trend, ui, uo, variance, vhf, vidya, vortex, vp, vwap, vwma, wcp, willr, wma, xsignals, zlma, zscore\n",
      "\n",
      "Candle Patterns:\n",
      "    2crows, 3blackcrows, 3inside, 3linestrike, 3outside, 3starsinsouth, 3whitesoldiers, abandonedbaby, advanceblock, belthold, breakaway, closingmarubozu, concealbabyswall, counterattack, darkcloudcover, doji, dojistar, dragonflydoji, engulfing, eveningdojistar, eveningstar, gapsidesidewhite, gravestonedoji, hammer, hangingman, harami, haramicross, highwave, hikkake, hikkakemod, homingpigeon, identical3crows, inneck, inside, invertedhammer, kicking, kickingbylength, ladderbottom, longleggeddoji, longline, marubozu, matchinglow, mathold, morningdojistar, morningstar, onneck, piercing, rickshawman, risefall3methods, separatinglines, shootingstar, shortline, spinningtop, stalledpattern, sticksandwich, takuri, tasukigap, thrusting, tristar, unique3river, upsidegap2crows, xsidegap3methods\n",
      "Help on NoneType object:\n",
      "\n",
      "class NoneType(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      True if self else False\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  we can easily check the available indicators in the pandas-ta library\n",
    "help(df.ta.indicators())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "9401d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function donchian in module pandas_ta.volatility.donchian:\n",
      "\n",
      "donchian(high, low, lower_length=None, upper_length=None, offset=None, **kwargs)\n",
      "    Donchian Channels (DC)\n",
      "    \n",
      "    Donchian Channels are used to measure volatility, similar to\n",
      "    Bollinger Bands and Keltner Channels.\n",
      "    \n",
      "    Sources:\n",
      "        https://www.tradingview.com/wiki/Donchian_Channels_(DC)\n",
      "    \n",
      "    Calculation:\n",
      "        Default Inputs:\n",
      "            lower_length=upper_length=20\n",
      "        LOWER = low.rolling(lower_length).min()\n",
      "        UPPER = high.rolling(upper_length).max()\n",
      "        MID = 0.5 * (LOWER + UPPER)\n",
      "    \n",
      "    Args:\n",
      "        high (pd.Series): Series of 'high's\n",
      "        low (pd.Series): Series of 'low's\n",
      "        lower_length (int): The short period. Default: 20\n",
      "        upper_length (int): The short period. Default: 20\n",
      "        offset (int): How many periods to offset the result. Default: 0\n",
      "    \n",
      "    Kwargs:\n",
      "        fillna (value, optional): pd.DataFrame.fillna(value)\n",
      "        fill_method (value, optional): Type of fill method\n",
      "    \n",
      "    Returns:\n",
      "        pd.DataFrame: lower, mid, upper columns.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCL_20_20</th>\n",
       "      <th>DCM_20_20</th>\n",
       "      <th>DCU_20_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.465000</td>\n",
       "      <td>29.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.465000</td>\n",
       "      <td>29.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-15</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>27.945001</td>\n",
       "      <td>30.690001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-22</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>28.065001</td>\n",
       "      <td>30.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>25.200001</td>\n",
       "      <td>28.370001</td>\n",
       "      <td>31.540001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DCL_20_20  DCM_20_20  DCU_20_20\n",
       "Date                                       \n",
       "2014-08-04        NaN        NaN        NaN\n",
       "2014-08-11        NaN        NaN        NaN\n",
       "2014-08-18        NaN        NaN        NaN\n",
       "2014-08-25        NaN        NaN        NaN\n",
       "2014-09-01        NaN        NaN        NaN\n",
       "...               ...        ...        ...\n",
       "2024-07-01  25.200001  27.465000  29.730000\n",
       "2024-07-08  25.200001  27.465000  29.730000\n",
       "2024-07-15  25.200001  27.945001  30.690001\n",
       "2024-07-22  25.200001  28.065001  30.930000\n",
       "2024-07-29  25.200001  28.370001  31.540001\n",
       "\n",
       "[518 rows x 3 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(ta.donchian)\n",
    "df.ta.donchian()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70aa6f",
   "metadata": {},
   "source": [
    "65 different technical indicators columns were added in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6d64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def add_technical_indicators(_df):\n",
    "    # apply macd on the close column in a df and add it to the dataframe    \n",
    "    macd = ta.macd(_df['close'])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, 'macd', macd.iloc[:,0])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(7, 'macd_histogram', macd.iloc[:,1])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(8, 'macd_signal', macd.iloc[:,2])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df['close'])\n",
    "    _df.insert(9, 'rsi', rsi)\n",
    "\n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df['close'])\n",
    "    _df.insert(10, 'sma', sma)\n",
    "\n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df['close'])\n",
    "    _df.insert(11, 'ema', ema)\n",
    "\n",
    "    ######## repeat the same proccess for all the technical indicators we want to include ##########\n",
    "    # aberration: A volatility indicator\n",
    "    aberration = ta.aberration(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(12, 'aberration_zg', aberration.iloc[:,0])\n",
    "    _df.insert(13, 'aberration_sg', aberration.iloc[:,1])\n",
    "    _df.insert(14, 'aberration_xg', aberration.iloc[:,2])\n",
    "    _df.insert(15, 'aberration_atr', aberration.iloc[:,3])\n",
    "    \n",
    "    # bbands: A popular volatility indicator by John Bollinger.\n",
    "    bbands = ta.bbands(_df['close'])\n",
    "    _df.insert(16, 'bbands_lower', bbands.iloc[:,0])\n",
    "    _df.insert(17, 'bbands_mid', bbands.iloc[:,1])\n",
    "    _df.insert(18, 'bbands_upper', bbands.iloc[:,2])\n",
    "    _df.insert(19, 'bbands_bandwidth', bbands.iloc[:,3])\n",
    "    _df.insert(20, 'bbands_percent', bbands.iloc[:,4])\n",
    "    \n",
    "    # adx:  Average Directional Movement is meant to quantify trend strength by measuring the amount of movement in a single direction.    \n",
    "    adx = ta.adx(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(21, 'adx_adx', adx.iloc[:,0])\n",
    "    _df.insert(22, 'adx_dmp', adx.iloc[:,1])\n",
    "    _df.insert(23, 'adx_dmn', adx.iloc[:,2])\n",
    "\n",
    "    # atr: Averge True Range is used to measure volatility, especially volatility caused by gaps or limit moves.\n",
    "    atr = ta.atr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(24, 'atr', atr)\n",
    "    \n",
    "    # stoch: The Stochastic Oscillator (STOCH) was developed by George Lane in the 1950's. He believed this indicator was a good way to measure momentum because changes in momentum precede changes in price.\n",
    "    stoch = ta.stoch(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(25, 'stoch_k', stoch.iloc[:,0])\n",
    "    _df.insert(26, 'stoch_d', stoch.iloc[:,1])\n",
    "    \n",
    "    # obv: On Balance Volume is a cumulative indicator to measure buying and selling pressure.\n",
    "    obv = ta.obv(_df['close'], _df['volume'])\n",
    "    _df.insert(27, 'obv', obv)\n",
    "    \n",
    "    # Supertrend: is an overlap indicator. It is used to help identify trend direction, setting stop loss, identify support and resistance, and/or generate buy & sell signals.\n",
    "    supertrend = ta.supertrend(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(28, 'supertrend_trend', supertrend.iloc[:,0])\n",
    "    _df.insert(29, 'supertrend_direction', supertrend.iloc[:,1])\n",
    "    \n",
    "    # dema: The Double Exponential Moving Average attempts to a smoother average with less lag than the normal Exponential Moving Average (EMA).\n",
    "    dema = ta.dema(_df['close'])\n",
    "    _df.insert(30, 'dema', dema)\n",
    "    \n",
    "    # tema: A less laggy Exponential Moving Average.\n",
    "    tema = ta.tema(_df['close'])\n",
    "    _df.insert(31, 'tema', tema)\n",
    "\n",
    "    # roc: Rate of Change is an indicator is also referred to as Momentum. It is a pure momentum oscillator that measures the percent change in price with the previous price 'n' (or length) periods ago.\n",
    "    roc = ta.roc(_df['close'])\n",
    "    _df.insert(32, 'roc', roc)\n",
    "    \n",
    "    # mom: Momentum is an indicator used to measure a security's speed (or strength) of movement.  Or simply the change in price.\n",
    "    mom = ta.mom(_df['close'])\n",
    "    _df.insert(33, 'mom', mom)\n",
    "    \n",
    "    # cci: Commodity Channel Index is a momentum oscillator used to primarily identify overbought and oversold levels relative to a mean.\n",
    "    cci = ta.cci(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(34, 'cci', cci)\n",
    "    \n",
    "    # aroon: attempts to identify if a security is trending and how strong.\n",
    "    aroon = ta.aroon(_df['high'], _df['low'])\n",
    "    _df.insert(35, 'aroon_up', aroon.iloc[:,0])\n",
    "    _df.insert(36, 'aroon_down', aroon.iloc[:,1])\n",
    "    _df.insert(37, 'aroon_osc', aroon.iloc[:,2])\n",
    "    \n",
    "    # natr: Normalized Average True Range attempt to normalize the average true range.\n",
    "    natr = ta.natr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(38, 'natr', natr)\n",
    "    \n",
    "    # William's Percent R is a momentum oscillator similar to the RSI that attempts to identify overbought and oversold conditions.\n",
    "    willr = ta.willr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(39, 'willr', willr)\n",
    "    \n",
    "    # vortex: Two oscillators that capture positive and negative trend movement.\n",
    "    vortex = ta.vortex(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(40, 'vortex_vip', vortex.iloc[:,0])\n",
    "    _df.insert(41, 'vortex_vim', vortex.iloc[:,1])\n",
    "        \n",
    "    # kama: Developed by Perry Kaufman, Kaufman's Adaptive Moving Average (KAMA) is a moving average designed to account for market noise or volatility. KAMA will closely follow prices when the price swings are relatively small and the noise is low. KAMA will adjust when the price swings widen and follow prices from a greater distance. This trend-following indicator can be used to identify the overall trend, time turning points and filter price movements.\n",
    "    kama = ta.kama(_df['close'])\n",
    "    _df.insert(42, 'kama', kama)\n",
    "                       \n",
    "    # trix: is a momentum oscillator to identify divergences.\n",
    "    trix = ta.trix(_df['close'])\n",
    "    _df.insert(43, 'trix', trix.iloc[:,0])\n",
    "    _df.insert(44, 'trixs', trix.iloc[:,1])\n",
    "                       \n",
    "    # hlc3: the average of high, low, and close prices\n",
    "    hlc3 = ta.hlc3(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(45, 'hlc3', hlc3)\n",
    "\n",
    "    # ohlc4: the average of open, high, low, and close prices\n",
    "    ohlc4 = ta.ohlc4(_df['open'], _df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(46, 'ohlc4', ohlc4)\n",
    "    \n",
    "    # hma: The Hull Exponential Moving Average attempts to reduce or remove lag in moving averages.\n",
    "    hma = ta.hma(_df['close'])\n",
    "    _df.insert(47, 'hma', hma)\n",
    "\n",
    "    # vwma: Volume Weighted Moving Average.\n",
    "    vwma = ta.vwma(_df['close'], _df['volume'])\n",
    "    _df.insert(48, 'vwma', vwma)\n",
    "    \n",
    "    # accbands: Acceleration Bands created by Price Headley plots upper and lower envelope bands around a simple moving average.\n",
    "    accbands = ta.accbands(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(49, 'accbands_lower', accbands.iloc[:,0])\n",
    "    _df.insert(50, 'accbands_mid', accbands.iloc[:,1])\n",
    "    _df.insert(51, 'accbands_upper', accbands.iloc[:,2])\n",
    "    \n",
    "    # adosc: Accumulation/Distribution Oscillator indicator utilizes Accumulation/Distribution and treats it similarily to MACD or APO.\n",
    "    adosc = ta.adosc(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(52, 'adosc', adosc)\n",
    "    \n",
    "    # alma: The ALMA moving average uses the curve of the Normal (Gauss) distribution, which can be shifted from 0 to 1. This allows regulating the smoothness and high sensitivity of the indicator. Sigma is another parameter that is responsible for the shape of the curve coefficients. This moving average reduces lag of the data in conjunction with smoothing to reduce noise.\n",
    "    alma = ta.alma(_df['close'])\n",
    "    _df.insert(53, 'alma', alma)\n",
    "    \n",
    "    # apo: The Absolute Price Oscillator is an indicator used to measure a security's momentum.  It is simply the difference of two Exponential Moving Averages (EMA) of two different periods. Note: APO and MACD lines are equivalent.\n",
    "    apo = ta.apo(_df['close'])\n",
    "    _df.insert(54, 'apo', apo)\n",
    "    \n",
    "    # cfo: The Forecast Oscillator calculates the percentage difference between the actualprice and the Time Series Forecast (the endpoint of a linear regression line).\n",
    "    cfo = ta.cfo(_df['close'])\n",
    "    _df.insert(55, 'cfo', cfo)\n",
    "    \n",
    "    # cg: The Center of Gravity Indicator by John Ehlers attempts to identify turning points while exhibiting zero lag and smoothing.\n",
    "    cg = ta.cg(_df['close'])\n",
    "    _df.insert(56, 'cg', cg)    \n",
    "    \n",
    "    # chop: The Choppiness Index was created by Australian commodity trader E.W. Dreiss and is designed to determine if the market is choppy (trading sideways) or not choppy (trading within a trend in either direction). Values closer to 100 implies the underlying is choppier whereas values closer to 0 implies the underlying is trending.\n",
    "    chop = ta.chop(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(57, 'chop', chop)\n",
    "    \n",
    "    # cmf: Chailin Money Flow measures the amount of money flow volume over a specific period in conjunction with Accumulation/Distribution.\n",
    "    cmf = ta.cmf(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(58, 'cmf', cmf)\n",
    "    \n",
    "    # cmo: Attempts to capture the momentum of an asset with overbought at 50 and oversold at -50.\n",
    "    cmo = ta.cmo(_df['close'])\n",
    "    _df.insert(59, 'cmo', cmo)\n",
    "    \n",
    "    # coppock: Coppock Curve (originally called the \"Trendex Model\") is a momentum indicator is designed for use on a monthly time scale.  Although designed for monthly use, a daily calculation over the same period can be made, converting the periods to 294-day and 231-day rate of changes, and a 210-day weighted moving average.\n",
    "    coppock = ta.coppock(_df['close'])\n",
    "    _df.insert(60, 'coppock', coppock)\n",
    "    \n",
    "    # cti: The Correlation Trend Indicator is an oscillator created by John Ehler in 2020. It assigns a value depending on how close prices in that range are to following a positively- or negatively-sloping straight line. Values range from -1 to 1. This is a wrapper for ta.linreg(close, r=True).\n",
    "    cti = ta.cti(_df['close'])\n",
    "    _df.insert(61, 'cti', cti)\n",
    "    \n",
    "    # decay: Creates a decay moving forward from prior signals like crosses. The default is \"linear\". Exponential is optional as \"exponential\" or \"exp\".\n",
    "    decay = ta.decay(_df['close'])\n",
    "    _df.insert(62, 'decay', decay)\n",
    "    \n",
    "    # decreasing: Returns True if the series is decreasing over a period, False otherwise. If the kwarg 'strict' is True, it returns True if it is continuously decreasing over the period. When using the kwarg 'asint', then it returns 1 for True or 0 for False.\n",
    "    decreasing = ta.decreasing(_df['close'])\n",
    "    _df.insert(63, 'decreasing', decreasing)\n",
    "    \n",
    "    # dm: The Directional Movement was developed by J. Welles Wilder in 1978 attempts to determine which direction the price of an asset is moving. It compares prior highs and lows to yield to two series +DM and -DM.\n",
    "    dm = ta.dm(_df['high'], _df['low'])\n",
    "    _df.insert(64, 'dm_positive', dm.iloc[:,0])\n",
    "    _df.insert(65, 'dm_negative', dm.iloc[:,1])\n",
    "\n",
    "    # donchian: Donchian Channels are used to measure volatility, similar to Bollinger Bands and Keltner Channels.\n",
    "    donchian = ta.donchian(_df['high'], _df['low'])\n",
    "    _df.insert(66, 'donchian_lower', donchian.iloc[:,0])\n",
    "    _df.insert(67, 'donchian_mid', donchian.iloc[:,1])\n",
    "    _df.insert(68, 'donchian_upper', donchian.iloc[:,2])\n",
    "    \n",
    "    # ebsw: This indicator measures market cycles and uses a low pass filter to remove noise. Its output is bound signal between -1 and 1 and the maximum length of a detected trend is limited by its length input.\n",
    "    ebsw = ta.ebsw(_df['close'])\n",
    "    _df.insert(69, 'ebsw', ebsw)\n",
    "    \n",
    "    # efi: Elder's Force Index measures the power behind a price movement using price and volume as well as potential reversals and price corrections.\n",
    "    efi = ta.efi(_df['close'], _df['volume'])\n",
    "    _df.insert(70, 'efi', efi)\n",
    "    \n",
    "    # entropy: Introduced by Claude Shannon in 1948, entropy measures the unpredictability of the data, or equivalently, of its average information. A die has higher entropy (p=1/6) versus a coin (p=1/2).\n",
    "    entropy = ta.entropy(_df['close'])\n",
    "    _df.insert(71, 'entropy', entropy)\n",
    "\n",
    "    #### we can add more technical indicators if we want using the same process ####\n",
    "    \n",
    "    # remove the NaN values and return the new dataframe\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "# call the function on the selected dataframe\n",
    "# full_df = add_technical_indicators(df.copy(deep=True))\n",
    "# full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea32a55",
   "metadata": {},
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c599f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a deep copy of the data\n",
    "# full_df = add_technical_indicators(df.copy(deep=True))\n",
    "\n",
    "\n",
    "# create a function to apply a given scaler to the features\n",
    "def apply_scaler(scaler, features):\n",
    "    \n",
    "    # set the training and test ratio to be 70-30\n",
    "    training_ratio = int(len(features) * 0.7)\n",
    "\n",
    "    # devide the feature set into training and test set\n",
    "    X_train, X_test = features[:training_ratio], features[training_ratio:]\n",
    "    \n",
    "    # apply a scaler on the training and test sets in isolation so we don't allow the test set to influence the scaling process, which reduces the likelihood of overfitting \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # concat the two scaled sets into one\n",
    "    X = np.concatenate((X_train_scaled, X_test_scaled), axis=0)\n",
    "\n",
    "    # return the scaled features\n",
    "    return X\n",
    "\n",
    "# source of isnpiration: https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4 [13]\n",
    "# create a function to reshape X and y into sequences of x timesteps\n",
    "def create_seqs(features, target, num_rows):\n",
    "    # create 2 empty lists to store the newly shaped features and target lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate over the features\n",
    "    for i in range(len(features) - num_rows):\n",
    "        # create indexes of the start and end of each sequence\n",
    "        seq_s = i\n",
    "        seq_e = i + num_rows\n",
    "        \n",
    "        # the ith sequence will be a slice of the features between the indexes, create it and add it to X\n",
    "        xi = features[seq_s : seq_e]\n",
    "        X.append(xi)\n",
    "        \n",
    "        # do the same for the target and add it to y\n",
    "        yi = target[seq_e]\n",
    "        y.append(yi)\n",
    "    \n",
    "    # return the X and y as numpy arraies\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# create a function to convert a dataframe into training and test sets\n",
    "def create_train_test_sets(_df, scaler, target=\"classification\", timesteps=6):\n",
    "\n",
    "    # reset the index\n",
    "    _df.reset_index(inplace = True)\n",
    "    \n",
    "    # drop the Date column as it's not necessary for now\n",
    "    _df.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "    # set the features set\n",
    "    X = _df.iloc[:, :-2]\n",
    "    \n",
    "    # set the target \n",
    "    if (target == \"classification\"):\n",
    "        # trend is the target for classification\n",
    "        y = _df.iloc[:, -1]\n",
    "    else:\n",
    "        # next_close is the target for regression\n",
    "        y = _df.iloc[:, -2]\n",
    "\n",
    "    # apply a scaler on the features set\n",
    "    X = apply_scaler(scaler, X)\n",
    "    \n",
    "    # create sequences\n",
    "    X_seq, y_seq = create_seqs(X, y, timesteps)\n",
    "    \n",
    "    # source of inspiration: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical [14]\n",
    "    # use to_categorical from tf to converts the target (trend) to binary class matrix, this will help us assign confidences to the classification prediction\n",
    "    if (target == \"classification\"):\n",
    "        y_seq = to_categorical(y_seq)\n",
    "\n",
    "    # devide the data into a training set and a test set in 70-30 ratio\n",
    "    training_ratio = int(len(X) * 0.7)\n",
    "    # add a vaidation ratio at 20% of the data, this will leave 10% as test\n",
    "    validation_ratio = int(len(X) * 0.2)\n",
    "    \n",
    "    X_train, X_vald, X_test = X_seq[:training_ratio], X_seq[training_ratio:training_ratio + validation_ratio], X_seq[training_ratio + validation_ratio:]\n",
    "    y_train, y_vald, y_test = y_seq[:training_ratio], y_seq[training_ratio:training_ratio + validation_ratio], y_seq[training_ratio + validation_ratio:]\n",
    "\n",
    "    # return the sets and the last_date\n",
    "    return X_train, X_vald, X_test, y_train, y_vald, y_test\n",
    "\n",
    "\n",
    "\n",
    "# initialize a MinMaxScaler instance for a range between 0 and 1\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# initialize a StandardScaler instance\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# initialize a RobustScaler instance\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = create_train_test_sets(full_df, robust_scaler, \"classification\", 6)\n",
    "\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec2b17",
   "metadata": {},
   "source": [
    "# Apply the helper functions on all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a515e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dict of dataframes, and return a dict of training, validation and testing datasets\n",
    "def prepare_data_to_train(dfs_dict, scaler, target, timesteps):\n",
    "    \n",
    "    # create a dict of dicts to store training, validation and test sets for each stock\n",
    "    sets_dict = {}\n",
    "    \n",
    "    for symbol in dfs_dict.keys():\n",
    "        # add target columns to all the data frames\n",
    "        _df = add_targets(dfs_dict[symbol].copy(deep=True))\n",
    "        \n",
    "        # add technical indicators on the selected dataframe\n",
    "        _df = add_technical_indicators(_df)\n",
    "        \n",
    "        # convert the dataframe into training, validation and test sets\n",
    "        X_train, X_vald, X_test, y_train, y_vald, y_test = create_train_test_sets(_df, scaler, target, timesteps)\n",
    "        \n",
    "        # create a dict of the sets and add it to the sets_dict\n",
    "        sets_dict[symbol] = {\n",
    "            'X_train': X_train, 'X_vald': X_vald, 'X_test': X_test, \n",
    "            'y_train': y_train, 'y_vald': y_vald, 'y_test': y_test\n",
    "        }\n",
    "    \n",
    "    # return the sets\n",
    "    return sets_dict\n",
    "\n",
    "# set a list of options for the timesteps\n",
    "timesteps_options = list(range(4, 11))\n",
    "\n",
    "# create a dict to store the different timesteps datasets\n",
    "timesteps_options_dict = {}\n",
    "\n",
    "# iterate over the different timesteps\n",
    "for timesteps in timesteps_options:\n",
    "    \n",
    "    # call prepare_data_to_train on each option \n",
    "    data_set = prepare_data_to_train(dfs, standard_scaler, \"classification\", timesteps)\n",
    "    \n",
    "    \n",
    "    \n",
    "    timesteps_options_dict[timesteps] = data_set\n",
    "\n",
    "# timesteps_options_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd9b725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335, 10, 72)\n",
      "(95, 10, 72)\n",
      "(39, 10, 72)\n"
     ]
    }
   ],
   "source": [
    "print(timesteps_options_dict[10]['PFE']['X_train'].shape)\n",
    "print(timesteps_options_dict[10]['PFE']['X_vald'].shape)\n",
    "print(timesteps_options_dict[10]['PFE']['X_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95813c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a881ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - 15ms/step - loss: 1.4369 - precision: 0.4545\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,768</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_34 (\u001b[38;5;33mSimpleRNN\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │           \u001b[38;5;34m8,768\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_35 (\u001b[38;5;33mSimpleRNN\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,464</span> (201.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,464\u001b[0m (201.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,154</span> (67.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,154\u001b[0m (67.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,310</span> (134.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m34,310\u001b[0m (134.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create and train the baseline classification model\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_model(_timesteps, X_train_shape):\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    # input layer\n",
    "    model.add(Input(shape=(_timesteps, X_train_shape[2])))\n",
    "    \n",
    "    model.add(SimpleRNN(64, return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['precision', 'accuracy', 'recall'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# setup the data to be passed to the model\n",
    "X_train, y_train = timesteps_options_dict[5]['PFE']['X_train'], timesteps_options_dict[5]['PFE']['y_train']\n",
    "X_vald, y_vald = timesteps_options_dict[5]['PFE']['X_vald'], timesteps_options_dict[5]['PFE']['y_vald']\n",
    "X_test, y_test = timesteps_options_dict[5]['PFE']['X_test'], timesteps_options_dict[5]['PFE']['y_test']\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_model(5, X_train.shape)\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X_train, y_train, validation_data=(X_vald, y_vald), epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "\n",
    "## Model evaluation and prototype conclusion\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# list the model architecture\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db9e83",
   "metadata": {},
   "source": [
    "# create models archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90235ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - 13ms/step - loss: 0.9822 - precision: 0.5349\n",
      "2/2 - 1s - 261ms/step - loss: 1.0006 - precision: 0.5581\n",
      "2/2 - 0s - 183ms/step - loss: 1.0127 - precision: 0.5349\n",
      "2/2 - 1s - 265ms/step - loss: 1.3909 - precision: 0.3953\n",
      "2/2 - 1s - 291ms/step - loss: 0.9874 - precision: 0.4884\n"
     ]
    }
   ],
   "source": [
    "# create a function that takes a model creation function, dictionary of datasets as inputs, \n",
    "# and train the given model on these datasets one by one, then save the models to a dictionary where keys are stock symbols, \n",
    "# and values are dictionaries containing the models and meta data about the models\n",
    "def models_archive(_create_model, _dataset_dict, _timesteps, model_name):\n",
    "    \n",
    "    # create the models archive dictionary\n",
    "    archive = {}\n",
    "    \n",
    "    # get the datasets associated with the given timesteps \n",
    "    dataset_timestep = _dataset_dict[_timesteps]\n",
    "    \n",
    "    # iterate over the symbols in the dictionary\n",
    "    for symbol in dataset_timestep.keys():\n",
    "        \n",
    "        # initiate a dict for the symbol\n",
    "        archive[symbol] = {}\n",
    "        \n",
    "        # setup the data to be passed to the model\n",
    "        X_train, y_train = dataset_timestep[symbol]['X_train'], dataset_timestep[symbol]['y_train']\n",
    "        X_vald, y_vald = dataset_timestep[symbol]['X_vald'], dataset_timestep[symbol]['y_vald']\n",
    "        X_test, y_test = dataset_timestep[symbol]['X_test'], dataset_timestep[symbol]['y_test']\n",
    "\n",
    "        # initialize the model\n",
    "        model = _create_model(_timesteps, X_train.shape)\n",
    "\n",
    "        # train the model\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_vald, y_vald), epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "        # source of inspiration: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "        # save model to device\n",
    "        model.save(f'models/{model_name}_{symbol}.keras')        \n",
    "        \n",
    "        # store the model in the associated symbol dict\n",
    "        archive[symbol]['model'] = load_model(f'models/{model_name}_{symbol}.keras')\n",
    "        \n",
    "        # evaluate the model on the test_set and store it in the associated symbol dict\n",
    "        archive[symbol]['evaluation'] = model.evaluate(X_test, y_test, verbose=2)  \n",
    "        \n",
    "    return archive\n",
    "\n",
    "    \n",
    "stocks_model_archive = models_archive(create_model, timesteps_options_dict, 6, \"baseline_SimpleRNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dcbba",
   "metadata": {},
   "source": [
    "# create model evaluation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec8d420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5023255944252014"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function that takes a model archive (a dict of models) and calculate the average precision for all the models in it\n",
    "def evaluate_models_archive(_models_archive):\n",
    "    \n",
    "    # create a total precision variable and initialize it to 0\n",
    "    total_precision = 0\n",
    "    \n",
    "    # iterate over the symbols of the dictionary\n",
    "    for symbol in _models_archive.keys():\n",
    "        total_precision += _models_archive[symbol]['evaluation'][1]\n",
    "        \n",
    "    # calculate average precision\n",
    "    average_precision = total_precision / len(_models_archive.keys())\n",
    "    \n",
    "    return average_precision\n",
    "        \n",
    "evaluate_models_archive(stocks_model_archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c6f7e",
   "metadata": {},
   "source": [
    "# Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38f804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
