{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23b6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2283\n"
     ]
    }
   ],
   "source": [
    "# source: https://stackoverflow.com/questions/71194571/word-count-of-markdown-cells-in-jupyter-notebook\n",
    "import json\n",
    "\n",
    "with open('Project_implementation_2.ipynb', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "wordCount = 0\n",
    "for each in data['cells']:\n",
    "    cellType = each['cell_type']\n",
    "    if cellType == \"markdown\":\n",
    "        content = each['source']\n",
    "        for line in content:\n",
    "            temp = [word for word in line.split() if \"#\" not in word] # we might need to filter for more markdown keywords here\n",
    "            wordCount = wordCount + len(temp)\n",
    "            \n",
    "print(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0c6d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Project', 'Implementation', '##', 'Install', 'and', 'import', 'the', 'required', 'libraries', '<br>', 'In', 'the', 'implementation', 'part,', 'we', 'will', 'start', 'by', 'importing', 'the', 'required', 'libraries', 'for', 'our', 'work.', 'We', 'will', 'work', 'mainly', 'with', 'yfinance', 'for', 'data', 'collection,', 'Pandas', 'and', 'Numpy', 'for', 'data', 'processing,', 'and', 'TensorFlow', 'for', 'machine', 'learning.', '<br>', 'Other', 'relevant', 'libraries', 'are', 'keras_tuner', 'for', 'hyperparameter', 'optimization,', 'scikit-learn', 'for', 'data', 'scaling', 'and', 'model', 'evaluation,', 'pandas-ta', 'for', 'calculating', 'technical', 'indicators', 'based', 'on', 'the', 'data', 'from', 'yfinance,', 'and', 'matplotlib', 'for', 'visualization.', '##', 'Load', 'Data', '<br>', 'In', 'this', 'implementation,', 'we', 'will', 'work', 'with', '5', 'different', 'stocks', 'from', 'the', 'S&P500(1)', 'list.', 'The', '5', 'stocks', 'we', 'will', 'work', 'with', 'are', 'chosen', 'based', 'on', 'their', 'ranking', 'in', 'this', 'list', 'from', 'most', 'valuable', 'to', 'least', 'valuable,', 'and', 'each', 'one', 'is', 'relatively', 'distant', 'from', 'the', 'other', 'and', 'belongs', 'to', 'a', 'different', 'industry.', 'This', 'will', 'ensure', 'a', 'diverse', 'sample', 'and', 'that', 'our', 'model', 'evaluation', 'results', 'generalize', 'relatively', 'well,', 'reducing', 'the', 'possibility', 'of', 'bias', 'and', 'overfitting.', 'Check', 'out', 'our', 'stock', 'list', 'for', 'this', 'project', '(2).', '<br>', 'The', 'yfinance', 'API', 'allows', 'us', 'to', 'request', 'the', 'stock', 'data', 'for', 'a', \"company's\", 'given', 'period', 'and', 'interval', 'values.', 'For', 'the', 'period', 'value,', 'we', 'will', 'set', 'it', 'to', '10', 'years', 'or', 'max', 'value', 'which', 'will', 'be', 'sufficient', 'for', 'all', 'of', 'our', 'experiments,', 'for', 'the', 'interval', 'value', 'however,', 'which', 'determines', 'the', 'frequency', 'of', 'the', 'data', 'rows,', 'we', 'will', 'experiment', 'with', 'many', 'options', 'to', 'see', 'if', 'our', 'approach', 'generalizes', 'better', 'with', 'specific', 'interval', 'values', 'as', 'different', 'intervals', 'are', 'relevant', 'to', 'other', 'groups', 'of', 'financial', 'analysts', 'and', 'traders', 'in', 'the', 'real', 'world,', 'therefore', 'we', 'must', 'try', 'to', 'create', 'the', 'best', 'model', 'relevant', 'to', 'each', 'of', 'these', 'groups.', \"That's\", 'why', 'we', 'will', 'define', 'a', 'function', 'that', 'allows', 'us', 'to', 'download', 'any', 'number', 'of', 'stock', 'data', 'at', 'any', 'period', 'or', 'interval,', 'save', 'the', 'data', 'as', 'a', 'CSV', 'file', 'to', 'local', 'storage,', 'load', 'it', 'from', 'storage,', 'split', 'it', 'into', 'different', 'data', 'frames', 'based', 'on', 'the', 'stock,', 'and', 'organize', 'the', 'data', 'frames', 'in', 'a', 'dictionary', 'so', \"it's\", 'easy', 'to', 'work', 'with', 'for', 'the', 'rest', 'of', 'the', 'project.', 'Check', 'out', 'the', 'loadData', 'function', '(3).', 'Load', 'the', 'data', 'of', 'the', '5', 'selected', 'stocks', 'for', 'the', 'last', '10', 'years', 'on', 'a', 'weekly', 'intrevals.', '##', 'Perform', 'simple', 'exploritory', 'data', 'analysis', '<br>', 'Now', 'that', 'we', 'have', 'a', 'dictionary', 'of', 'dataframes,', 'we', 'can', 'analyze', 'the', 'data', 'and', 'make', 'some', 'observations.', '1.', 'We', 'can', 'get', 'the', 'shape', 'of', 'the', 'data', 'for', 'any', 'stock', '2.', 'We', 'can', 'get', 'the', 'basic', 'stats', 'for', 'any', 'stock', '3.', 'We', 'can', 'check', 'how', 'many', 'missing', 'values', 'each', 'column', 'have', 'for', 'a', 'any', 'stock', 'dataframe', '####', 'Columns', 'breakdown', '<br>', 'Date:', 'The', 'index', 'is', 'the', 'date', 'on', 'which', 'the', 'information', 'on', 'the', 'rest', 'of', 'the', 'columns', 'takes', 'place.', '<br>', 'Adjusted', 'close:', 'is', 'the', 'closing', 'price', 'after', 'adjustments', 'for', 'all', 'applicable', 'splits', 'and', 'dividend', 'distributions', 'which', 'represents', 'the', 'true', 'closing', 'price.', '<br>', 'Close:', 'is', 'the', 'historical', 'closing', 'price', 'of', 'the', 'stock.', '<br>', 'High:', 'is', 'the', 'highest', 'point', 'a', 'stock', 'has', 'reached.', '<br>', 'Low:', 'is', 'the', 'lowest', 'point', 'of', 'a', 'stock.', '<br>', 'Open:', 'the', 'opening', 'price', 'of', 'the', 'stock.', '<br>', 'Volume:', 'the', 'volume', 'of', 'stocks', 'traded', 'in', 'that', 'timeframe.', '<br>', 'Usually', 'for', 'this', 'type', 'of', 'model', 'we', 'would', 'only', 'keep', 'either', 'Adjusted', 'close', 'or', 'close,', 'we', 'are', 'going', 'to', 'keep', 'the', 'adjusted', 'close', 'for', 'now', 'but', 'it', 'worth', 'mentiong', 'that', 'most', 'of', 'the', 'technical', 'indicators', 'we', 'are', 'utilizing', 'are', 'dependent', 'on', 'the', 'none', 'adjusted', 'close.', '##', 'Adding', 'Targets', '<br>', 'To', 'predict', 'stock', 'trends', 'based', 'on', 'past', 'data,', \"we'll\", 'create', 'two', 'new', 'columns:', '-', \"'next_close':\", 'Represents', 'the', 'next', 'closing', 'price,', 'serving', 'as', 'the', 'target', 'for', 'the', 'regression', 'model.', '-', \"'trend':\", 'Indicates', 'whether', 'the', 'next', 'close', 'is', 'higher', \"'1'\", 'or', 'lower', \"'0'\", 'than', 'the', 'current', 'close,', 'serving', 'as', 'the', 'target', 'for', 'the', 'classification', 'model.', 'So', 'we', 'will', 'train', 'the', 'model', 'on', 'to', 'the', 'current', 'closing', 'price', 'and', 'make', 'it', 'predict', 'the', 'next', 'closing', 'price/trend', 'for', 'any', 'given', 'timestep.', '<br>', 'To', 'do', 'that', 'we', 'define', 'the', 'add_targets(4)', 'function', 'which', 'takes', 'a', 'data', 'frame', 'as', 'input,', 'adds', 'the', \"'next_close'\", 'and', \"'trend'\", 'columns', 'to', 'it,', 'and', 'returns', 'it', 'as', 'an', 'output.', '##', 'Feature', 'Engineering', '<br>', 'Adding', 'indicators', 'to', 'the', 'dataframe', 'is', 'important', 'for', 'enhancing', 'model', 'performance', 'and', 'accuracy.', '<br>', 'We', 'can', 'either', 'manually', 'calculate', 'technical', 'indicators,', 'which', 'is', 'time-consuming', 'and', 'prone', 'to', 'errors,', 'or', 'we', 'can', 'utilize', 'an', 'existing', 'library', 'designed', 'for', 'this', 'purpose.', 'pandas-ta', 'is', 'a', 'library', 'that', 'includes', 'a', 'wide', 'set', 'of', 'technical', 'indicators', 'and', 'is', 'designed', 'to', 'work', 'seamlessly', 'with', 'pandas', 'dataframes.', 'To', 'explore', 'the', 'available', 'indicators', 'in', 'pandas-ta,', 'you', 'can', 'use', 'the', 'following:', 'For', 'this', 'project,', 'we', 'added', 'a', 'total', 'of', '66', 'technical', 'indicators.', 'Each', 'feature', 'is', 'carefully', 'selected', 'based', 'on', 'the', 'technical', \"indicator's\", 'definition', 'and', 'description.', 'Check', 'the', 'full', 'list', 'of', 'the', 'selected', 'indicators', 'and', 'the', 'implementation', 'of', 'the', 'add_technical_indicators', 'function', 'which', 'take', 'a', 'dataframe', 'as', 'input', 'and', 'add', 'these', 'indicators', 'to', 'it', '(5).', 'We', 'can', 'also', 'get', 'detailed', 'information', 'on', 'specific', 'indicators:', 'Then,', 'weâ€™ll', 'group', 'the', 'features', 'into', 'four', 'categories:', '1.', 'Base', 'Features:', 'Original', 'features', 'from', 'yfinance', '(6', 'features).', '2.', 'Technical', 'Indicators', 'based', 'on', 'Closing', 'Price:', '(30', 'features).', '3.', 'Technical', 'Indicators', 'based', 'on', 'Highs', 'and', 'Lows:', '(31', 'features).', '4.', 'Technical', 'Indicators', 'based', 'on', 'Volume:', '(5', 'features).', 'This', 'grouping', 'will', 'enable', 'us', 'to', 'create', 'more', 'sophisticated', 'models,', 'such', 'as', 'multi-output', 'or', 'inception', 'models,', 'which', 'we', 'will', 'explore', 'later.', 'Finally', 'we', 'will', 'create', 'add_targets_and_indicators', '(6),', 'a', 'helper', 'functions', 'to', 'add', 'the', 'targets', 'and', 'indicators', 'to', 'all', 'dataframes', 'in', 'a', 'dictionary.', '##', 'Data', 'balance', 'and', 'common', 'sense', 'baseline', '<br>', 'With', 'the', 'target', 'column', 'added,', 'we', 'can', 'now', 'assess', 'the', 'balance', 'of', 'our', 'dataset', 'from', 'a', 'classification', 'perspective.', 'An', 'unbalanced', 'dataset', 'may', 'skew', 'the', \"model's\", 'predictions.', 'The', 'trend', 'column', 'indicates', 'whether', 'the', 'stock', 'will', 'rise', 'or', 'fall.', 'By', 'calculating', 'the', 'ratio', 'of', '(trend', '=', '1)', 'to', 'the', 'total', 'number', 'of', 'samples,', 'we', 'can', 'evaluate', 'the', 'data', 'balance.', 'To', 'acomplish', 'that,', \"we'll\", 'create', 'calculate_data_balance', '(6),', 'a', 'function', 'that', 'computes', 'the', 'ratio', 'of', 'trend', '=', '1', 'for', 'each', 'individual', 'dataframe', 'in', 'our', 'dictionary', 'and', 'the', 'overall', 'ratio', 'across', 'all', 'dataframes.', 'Based', 'on', 'these', 'results,', 'the', 'data', 'appears', 'well-balanced.', '####', 'Common', 'Sense', 'Baseline', 'Establishing', 'a', 'common', 'sense', 'baseline', 'is', 'crucial', 'in', 'machine', 'learning', 'to', 'ensure', 'that', 'our', 'model', 'performs', 'at', 'least', 'as', 'well', 'as', 'basic', 'logical', 'assumptions.', 'In', 'stock', 'prediction,', 'a', 'reasonable', 'baseline', 'assumes', 'that', 'the', 'future', 'trend', 'will', 'mirror', 'the', 'current', 'trend;', 'if', 'a', 'stock', 'is', 'rising,', 'itâ€™s', 'expected', 'to', 'continue', 'rising,', 'and', 'vice', 'versa.', 'This', 'baseline', 'is', 'realistic', 'and', 'aligns', 'with', 'principles', 'from', 'behavioral', 'finance[12],', 'however,', \"it's\", 'outside', 'the', 'scope', 'of', 'this', 'project.', 'To', 'acheive', 'that', 'we', 'will', 'create', 'calculate_common_sense_baseline', '(7),', 'a', 'function', 'that', 'calculates', 'this', 'common', 'sense', 'score', 'for', 'each', 'stock', 'in', 'the', 'dictionary,', 'as', 'well', 'as', 'the', 'average', 'overall', 'score.', '##', 'Prepare', 'the', 'data', 'for', 'training', '####', 'Features', 'scaling', 'To', 'prepare', 'the', 'data', 'for', 'training,', 'we', 'first', 'need', 'to', 'scale', 'the', 'features', 'to', 'a', 'specific', 'range.', 'This', 'scaling', 'process', 'enhances', 'the', \"model's\", 'ability', 'to', 'learn', 'patterns', 'effectively', 'and', 'efficiently.', \"We'll\", 'test', 'three', 'different', 'scalers', 'from', 'the', 'scikit-learn', 'library:', 'MinMaxScaler,', 'StandardScaler,', 'and', 'RobustScaler.', '1.', 'MinMaxScaler', 'Transform', 'features', 'by', 'scaling', 'each', 'feature', 'to', 'a', 'given', 'range.', '(0-1', 'in', 'our', 'case)', '[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html]', '2.', 'StandardScaler', 'Standardize', 'features', 'by', 'removing', 'the', 'mean', 'and', 'scaling', 'to', 'unit', 'variance.', '[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html]', '3.', 'RobustScaler', 'Scale', 'features', 'using', 'statistics', 'that', 'are', 'robust', 'to', 'outliers', 'by', 'removing', 'the', 'median', 'and', 'scaling', 'the', 'data', 'according', 'to', 'the', 'quantile', 'range.', '[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html]', 'Given', 'the', 'continuous', 'pattern', 'shifts', 'in', 'financial', 'data,', 'no', 'single', 'scaler', 'is', 'guaranteed', 'to', 'consistently', 'outperform', 'the', 'others.', 'Therefore,', \"we'll\", 'implement', 'apply_scaler(8),', 'a', 'function', 'that', 'applies', 'any', 'given', 'scaler', 'to', 'a', 'set', 'of', 'features.', 'This', 'approach', 'allows', 'us', 'to', 'experiment', 'with', 'different', 'scalers', 'to', 'achieve', 'the', 'best', 'improvement', 'to', 'model', 'performance.', \"It's\", 'important', 'to', 'note', 'that', 'apply_scaler', 'scales', 'the', 'training', 'set', 'independently', 'from', 'the', 'test', 'set', 'to', 'prevent', 'data', 'leakage,', 'thereby', 'reducing', 'the', 'risk', 'of', 'overfitting.', '####', 'Data', 'Sequences', 'To', 'predict', 'the', 'stock', 'trend', 'for', 'the', 'next', 'timestep,', 'we', 'look', 'back', 'at', 'the', 'last', 'x', 'timesteps', 'and', 'make', 'a', 'prediction', 'based', 'on', 'that', 'sequence.', 'This', 'requires', 'reshaping', 'the', 'data', 'into', 'sequences', 'of', 'x', 'timesteps,', 'a', 'necessary', 'step', 'for', 'RNNs,', 'which', 'require', 'input', 'data', 'in', 'the', 'shape', 'of', '(samples,', 'timesteps,', 'and', 'number', 'of', 'features).', 'We', 'achieve', 'this', 'by', 'using', 'the', 'create_seqs(9)', 'function,', 'which', 'takes', 'features', 'and', 'the', 'number', 'of', 'timesteps', 'of', 'each', 'sequence', 'and', 'then', 'reshapes', 'the', 'data', 'based', 'on', 'the', 'given', 'timesteps.', '####', 'Train-Validation-Test', 'Split', \"We'll\", 'split', 'the', 'data', 'into', '70%', 'for', 'training,', '20%', 'for', 'validation,', 'and', '10%', 'for', 'testing.', 'This', '70-20-10', 'split', 'maximizes', 'model', 'accuracy', 'while', 'reducing', 'the', 'likelihood', 'of', 'overfitting', 'that', 'might', 'occur', 'with', 'different', 'ratios.', 'create_train_vald_test_sets(11)', 'function', 'is', 'where', 'all', 'these', 'three', 'processes', 'will', 'be', 'applied', 'in', 'order', 'to', 'the', 'data,', 'making', 'it', 'ready', 'for', 'training.', 'To', 'apply', 'these', 'steps', 'on', 'each', 'dataframe', 'in', 'the', 'dictionary', 'we', 'will', 'use', 'the', 'prepare_data_to_train(12),', 'a', 'function', 'that', 'converts', 'the', 'dictionary', 'of', 'dataframes', 'into', 'a', 'dictionary', 'of', 'train-validation-test', 'sets.', '##', 'Models', 'archive', 'For', 'this', 'section,', 'we', 'created', 'create_models_archive(13),', 'a', 'function', 'to', 'train', 'and', 'save', 'individual', 'models', 'for', 'each', 'dataset', 'within', 'a', 'dictionary.', 'This', 'setup', 'is', 'crucial', 'for', 'scenarios', 'where', 'multiple', 'stock', 'choices', 'are', 'analyzed', 'and', 'assessed', 'simultaneously.', 'The', 'function', 'accepts', 'a', 'dictionary', 'of', 'datasets', 'and', 'a', 'model', 'creation', 'function', 'or', 'class', 'instance,', 'then', 'trains', 'the', 'model', 'on', 'each', 'dataset', 'separately.', 'It', 'stores', 'the', 'trained', 'models,', 'evaluation', 'data,', 'and', 'hyperparameters', '(if', 'applicable)', 'in', 'a', 'dictionary', 'associated', 'with', 'each', 'stock.', 'This', 'implementation', 'supports', 'classification', 'and', 'regression-type', 'models,', 'so', 'we', 'can', 'easily', 'switch', 'between', 'the', 'two', 'prediction', 'approaches.', 'Additionally,', 'this', 'function', 'incorporates', 'EarlyStopping[]', 'and', 'ReduceLROnPlateau[]', 'callbacks', 'to', 'enhance', 'the', 'training', 'process.', 'These', 'callbacks', 'help', 'prevent', 'overfitting', 'by', 'stopping', 'training', 'early', 'if', 'the', \"model's\", 'performance', 'stops', 'improving', 'and', 'reducing', 'the', 'learning', 'rate', 'when', 'necessary.', '[source:', '7.2.', 'Inspecting', 'and', 'monitoring', 'deep-learning', 'models', 'using', 'Keras', 'callba-', 'acks', 'and', 'TensorBoard,', 'source:', 'EarlyStopping,', 'https://keras.io/api/callbacks/early_stopping/]', '[source:', 'ReduceLROnPlateau,', 'https://keras.io/api/callbacks/reduce_lr_on_plateau/]', 'Optionally,', 'it', 'enables', 'hyperparameter', 'optimization', 'during', 'training,', 'so', 'if', 'a', 'hyperparameter', 'tuner', 'is', 'passed,', 'the', 'function', 'will', 'fine-tune', 'the', \"model's\", 'hyperparameters', 'to', 'achieve', 'the', 'best', 'possible', 'performance', 'before', 'saving', 'the', 'final', 'model.', '##', 'Models', 'Evaluation', 'To', 'evaluate', 'the', 'models', 'in', 'the', 'created', 'models', 'archive', 'we', 'will', 'create', 'evaluate_models_archive(14),', 'a', 'function', 'that', 'takes', 'the', 'model', 'archive', 'and', 'for', 'each', 'model', 'in', 'it:', '1.', 'Calculate', 'and', 'print', 'accuracy,', 'precision,', 'recall,', 'F-score,', 'and', 'any', 'other', 'relevant', 'evaluation', 'metric.', '2.', 'Create', 'a', 'confusion', 'matrix.', '3.', 'Plot', 'Training', 'vs.', 'validation', 'loss', 'to', 'assess', 'model', 'performance', 'over', 'epochs.', 'Finally,', 'the', 'function', 'calculates', 'and', 'prints', 'the', 'average', 'metrics', 'across', 'all', 'models.', '##', 'Machine', 'Learning', 'Models', 'and', 'RNNs', 'For', 'this', 'type', 'of', 'problem,', 'we', \"can't\", 'simply', 'rely', 'on', 'feedforward', 'models', 'such', 'as', 'densely', 'connected', 'networks', 'or', 'convolutional', 'networks,', 'since', 'they', 'are', 'designed', 'to', 'process', 'each', 'input', 'independently', 'without', 'retaining', 'the', 'state', 'between', 'inputs', 'which', 'is', 'suitable', 'when', 'dealing', 'with', 'discrete', 'data.', 'However,', 'when', 'dealing', 'with', 'continuous', 'data', 'such', 'as', 'stock', 'data,', 'handling', 'them', 'as', 'sequences', 'would', 'be', 'more', 'appropriate', 'and', 'this', 'is', 'where', 'RNNs', 'come', 'into', 'play.', 'RNNs', '(recurrent', 'neural', 'networks)', 'were', 'designed', 'specifically', 'to', 'process', 'continuous', 'data', 'such', 'as', 'time', 'series.', 'As', 'they', 'can', 'retain', 'information', 'over', 'time', 'allows', 'them', 'to', 'identify', 'patterns,', 'making', 'them', 'powerful', 'tools', 'for', 'forecasting.', 'For', 'this', 'project,', 'we', 'will', 'consider', 'the', 'three', 'available', 'types', 'of', 'RNNS', 'in', 'TensorFlow:', 'SimpleRNN:', 'The', 'basic', 'RNN,', 'processes', 'sequences', 'by', 'maintaining', 'a', 'hidden', 'state', 'that', 'is', 'updated', 'at', 'each', 'timestep.', 'However,', 'it', 'suffers', 'from', 'the', 'vanishing', 'gradient', 'problem,', 'as', 'adding', 'more', 'layers,', 'the', 'network', 'eventually', 'becomes', 'untrainable.', 'LSTM', '(Long', 'Short-Term', 'Memory):', 'an', 'improvement', 'over', 'SimpleRNN', 'as', 'it', 'addresses', 'the', 'vanishing', 'gradient', 'issue', 'by', 'incorporating', 'three', 'gates', '(input,', 'forget,', 'and', 'output).', 'These', 'gates', 'regulate', 'the', 'flow', 'of', 'information,', 'allowing', 'LSTMs', 'to', 'capture', 'long-term', 'dependencies', 'effectively.', 'However,', 'LSTM', 'is', 'more', 'computationally', 'expensive', 'than', 'SimpleRNN.', 'GRU', '(Gated', 'Recurrent', 'Unit):', 'a', 'simplified', 'implementation', 'of', 'LSTMs', 'with', 'only', 'two', 'gates', '(reset', 'and', 'update).', 'They', 'are', 'computationally', 'more', 'efficient', 'while', 'still', 'theoretically', 'countering', 'the', 'vanishing', 'gradient', 'problem.', '[][][]', '[deep', 'learning', 'with', 'python,', 'chapter', '6,', '6.2.', 'Understanding', 'recurrent', 'neural', 'networks]', '[What', 'is', 'LSTM', 'â€“', 'Long', 'Short', 'Term', 'Memory?,', 'https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/]', '[Gated', 'Recurrent', 'Unit', 'Networks,', 'https://www.geeksforgeeks.org/gated-recurrent-unit-networks/]', 'RNNModel(15)', 'is', 'a', 'class', 'we', 'created', 'that', 'enables', 'us', 'to', 'instantiate', 'a', 'classification', 'or', 'regression', 'model,', 'where', 'the', 'layers', 'consist', 'of', 'either', 'SimpleRNN,', 'LSTM,', 'or', 'GRU.', 'RNNModel', 'will', 'allow', 'us', 'to', 'create', 'a', 'compare', 'a', 'wide', 'range', 'of', 'models', 'easily', 'and', 'fairly.', '##', 'SimpleRNN', 'vs', 'LSTM', 'vs', 'GRU', '###', 'SimpleRNN', '###', 'LSTM', '###', 'GRU', '##', 'Classification', 'vs', 'Regression', 'approach', 'As', 'we', 'stated', 'before', 'regression', 'models', 'could', 'be', 'more', 'beneficial', 'for', 'predicting', 'the', 'actual', 'future', 'closing', 'prices', 'in', 'the', 'stock', 'market', 'since', 'it', 'would', 'enable', 'us', 'to', 'perform', 'more', 'advanced', 'tasks', 'such', 'as', 'portfolio', 'optimization.', 'Therefore', 'before', 'proceeding', 'we', 'will', 'create', 'a', 'baseline', 'regression', 'model', 'and', 'compare', 'it', 'with', 'our', 'baseline', 'classification', 'model.', 'If', 'the', 'regression', 'model', 'sounds', 'promising,', 'we', 'will', 'be', 'improving', 'it', 'for', 'the', 'rest', 'of', 'the', 'project,', 'otherwise', 'will', 'proceed', 'with', 'the', 'classification', 'approach.', 'The', \"model's\", 'predictions', 'are,', 'on', 'average,', '16.5%', 'off,', 'which', 'is', 'significant', 'for', 'stock', 'prediction.', 'Hence,', 'the', 'regression', 'approach', 'is', 'unsuitable', 'with', 'the', 'current', 'data,', 'and', 'we', 'should', 'focus', 'on', 'the', 'classification', 'approach.', '##', 'BatchNormalization', 'BatchNormalization', 'is', 'a', 'tensorflow', 'layer', 'type', 'that', 'adaptively', 'normalize', 'data', 'as', 'the', 'mean', 'and', 'variance', 'changes', 'while', 'training.', 'It', 'helps', 'with', 'gradient', 'propagation', 'allowing', 'for', 'deeper', 'networks.', 'We', 'will', 'add', 'one', 'BatchNormalization', 'layer', 'after', 'each', 'RNN', 'layer', 'and', 'see', 'if', 'there', 'are', 'any', 'improvements.', 'Check', 'RNNModelNorm(16)', 'On', 'average', 'we', 'see', 'worst', 'performance.', '##', 'Hyperparameter', 'Optimization', 'We', 'implemented', 'hyperparameter', 'optimization', 'using', 'keras_tuner', 'library.', 'HP_RNNModel(17),', 'a', 'class', 'we', 'created,', 'inherates', 'from', \"keras_tuner's\", 'HyperModel', 'class.', 'HP_RNNModel', 'allows', 'us', 'to', 'optimize:', '1.', 'Model', 'hyperparameters:', '-', 'Number', 'of', 'RNN', 'layers', '(2-4)', '-', 'Layer', 'type', '(SimpleRNN,', 'LSTM,', 'GRU)', '-', 'Units', 'per', 'layer', '(64-128)', '-', 'Recurrent', 'dropout', '(0.0-0.5)', '2.', 'Algorithm', 'hyperparameters:', '-', 'Optimizer', 'type', '(Adam,', 'RMSprop,', 'SGD)', '-', 'Learning', 'rate', '(0.0001-0.01)', '3.', 'training', 'hyperparameters:', '-', 'Batch', 'size', '(32,', '64)', 'Hyperband[]', 'is', 'the', 'tuner', 'we', 'used', 'as', \"it's\", 'computationally', 'efficient,', 'making', 'it', 'suitable', 'for', 'tuning', 'multiple', 'models', 'quickly.', 'create_models_archive', 'was', 'created', 'to', 'be', 'compatible', 'with', 'this', 'approach', 'by', 'being', 'able', 'to', 'use', 'the', 'passed', 'tuner', 'for', 'optimization.', 'create_models_archive', 'also', 'optimizes', 'the', 'number', 'of', 'epochs.', '[https://www.tensorflow.org/tutorials/keras/keras_tuner]', '##', 'Inception', 'Model', 'Inception', 'models', 'are', 'designed', 'to', 'process', 'input', 'data', 'through', 'multiple', 'parallel', 'branches.', 'They', 'help', 'the', 'network', 'separately', 'learn', 'features', 'from', 'different', 'groups,', 'which', 'is', 'more', 'efficient', 'than', 'learning', 'them', 'jointly.', 'In', 'my', 'implementation:', '1.', 'I', 'created', 'an', 'InputSplitLayer(18)', 'a', 'custom', 'layer', 'to', 'divide', 'the', 'input', 'features', 'into', '4', 'groups.', '2.', 'Each', 'feature', 'group', 'is', 'passed', 'through', 'a', 'separate', 'HP_Hollow_RNN_Branch(19)', 'class', 'instance', 'which', 'is', 'similar', 'to', 'the', 'hp_RNN_model', 'but', 'without', 'an', 'output', 'layer.', '3.', 'These', 'branches', 'are', 'then', 'combined', 'in', 'the', 'HP_Hollow_RNN_Inception_Model,', 'which', 'merges', 'the', 'outputs', 'from', 'all', 'the', 'different', 'branches', 'before', 'passing', 'them', 'to', 'the', 'output', 'layer.', 'The', 'hyperparameter', 'optimization', 'process', 'within', 'HP_Hollow_RNN_Branch', 'ensures', 'the', 'best', 'possible', 'configurations', 'for', 'each', 'branch', 'in', 'the', 'inception', 'model.', '##', 'Different', 'data', 'intervals', '###', '1', 'Day', 'interval', '###', '1', 'Hour', 'interval', '###', '5', 'Minutes', 'intreval']\n"
     ]
    }
   ],
   "source": [
    "# source: https://stackoverflow.com/questions/71194571/word-count-of-markdown-cells-in-jupyter-notebook\n",
    "import json\n",
    "\n",
    "with open('Project_implementation_2.ipynb', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "wordCount = []\n",
    "for each in data['cells']:\n",
    "    cellType = each['cell_type']\n",
    "    if cellType == \"markdown\":\n",
    "        content = each['source']\n",
    "        for line in content:\n",
    "            temp = [word for word in line.split()] # we might need to filter for more markdown keywords here\n",
    "#             print(temp)\n",
    "            wordCount = wordCount + temp\n",
    "            \n",
    "print(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e98be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Project Implementation ## Install and import the required libraries <br> In the implementation part, we will start by importing the required libraries for our work. We will work mainly with yfinance for data collection, Pandas and Numpy for data processing, and TensorFlow for machine learning. <br> Other relevant libraries are keras_tuner for hyperparameter optimization, scikit-learn for data scaling and model evaluation, pandas-ta for calculating technical indicators based on the data from yfinance, and matplotlib for visualization. ## Load Data <br> In this implementation, we will work with 5 different stocks from the S&P500(1) list. The 5 stocks we will work with are chosen based on their ranking in this list from most valuable to least valuable, and each one is relatively distant from the other and belongs to a different industry. This will ensure a diverse sample and that our model evaluation results generalize relatively well, reducing the possibility of bias and overfitting. Check out our stock list for this project (2). <br> The yfinance API allows us to request the stock data for a company's given period and interval values. For the period value, we will set it to 10 years or max value which will be sufficient for all of our experiments, for the interval value however, which determines the frequency of the data rows, we will experiment with many options to see if our approach generalizes better with specific interval values as different intervals are relevant to other groups of financial analysts and traders in the real world, therefore we must try to create the best model relevant to each of these groups. That's why we will define a function that allows us to download any number of stock data at any period or interval, save the data as a CSV file to local storage, load it from storage, split it into different data frames based on the stock, and organize the data frames in a dictionary so it's easy to work with for the rest of the project. Check out the loadData function (3). Load the data of the 5 selected stocks for the last 10 years on a weekly intrevals. ## Perform simple exploritory data analysis <br> Now that we have a dictionary of dataframes, we can analyze the data and make some observations. 1. We can get the shape of the data for any stock 2. We can get the basic stats for any stock 3. We can check how many missing values each column have for a any stock dataframe #### Columns breakdown <br> Date: The index is the date on which the information on the rest of the columns takes place. <br> Adjusted close: is the closing price after adjustments for all applicable splits and dividend distributions which represents the true closing price. <br> Close: is the historical closing price of the stock. <br> High: is the highest point a stock has reached. <br> Low: is the lowest point of a stock. <br> Open: the opening price of the stock. <br> Volume: the volume of stocks traded in that timeframe. <br> Usually for this type of model we would only keep either Adjusted close or close, we are going to keep the adjusted close for now but it worth mentiong that most of the technical indicators we are utilizing are dependent on the none adjusted close. ## Adding Targets <br> To predict stock trends based on past data, we'll create two new columns: - 'next_close': Represents the next closing price, serving as the target for the regression model. - 'trend': Indicates whether the next close is higher '1' or lower '0' than the current close, serving as the target for the classification model. So we will train the model on to the current closing price and make it predict the next closing price/trend for any given timestep. <br> To do that we define the add_targets(4) function which takes a data frame as input, adds the 'next_close' and 'trend' columns to it, and returns it as an output. ## Feature Engineering <br> Adding indicators to the dataframe is important for enhancing model performance and accuracy. <br> We can either manually calculate technical indicators, which is time-consuming and prone to errors, or we can utilize an existing library designed for this purpose. pandas-ta is a library that includes a wide set of technical indicators and is designed to work seamlessly with pandas dataframes. To explore the available indicators in pandas-ta, you can use the following: For this project, we added a total of 66 technical indicators. Each feature is carefully selected based on the technical indicator's definition and description. Check the full list of the selected indicators and the implementation of the add_technical_indicators function which take a dataframe as input and add these indicators to it (5). We can also get detailed information on specific indicators: Then, weâ€™ll group the features into four categories: 1. Base Features: Original features from yfinance (6 features). 2. Technical Indicators based on Closing Price: (30 features). 3. Technical Indicators based on Highs and Lows: (31 features). 4. Technical Indicators based on Volume: (5 features). This grouping will enable us to create more sophisticated models, such as multi-output or inception models, which we will explore later. Finally we will create add_targets_and_indicators (6), a helper functions to add the targets and indicators to all dataframes in a dictionary. ## Data balance and common sense baseline <br> With the target column added, we can now assess the balance of our dataset from a classification perspective. An unbalanced dataset may skew the model's predictions. The trend column indicates whether the stock will rise or fall. By calculating the ratio of (trend = 1) to the total number of samples, we can evaluate the data balance. To acomplish that, we'll create calculate_data_balance (6), a function that computes the ratio of trend = 1 for each individual dataframe in our dictionary and the overall ratio across all dataframes. Based on these results, the data appears well-balanced. #### Common Sense Baseline Establishing a common sense baseline is crucial in machine learning to ensure that our model performs at least as well as basic logical assumptions. In stock prediction, a reasonable baseline assumes that the future trend will mirror the current trend; if a stock is rising, itâ€™s expected to continue rising, and vice versa. This baseline is realistic and aligns with principles from behavioral finance[12], however, it's outside the scope of this project. To acheive that we will create calculate_common_sense_baseline (7), a function that calculates this common sense score for each stock in the dictionary, as well as the average overall score. ## Prepare the data for training #### Features scaling To prepare the data for training, we first need to scale the features to a specific range. This scaling process enhances the model's ability to learn patterns effectively and efficiently. We'll test three different scalers from the scikit-learn library: MinMaxScaler, StandardScaler, and RobustScaler. 1. MinMaxScaler Transform features by scaling each feature to a given range. (0-1 in our case) [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html] 2. StandardScaler Standardize features by removing the mean and scaling to unit variance. [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html] 3. RobustScaler Scale features using statistics that are robust to outliers by removing the median and scaling the data according to the quantile range. [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html] Given the continuous pattern shifts in financial data, no single scaler is guaranteed to consistently outperform the others. Therefore, we'll implement apply_scaler(8), a function that applies any given scaler to a set of features. This approach allows us to experiment with different scalers to achieve the best improvement to model performance. It's important to note that apply_scaler scales the training set independently from the test set to prevent data leakage, thereby reducing the risk of overfitting. #### Data Sequences To predict the stock trend for the next timestep, we look back at the last x timesteps and make a prediction based on that sequence. This requires reshaping the data into sequences of x timesteps, a necessary step for RNNs, which require input data in the shape of (samples, timesteps, and number of features). We achieve this by using the create_seqs(9) function, which takes features and the number of timesteps of each sequence and then reshapes the data based on the given timesteps. #### Train-Validation-Test Split We'll split the data into 70% for training, 20% for validation, and 10% for testing. This 70-20-10 split maximizes model accuracy while reducing the likelihood of overfitting that might occur with different ratios. create_train_vald_test_sets(11) function is where all these three processes will be applied in order to the data, making it ready for training. To apply these steps on each dataframe in the dictionary we will use the prepare_data_to_train(12), a function that converts the dictionary of dataframes into a dictionary of train-validation-test sets. ## Models archive For this section, we created create_models_archive(13), a function to train and save individual models for each dataset within a dictionary. This setup is crucial for scenarios where multiple stock choices are analyzed and assessed simultaneously. The function accepts a dictionary of datasets and a model creation function or class instance, then trains the model on each dataset separately. It stores the trained models, evaluation data, and hyperparameters (if applicable) in a dictionary associated with each stock. This implementation supports classification and regression-type models, so we can easily switch between the two prediction approaches. Additionally, this function incorporates EarlyStopping[] and ReduceLROnPlateau[] callbacks to enhance the training process. These callbacks help prevent overfitting by stopping training early if the model's performance stops improving and reducing the learning rate when necessary. [source: 7.2. Inspecting and monitoring deep-learning models using Keras callba- acks and TensorBoard, source: EarlyStopping, https://keras.io/api/callbacks/early_stopping/] [source: ReduceLROnPlateau, https://keras.io/api/callbacks/reduce_lr_on_plateau/] Optionally, it enables hyperparameter optimization during training, so if a hyperparameter tuner is passed, the function will fine-tune the model's hyperparameters to achieve the best possible performance before saving the final model. ## Models Evaluation To evaluate the models in the created models archive we will create evaluate_models_archive(14), a function that takes the model archive and for each model in it: 1. Calculate and print accuracy, precision, recall, F-score, and any other relevant evaluation metric. 2. Create a confusion matrix. 3. Plot Training vs. validation loss to assess model performance over epochs. Finally, the function calculates and prints the average metrics across all models. ## Machine Learning Models and RNNs For this type of problem, we can't simply rely on feedforward models such as densely connected networks or convolutional networks, since they are designed to process each input independently without retaining the state between inputs which is suitable when dealing with discrete data. However, when dealing with continuous data such as stock data, handling them as sequences would be more appropriate and this is where RNNs come into play. RNNs (recurrent neural networks) were designed specifically to process continuous data such as time series. As they can retain information over time allows them to identify patterns, making them powerful tools for forecasting. For this project, we will consider the three available types of RNNS in TensorFlow: SimpleRNN: The basic RNN, processes sequences by maintaining a hidden state that is updated at each timestep. However, it suffers from the vanishing gradient problem, as adding more layers, the network eventually becomes untrainable. LSTM (Long Short-Term Memory): an improvement over SimpleRNN as it addresses the vanishing gradient issue by incorporating three gates (input, forget, and output). These gates regulate the flow of information, allowing LSTMs to capture long-term dependencies effectively. However, LSTM is more computationally expensive than SimpleRNN. GRU (Gated Recurrent Unit): a simplified implementation of LSTMs with only two gates (reset and update). They are computationally more efficient while still theoretically countering the vanishing gradient problem. [][][] [deep learning with python, chapter 6, 6.2. Understanding recurrent neural networks] [What is LSTM â€“ Long Short Term Memory?, https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/] [Gated Recurrent Unit Networks, https://www.geeksforgeeks.org/gated-recurrent-unit-networks/] RNNModel(15) is a class we created that enables us to instantiate a classification or regression model, where the layers consist of either SimpleRNN, LSTM, or GRU. RNNModel will allow us to create a compare a wide range of models easily and fairly. ## SimpleRNN vs LSTM vs GRU ### SimpleRNN ### LSTM ### GRU ## Classification vs Regression approach As we stated before regression models could be more beneficial for predicting the actual future closing prices in the stock market since it would enable us to perform more advanced tasks such as portfolio optimization. Therefore before proceeding we will create a baseline regression model and compare it with our baseline classification model. If the regression model sounds promising, we will be improving it for the rest of the project, otherwise will proceed with the classification approach. The model's predictions are, on average, 16.5% off, which is significant for stock prediction. Hence, the regression approach is unsuitable with the current data, and we should focus on the classification approach. ## BatchNormalization BatchNormalization is a tensorflow layer type that adaptively normalize data as the mean and variance changes while training. It helps with gradient propagation allowing for deeper networks. We will add one BatchNormalization layer after each RNN layer and see if there are any improvements. Check RNNModelNorm(16) On average we see worst performance. ## Hyperparameter Optimization We implemented hyperparameter optimization using keras_tuner library. HP_RNNModel(17), a class we created, inherates from keras_tuner's HyperModel class. HP_RNNModel allows us to optimize: 1. Model hyperparameters: - Number of RNN layers (2-4) - Layer type (SimpleRNN, LSTM, GRU) - Units per layer (64-128) - Recurrent dropout (0.0-0.5) 2. Algorithm hyperparameters: - Optimizer type (Adam, RMSprop, SGD) - Learning rate (0.0001-0.01) 3. training hyperparameters: - Batch size (32, 64) Hyperband[] is the tuner we used as it's computationally efficient, making it suitable for tuning multiple models quickly. create_models_archive was created to be compatible with this approach by being able to use the passed tuner for optimization. create_models_archive also optimizes the number of epochs. [https://www.tensorflow.org/tutorials/keras/keras_tuner] ## Inception Model Inception models are designed to process input data through multiple parallel branches. They help the network separately learn features from different groups, which is more efficient than learning them jointly. In my implementation: 1. I created an InputSplitLayer(18) a custom layer to divide the input features into 4 groups. 2. Each feature group is passed through a separate HP_Hollow_RNN_Branch(19) class instance which is similar to the hp_RNN_model but without an output layer. 3. These branches are then combined in the HP_Hollow_RNN_Inception_Model, which merges the outputs from all the different branches before passing them to the output layer. The hyperparameter optimization process within HP_Hollow_RNN_Branch ensures the best possible configurations for each branch in the inception model. ## Different data intervals ### 1 Day interval ### 1 Hour interval ### 5 Minutes intreval \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = ''\n",
    "for i in wordCount:\n",
    "    full += i + \" \"\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd5e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
