{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d031bbc",
   "metadata": {},
   "source": [
    "# Project Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce398b2d",
   "metadata": {},
   "source": [
    "## Install and import the required libraries\n",
    "\n",
    "<br>\n",
    "In the implementation part, we will start by importing the required libraries for our work. We will work mainly with yfinance for data collection, Pandas and Numpy for data processing, and TensorFlow for machine learning.\n",
    "\n",
    "<br>\n",
    "Other relevant libraries are keras_tuner for hyperparameter optimization, scikit-learn for data scaling and model evaluation, pandas-ta for calculating technical indicators based on the data from yfinance, and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a3daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Dependencies and import libraries\n",
    "# !pip install yfinance pandas numpy tensorflow scikit-learn pandas-ta matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d117f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "# import yfinance, our data source\n",
    "import yfinance as yf\n",
    "\n",
    "# import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# import from tensorflow\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Input, GRU, SeparableConv1D, BatchNormalization, MaxPooling1D, add, Layer, concatenate\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# import from keras_tuner\n",
    "from keras_tuner import HyperModel, Hyperband, Tuner, Oracle\n",
    "\n",
    "# import from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# https://pypi.org/project/pandas-ta/ (\"\"\"An easy to use Python 3 Pandas Extension with 130+ Technical Analysis Indicators. Can be called from a Pandas DataFrame or standalone\"\"\")\n",
    "# import pandas-ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# import matplotlib for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this library allow us to calculate how long a process would take \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01a141",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "\n",
    "<br>\n",
    "In this implementation, we will work with 5 different stocks from the S&P500(1) list. The 5 stocks we will work with are chosen based on their ranking in this list from most valuable to least valuable, and each one is relatively distant from the other and belongs to a different industry. This will ensure a diverse sample and that our model evaluation results generalize relatively well, reducing the possibility of bias and overfitting.\n",
    "\n",
    "Check out our stock list for this project (2).\n",
    "\n",
    "<br>\n",
    "The yfinance API allows us to request the stock data for a company's given period and interval values. For the period value, we will set it to 10 years or max value which will be sufficient for all of our experiments, for the interval value however, which determines the frequency of the data rows, we will experiment with many options to see if our approach generalizes better with specific interval values as different intervals are relevant to other groups of financial analysts and traders in the real world, therefore we must try to create the best model relevant to each of these groups.\n",
    "\n",
    "That's why we will define a function that allows us to download any number of stock data at any period or interval, save the data as a CSV file to local storage, load it from storage, split it into different data frames based on the stock, and organize the data frames in a dictionary so it's easy to work with for the rest of the project.\n",
    "\n",
    "Check out the loadData function (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3dd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the stock symbols into a list\n",
    "symbols_list = ['PFE', 'ROP', 'XYL', 'CPAY', 'INCY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48a6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to load the data from source (yfinance API), and save it as a csv to local storage\n",
    "def loadData(symbols=symbols_list, period='10y', interval='1wk'):\n",
    "    \n",
    "    try:\n",
    "        # load the the dataframe from the csv file if it already exist\n",
    "        df = pd.read_csv(f'{period}_{interval}_stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "        \n",
    "        print(\"Data loaded from directory\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        # print a message stating the data does not already exists and need to be downloaded from yfinance\n",
    "        print(f\"There is no {period}_{interval}_stocks_data.csv. Data will be downloaded from yfinance.\")\n",
    "        \n",
    "        # download the data from source and store it in the stock_data variable which will hold the data as a pandas dataframe\n",
    "        stocks_data =  yf.download(symbols, period=period, interval=interval)\n",
    "\n",
    "        # reshape the dataframe as a multi-level index dataframe\n",
    "        stocks_data = stocks_data.stack()\n",
    "\n",
    "        # source: https://www.statology.org/pandas-change-column-names-to-lowercase/\n",
    "        # convert column names to lowercase\n",
    "        stocks_data.columns = stocks_data.columns.str.lower()\n",
    "\n",
    "        # save the dataframe to a csv file (Save the data to a CSV so we don't have to make any extra unnecessary requests to the API every time we reload the notebook)\n",
    "        stocks_data.to_csv(f'{period}_{interval}_stocks_data.csv', index=True)\n",
    "\n",
    "        # load the the dataframe from the csv file\n",
    "        df = pd.read_csv(f'{period}_{interval}_stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "\n",
    "    finally: \n",
    "        # create a dict to store the dataframe of each unique symbol where keys are symbol, values are dataframes\n",
    "        df_dict = {}\n",
    "\n",
    "        # iterate over the symbols\n",
    "        for symbol in symbols:\n",
    "\n",
    "            # source of inspiration https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html [11]\n",
    "            # extract the specific stock data at the 'Ticker' level of this multi index dataframe and save it as a dataframe\n",
    "            symbol_df = df.xs(symbol, axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "            # store the datafram into the df_dict\n",
    "            df_dict[symbol] = symbol_df\n",
    "\n",
    "        # return the dictionary\n",
    "        return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fae157",
   "metadata": {},
   "source": [
    "Load the data of the 5 selected stocks for the last 10 years on a weekly intrevals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b9e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from directory\n"
     ]
    }
   ],
   "source": [
    "# load the stock data for the 5 companies into a dictionary\n",
    "dfs = loadData(symbols=symbols_list, period='10y', interval='1wk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da142429",
   "metadata": {},
   "source": [
    "## Perform simple exploritory data analysis\n",
    "\n",
    "<br> \n",
    "Now that we have a dictionary of dataframes, we can analyze the data and make some observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef440b55",
   "metadata": {},
   "source": [
    "1. We can get the shape of the data for any stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9587216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: PFE, Shape: (523, 6) \n",
      "Symbol: ROP, Shape: (523, 6) \n",
      "Symbol: XYL, Shape: (523, 6) \n",
      "Symbol: CPAY, Shape: (523, 6) \n",
      "Symbol: INCY, Shape: (523, 6) \n"
     ]
    }
   ],
   "source": [
    "# the data shape\n",
    "for symbol in dfs.keys():\n",
    "    print(f\"Symbol: {symbol}, Shape: {dfs[symbol].shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35056b9a",
   "metadata": {},
   "source": [
    "2. We can get the basic stats for any stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16aa76f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>5.230000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.870774</td>\n",
       "      <td>36.269160</td>\n",
       "      <td>37.066481</td>\n",
       "      <td>35.423588</td>\n",
       "      <td>36.250718</td>\n",
       "      <td>1.373323e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.594988</td>\n",
       "      <td>6.862873</td>\n",
       "      <td>7.114429</td>\n",
       "      <td>6.536369</td>\n",
       "      <td>6.851178</td>\n",
       "      <td>6.265050e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.923565</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>26.170000</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>25.580000</td>\n",
       "      <td>3.922725e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.773307</td>\n",
       "      <td>31.555978</td>\n",
       "      <td>32.129982</td>\n",
       "      <td>30.858634</td>\n",
       "      <td>31.555977</td>\n",
       "      <td>9.714320e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.344492</td>\n",
       "      <td>34.478176</td>\n",
       "      <td>34.914612</td>\n",
       "      <td>33.785580</td>\n",
       "      <td>34.487667</td>\n",
       "      <td>1.215046e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.093800</td>\n",
       "      <td>40.028976</td>\n",
       "      <td>40.682581</td>\n",
       "      <td>39.165085</td>\n",
       "      <td>40.033976</td>\n",
       "      <td>1.583856e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>52.740730</td>\n",
       "      <td>59.480000</td>\n",
       "      <td>61.709999</td>\n",
       "      <td>57.160000</td>\n",
       "      <td>60.599998</td>\n",
       "      <td>6.333997e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        adj close       close        high         low        open  \\\n",
       "count  523.000000  523.000000  523.000000  523.000000  523.000000   \n",
       "mean    29.870774   36.269160   37.066481   35.423588   36.250718   \n",
       "std      7.594988    6.862873    7.114429    6.536369    6.851178   \n",
       "min     17.923565   25.400000   26.170000   25.200001   25.580000   \n",
       "25%     23.773307   31.555978   32.129982   30.858634   31.555977   \n",
       "50%     28.344492   34.478176   34.914612   33.785580   34.487667   \n",
       "75%     33.093800   40.028976   40.682581   39.165085   40.033976   \n",
       "max     52.740730   59.480000   61.709999   57.160000   60.599998   \n",
       "\n",
       "             volume  \n",
       "count  5.230000e+02  \n",
       "mean   1.373323e+08  \n",
       "std    6.265050e+07  \n",
       "min    3.922725e+07  \n",
       "25%    9.714320e+07  \n",
       "50%    1.215046e+08  \n",
       "75%    1.583856e+08  \n",
       "max    6.333997e+08  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data basic stats\n",
    "dfs[\"PFE\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe0c0c",
   "metadata": {},
   "source": [
    "3. We can check how many missing values each column have for a any stock dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0824543b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adj close    0\n",
       "close        0\n",
       "high         0\n",
       "low          0\n",
       "open         0\n",
       "volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many null values in each column\n",
    "dfs['PFE'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade84a71",
   "metadata": {},
   "source": [
    "#### Columns breakdown\n",
    "\n",
    "<br>\n",
    "Date: The index is the date on which the information on the rest of the columns takes place.\n",
    "\n",
    "<br>\n",
    "Adjusted close: is the closing price after adjustments for all applicable splits and dividend distributions which represents the true closing price.\n",
    "\n",
    "<br>\n",
    "Close: is the historical closing price of the stock.\n",
    "\n",
    "<br>\n",
    "High: is the highest point a stock has reached.\n",
    "\n",
    "<br>\n",
    "Low: is the lowest point of a stock.\n",
    "\n",
    "<br>\n",
    "Open: the opening price of the stock.\n",
    "\n",
    "<br>\n",
    "Volume: the volume of stocks traded in that timeframe.\n",
    "\n",
    "<br>\n",
    "Usually for this type of model we would only keep either Adjusted close or close, we are going to keep the adjusted close for now but it worth mentiong that most of the technical indicators we are utilizing are dependent on the none adjusted close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b832e",
   "metadata": {},
   "source": [
    "## Adding Targets\n",
    "\n",
    "<br>\n",
    "To predict stock trends based on past data, we'll create two new columns:\n",
    "\n",
    "- 'next_close': Represents the next closing price, serving as the target for the regression model.\n",
    "\n",
    "- 'trend': Indicates whether the next close is higher '1' or lower '0' than the current close, serving as the target for the classification model.\n",
    "\n",
    "So we will train the model on to the current closing price and make it predict the next closing price/trend for any given timestep.\n",
    "\n",
    "<br>\n",
    "To do that we define the add_targets(4) function which takes a data frame as input, adds the 'next_close' and 'trend' columns to it, and returns it as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c928aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dataframe and create 'next_close' column based on its 'close' column\n",
    "def get_next_close(_df):\n",
    "    \n",
    "    # create the 'next_close' column to be equal to the next closing price\n",
    "    # this can be accomplished easily by shifting the close column backward by 1\n",
    "    return _df['close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['next_close'] > row['close']:\n",
    "        return 1\n",
    "    elif row['next_close'] < row['close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "# create a function that add the target columns to the dataframe\n",
    "def add_targets(_df):\n",
    "    \n",
    "    # add the next_close column to the dataframe\n",
    "    _df['next_close'] = get_next_close(_df)\n",
    "    \n",
    "    # add the trend column to the dataframe\n",
    "    _df['trend'] = _df.apply(assign_trend, axis=1)\n",
    "    \n",
    "    # drop the NaN values\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    # fix the 'trend' data type to be int\n",
    "    _df = _df.astype({'trend': int})\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61556744",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "<br>\n",
    "Adding indicators to the dataframe is important for enhancing model performance and accuracy.\n",
    "\n",
    "<br>\n",
    "We can either manually calculate technical indicators, which is time-consuming and prone to errors, or we can utilize an existing library designed for this purpose. pandas-ta is a library that includes a wide set of technical indicators and is designed to work seamlessly with pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3abd3",
   "metadata": {},
   "source": [
    "To explore the available indicators in pandas-ta, you can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4082bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas TA - Technical Analysis Indicators - v0.3.14b0\n",
      "Total Indicators & Utilities: 205\n",
      "Abbreviations:\n",
      "    aberration, above, above_value, accbands, ad, adosc, adx, alma, amat, ao, aobv, apo, aroon, atr, bbands, below, below_value, bias, bop, brar, cci, cdl_pattern, cdl_z, cfo, cg, chop, cksp, cmf, cmo, coppock, cross, cross_value, cti, decay, decreasing, dema, dm, donchian, dpo, ebsw, efi, ema, entropy, eom, er, eri, fisher, fwma, ha, hilo, hl2, hlc3, hma, hwc, hwma, ichimoku, increasing, inertia, jma, kama, kc, kdj, kst, kurtosis, kvo, linreg, log_return, long_run, macd, mad, massi, mcgd, median, mfi, midpoint, midprice, mom, natr, nvi, obv, ohlc4, pdist, percent_return, pgo, ppo, psar, psl, pvi, pvo, pvol, pvr, pvt, pwma, qqe, qstick, quantile, rma, roc, rsi, rsx, rvgi, rvi, short_run, sinwma, skew, slope, sma, smi, squeeze, squeeze_pro, ssf, stc, stdev, stoch, stochrsi, supertrend, swma, t3, td_seq, tema, thermo, tos_stdevall, trima, trix, true_range, tsi, tsignals, ttm_trend, ui, uo, variance, vhf, vidya, vortex, vp, vwap, vwma, wcp, willr, wma, xsignals, zlma, zscore\n",
      "\n",
      "Candle Patterns:\n",
      "    2crows, 3blackcrows, 3inside, 3linestrike, 3outside, 3starsinsouth, 3whitesoldiers, abandonedbaby, advanceblock, belthold, breakaway, closingmarubozu, concealbabyswall, counterattack, darkcloudcover, doji, dojistar, dragonflydoji, engulfing, eveningdojistar, eveningstar, gapsidesidewhite, gravestonedoji, hammer, hangingman, harami, haramicross, highwave, hikkake, hikkakemod, homingpigeon, identical3crows, inneck, inside, invertedhammer, kicking, kickingbylength, ladderbottom, longleggeddoji, longline, marubozu, matchinglow, mathold, morningdojistar, morningstar, onneck, piercing, rickshawman, risefall3methods, separatinglines, shootingstar, shortline, spinningtop, stalledpattern, sticksandwich, takuri, tasukigap, thrusting, tristar, unique3river, upsidegap2crows, xsidegap3methods\n",
      "Help on NoneType object:\n",
      "\n",
      "class NoneType(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      True if self else False\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list available technical indicators\n",
    "help(dfs['PFE'].ta.indicators())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaebc0a",
   "metadata": {},
   "source": [
    "For this project, we added a total of 66 technical indicators. Each feature is carefully selected based on the technical indicator's definition and description. \n",
    "\n",
    "Check the full list of the selected indicators and the implementation of the add_technical_indicators function which take a dataframe as input and add these indicators to it (5).\n",
    "\n",
    "We can also get detailed information on specific indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5263de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function macd in module pandas_ta.momentum.macd:\n",
      "\n",
      "macd(close, fast=None, slow=None, signal=None, talib=None, offset=None, **kwargs)\n",
      "    Moving Average Convergence Divergence (MACD)\n",
      "    \n",
      "    The MACD is a popular indicator to that is used to identify a security's trend.\n",
      "    While APO and MACD are the same calculation, MACD also returns two more series\n",
      "    called Signal and Histogram. The Signal is an EMA of MACD and the Histogram is\n",
      "    the difference of MACD and Signal.\n",
      "    \n",
      "    Sources:\n",
      "        https://www.tradingview.com/wiki/MACD_(Moving_Average_Convergence/Divergence)\n",
      "        AS Mode: https://tr.tradingview.com/script/YFlKXHnP/\n",
      "    \n",
      "    Calculation:\n",
      "        Default Inputs:\n",
      "            fast=12, slow=26, signal=9\n",
      "        EMA = Exponential Moving Average\n",
      "        MACD = EMA(close, fast) - EMA(close, slow)\n",
      "        Signal = EMA(MACD, signal)\n",
      "        Histogram = MACD - Signal\n",
      "    \n",
      "        if asmode:\n",
      "            MACD = MACD - Signal\n",
      "            Signal = EMA(MACD, signal)\n",
      "            Histogram = MACD - Signal\n",
      "    \n",
      "    Args:\n",
      "        close (pd.Series): Series of 'close's\n",
      "        fast (int): The short period. Default: 12\n",
      "        slow (int): The long period. Default: 26\n",
      "        signal (int): The signal period. Default: 9\n",
      "        talib (bool): If TA Lib is installed and talib is True, Returns the TA Lib\n",
      "            version. Default: True\n",
      "        offset (int): How many periods to offset the result. Default: 0\n",
      "    \n",
      "    Kwargs:\n",
      "        asmode (value, optional): When True, enables AS version of MACD.\n",
      "            Default: False\n",
      "        fillna (value, optional): pd.DataFrame.fillna(value)\n",
      "        fill_method (value, optional): Type of fill method\n",
      "    \n",
      "    Returns:\n",
      "        pd.DataFrame: macd, histogram, signal columns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine the MACD indicator\n",
    "help(ta.macd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e79e205",
   "metadata": {},
   "source": [
    "Then, we’ll group the features into four categories:\n",
    "1. Base Features: Original features from yfinance (6 features).\n",
    "2. Technical Indicators based on Closing Price: (30 features).\n",
    "3. Technical Indicators based on Highs and Lows: (31 features).\n",
    "4. Technical Indicators based on Volume: (5 features).\n",
    "This grouping will enable us to create more sophisticated models, such as multi-output or inception models, which we will explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b85c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def add_technical_indicators(_df):\n",
    "    \n",
    "    ##### indicators based on the closing price ##### index range: 6:36\n",
    "    # apply macd on the close column in a df and add it to the dataframe    \n",
    "    macd = ta.macd(_df['close'])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, 'macd', macd.iloc[:,0])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(7, 'macd_histogram', macd.iloc[:,1])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(8, 'macd_signal', macd.iloc[:,2])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df['close'])\n",
    "    _df.insert(9, 'rsi', rsi)\n",
    "\n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df['close'])\n",
    "    _df.insert(10, 'sma', sma)\n",
    "\n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df['close'])\n",
    "    _df.insert(11, 'ema', ema)\n",
    "    \n",
    "    ######## repeat the same proccess for all the technical indicators we want to include ##########\n",
    "    # bbands: A popular volatility indicator by John Bollinger.\n",
    "    bbands = ta.bbands(_df['close'])\n",
    "    _df.insert(12, 'bbands_lower', bbands.iloc[:,0])\n",
    "    _df.insert(13, 'bbands_mid', bbands.iloc[:,1])\n",
    "    _df.insert(14, 'bbands_upper', bbands.iloc[:,2])\n",
    "    _df.insert(15, 'bbands_bandwidth', bbands.iloc[:,3])\n",
    "    _df.insert(16, 'bbands_percent', bbands.iloc[:,4])\n",
    "    \n",
    "    # dema: The Double Exponential Moving Average attempts to a smoother average with less lag than the normal Exponential Moving Average (EMA).\n",
    "    dema = ta.dema(_df['close'])\n",
    "    _df.insert(17, 'dema', dema)\n",
    "    \n",
    "    # tema: A less laggy Exponential Moving Average.\n",
    "    tema = ta.tema(_df['close'])\n",
    "    _df.insert(18, 'tema', tema)\n",
    "\n",
    "    # roc: Rate of Change is an indicator is also referred to as Momentum. It is a pure momentum oscillator that measures the percent change in price with the previous price 'n' (or length) periods ago.\n",
    "    roc = ta.roc(_df['close'])\n",
    "    _df.insert(19, 'roc', roc)\n",
    "    \n",
    "    # mom: Momentum is an indicator used to measure a security's speed (or strength) of movement.  Or simply the change in price.\n",
    "    mom = ta.mom(_df['close'])\n",
    "    _df.insert(20, 'mom', mom)\n",
    "    \n",
    "    # kama: Developed by Perry Kaufman, Kaufman's Adaptive Moving Average (KAMA) is a moving average designed to account for market noise or volatility. KAMA will closely follow prices when the price swings are relatively small and the noise is low. KAMA will adjust when the price swings widen and follow prices from a greater distance. This trend-following indicator can be used to identify the overall trend, time turning points and filter price movements.\n",
    "    kama = ta.kama(_df['close'])\n",
    "    _df.insert(21, 'kama', kama)\n",
    "                       \n",
    "    # trix: is a momentum oscillator to identify divergences.\n",
    "    trix = ta.trix(_df['close'])\n",
    "    _df.insert(22, 'trix', trix.iloc[:,0])\n",
    "    _df.insert(23, 'trixs', trix.iloc[:,1])\n",
    "    \n",
    "    # hma: The Hull Exponential Moving Average attempts to reduce or remove lag in moving averages.\n",
    "    hma = ta.hma(_df['close'])\n",
    "    _df.insert(24, 'hma', hma)\n",
    "    \n",
    "    # alma: The ALMA moving average uses the curve of the Normal (Gauss) distribution, which can be shifted from 0 to 1. This allows regulating the smoothness and high sensitivity of the indicator. Sigma is another parameter that is responsible for the shape of the curve coefficients. This moving average reduces lag of the data in conjunction with smoothing to reduce noise.\n",
    "    alma = ta.alma(_df['close'])\n",
    "    _df.insert(25, 'alma', alma)\n",
    "    \n",
    "    # apo: The Absolute Price Oscillator is an indicator used to measure a security's momentum.  It is simply the difference of two Exponential Moving Averages (EMA) of two different periods. Note: APO and MACD lines are equivalent.\n",
    "    apo = ta.apo(_df['close'])\n",
    "    _df.insert(26, 'apo', apo)\n",
    "    \n",
    "    # cfo: The Forecast Oscillator calculates the percentage difference between the actualprice and the Time Series Forecast (the endpoint of a linear regression line).\n",
    "    cfo = ta.cfo(_df['close'])\n",
    "    _df.insert(27, 'cfo', cfo)\n",
    "    \n",
    "    # cg: The Center of Gravity Indicator by John Ehlers attempts to identify turning points while exhibiting zero lag and smoothing.\n",
    "    cg = ta.cg(_df['close'])\n",
    "    _df.insert(28, 'cg', cg)\n",
    "    \n",
    "    # cmo: Attempts to capture the momentum of an asset with overbought at 50 and oversold at -50.\n",
    "    cmo = ta.cmo(_df['close'])\n",
    "    _df.insert(29, 'cmo', cmo)\n",
    "    \n",
    "    # coppock: Coppock Curve (originally called the \"Trendex Model\") is a momentum indicator is designed for use on a monthly time scale.  Although designed for monthly use, a daily calculation over the same period can be made, converting the periods to 294-day and 231-day rate of changes, and a 210-day weighted moving average.\n",
    "    coppock = ta.coppock(_df['close'])\n",
    "    _df.insert(30, 'coppock', coppock)\n",
    "    \n",
    "    # cti: The Correlation Trend Indicator is an oscillator created by John Ehler in 2020. It assigns a value depending on how close prices in that range are to following a positively- or negatively-sloping straight line. Values range from -1 to 1. This is a wrapper for ta.linreg(close, r=True).\n",
    "    cti = ta.cti(_df['close'])\n",
    "    _df.insert(31, 'cti', cti)\n",
    "    \n",
    "    # decay: Creates a decay moving forward from prior signals like crosses. The default is \"linear\". Exponential is optional as \"exponential\" or \"exp\".\n",
    "    decay = ta.decay(_df['close'])\n",
    "    _df.insert(32, 'decay', decay)\n",
    "    \n",
    "    # decreasing: Returns True if the series is decreasing over a period, False otherwise. If the kwarg 'strict' is True, it returns True if it is continuously decreasing over the period. When using the kwarg 'asint', then it returns 1 for True or 0 for False.\n",
    "    decreasing = ta.decreasing(_df['close'])\n",
    "    _df.insert(33, 'decreasing', decreasing)\n",
    "    \n",
    "    # ebsw: This indicator measures market cycles and uses a low pass filter to remove noise. Its output is bound signal between -1 and 1 and the maximum length of a detected trend is limited by its length input.\n",
    "    ebsw = ta.ebsw(_df['close'])\n",
    "    _df.insert(34, 'ebsw', ebsw)\n",
    "    \n",
    "    # entropy: Introduced by Claude Shannon in 1948, entropy measures the unpredictability of the data, or equivalently, of its average information. A die has higher entropy (p=1/6) versus a coin (p=1/2).\n",
    "    entropy = ta.entropy(_df['close'])\n",
    "    _df.insert(35, 'entropy', entropy)\n",
    "    \n",
    "    \n",
    "    ##### indicators based on the high and lows of the price ##### range= 36:67\n",
    "    \n",
    "    # aberration: A volatility indicator\n",
    "    aberration = ta.aberration(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(36, 'aberration_zg', aberration.iloc[:,0])\n",
    "    _df.insert(37, 'aberration_sg', aberration.iloc[:,1])\n",
    "    _df.insert(38, 'aberration_xg', aberration.iloc[:,2])\n",
    "    _df.insert(39, 'aberration_atr', aberration.iloc[:,3])\n",
    "    \n",
    "    # adx:  Average Directional Movement is meant to quantify trend strength by measuring the amount of movement in a single direction.    \n",
    "    adx = ta.adx(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(40, 'adx_adx', adx.iloc[:,0])\n",
    "    _df.insert(41, 'adx_dmp', adx.iloc[:,1])\n",
    "    _df.insert(42, 'adx_dmn', adx.iloc[:,2])\n",
    "\n",
    "    # atr: Averge True Range is used to measure volatility, especially volatility caused by gaps or limit moves.\n",
    "    atr = ta.atr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(43, 'atr', atr)\n",
    "    \n",
    "    # stoch: The Stochastic Oscillator (STOCH) was developed by George Lane in the 1950's. He believed this indicator was a good way to measure momentum because changes in momentum precede changes in price.\n",
    "    stoch = ta.stoch(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(44, 'stoch_k', stoch.iloc[:,0])\n",
    "    _df.insert(45, 'stoch_d', stoch.iloc[:,1])\n",
    "    \n",
    "    # Supertrend: is an overlap indicator. It is used to help identify trend direction, setting stop loss, identify support and resistance, and/or generate buy & sell signals.\n",
    "    supertrend = ta.supertrend(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(46, 'supertrend_trend', supertrend.iloc[:,0])\n",
    "    _df.insert(47, 'supertrend_direction', supertrend.iloc[:,1])\n",
    "    \n",
    "    # cci: Commodity Channel Index is a momentum oscillator used to primarily identify overbought and oversold levels relative to a mean.\n",
    "    cci = ta.cci(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(48, 'cci', cci)\n",
    "    \n",
    "    # aroon: attempts to identify if a security is trending and how strong.\n",
    "    aroon = ta.aroon(_df['high'], _df['low'])\n",
    "    _df.insert(49, 'aroon_up', aroon.iloc[:,0])\n",
    "    _df.insert(50, 'aroon_down', aroon.iloc[:,1])\n",
    "    _df.insert(51, 'aroon_osc', aroon.iloc[:,2])\n",
    "    \n",
    "    # natr: Normalized Average True Range attempt to normalize the average true range.\n",
    "    natr = ta.natr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(52, 'natr', natr)\n",
    "    \n",
    "    # William's Percent R is a momentum oscillator similar to the RSI that attempts to identify overbought and oversold conditions.\n",
    "    willr = ta.willr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(53, 'willr', willr)\n",
    "    \n",
    "    # vortex: Two oscillators that capture positive and negative trend movement.\n",
    "    vortex = ta.vortex(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(54, 'vortex_vip', vortex.iloc[:,0])\n",
    "    _df.insert(55, 'vortex_vim', vortex.iloc[:,1])\n",
    "    \n",
    "    # hlc3: the average of high, low, and close prices\n",
    "    hlc3 = ta.hlc3(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(56, 'hlc3', hlc3)\n",
    "    \n",
    "    # ohlc4: the average of open, high, low, and close prices\n",
    "    ohlc4 = ta.ohlc4(_df['open'], _df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(57, 'ohlc4', ohlc4)\n",
    "    \n",
    "    # accbands: Acceleration Bands created by Price Headley plots upper and lower envelope bands around a simple moving average.\n",
    "    accbands = ta.accbands(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(58, 'accbands_lower', accbands.iloc[:,0])\n",
    "    _df.insert(59, 'accbands_mid', accbands.iloc[:,1])\n",
    "    _df.insert(60, 'accbands_upper', accbands.iloc[:,2])\n",
    "\n",
    "    # chop: The Choppiness Index was created by Australian commodity trader E.W. Dreiss and is designed to determine if the market is choppy (trading sideways) or not choppy (trading within a trend in either direction). Values closer to 100 implies the underlying is choppier whereas values closer to 0 implies the underlying is trending.\n",
    "    chop = ta.chop(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(61, 'chop', chop)\n",
    "    \n",
    "    # dm: The Directional Movement was developed by J. Welles Wilder in 1978 attempts to determine which direction the price of an asset is moving. It compares prior highs and lows to yield to two series +DM and -DM.\n",
    "    dm = ta.dm(_df['high'], _df['low'])\n",
    "    _df.insert(62, 'dm_positive', dm.iloc[:,0])\n",
    "    _df.insert(63, 'dm_negative', dm.iloc[:,1])\n",
    "\n",
    "    # donchian: Donchian Channels are used to measure volatility, similar to Bollinger Bands and Keltner Channels.\n",
    "    donchian = ta.donchian(_df['high'], _df['low'])\n",
    "    _df.insert(64, 'donchian_lower', donchian.iloc[:,0])\n",
    "    _df.insert(65, 'donchian_mid', donchian.iloc[:,1])\n",
    "    _df.insert(66, 'donchian_upper', donchian.iloc[:,2])\n",
    "    \n",
    "    \n",
    "    ##### indicators based on the volume of the price ##### range= 67:72\n",
    "    \n",
    "    # obv: On Balance Volume is a cumulative indicator to measure buying and selling pressure.\n",
    "    obv = ta.obv(_df['close'], _df['volume'])\n",
    "    _df.insert(67, 'obv', obv)\n",
    "    \n",
    "    # vwma: Volume Weighted Moving Average.\n",
    "    vwma = ta.vwma(_df['close'], _df['volume'])\n",
    "    _df.insert(68, 'vwma', vwma)\n",
    "    \n",
    "    # adosc: Accumulation/Distribution Oscillator indicator utilizes Accumulation/Distribution and treats it similarily to MACD or APO.\n",
    "    adosc = ta.adosc(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(69, 'adosc', adosc)\n",
    "    \n",
    "    # cmf: Chailin Money Flow measures the amount of money flow volume over a specific period in conjunction with Accumulation/Distribution.\n",
    "    cmf = ta.cmf(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(70, 'cmf', cmf)\n",
    "    \n",
    "    # efi: Elder's Force Index measures the power behind a price movement using price and volume as well as potential reversals and price corrections.\n",
    "    efi = ta.efi(_df['close'], _df['volume'])\n",
    "    _df.insert(71, 'efi', efi)\n",
    "\n",
    "\n",
    "    #### we can add more technical indicators if we want using the same process ####\n",
    "    \n",
    "    # remove the NaN values and return the new dataframe\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a677b",
   "metadata": {},
   "source": [
    "Finally we will create add_targets_and_indicators (6), a helper functions to add the targets and indicators to all dataframes in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4928d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dictionary of dataframes as input and add the targets and features to them \n",
    "def add_targets_and_indicators(_dfs):\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # copy the dataframe\n",
    "        _df = _dfs[symbol].copy(deep=True)\n",
    "        \n",
    "        # add target columns to the copied dataframe\n",
    "        _df = add_targets(_df)\n",
    "        \n",
    "        # add technical indicators to the copied dataframe\n",
    "        _df = add_technical_indicators(_df)\n",
    "        \n",
    "        # replace the original dataframe with the new dataframe\n",
    "        _dfs[symbol] = _df\n",
    "    \n",
    "    # return the new dataframes dictionary\n",
    "    return _dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f45f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the targets and technical indicators to each dataframe in the dictionary\n",
    "full_dfs = add_targets_and_indicators(dfs.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91baee00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479, 74)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the new dataframes\n",
    "full_dfs['PFE'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a34863",
   "metadata": {},
   "source": [
    "## Data balance and common sense baseline\n",
    "\n",
    "<br>\n",
    "With the target column added, we can now assess the balance of our dataset from a classification perspective. An unbalanced dataset may skew the model's predictions. The trend column indicates whether the stock will rise or fall. By calculating the ratio of (trend = 1) to the total number of samples, we can evaluate the data balance.\n",
    "\n",
    "To acomplish that, we'll create calculate_data_balance (6), a function that computes the ratio of trend = 1 for each individual dataframe in our dictionary and the overall ratio across all dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27cb96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to computes the ratio of trend = 1 for each individual dataframe in a dictionary \n",
    "def calculate_data_balance(_dfs):\n",
    "    \n",
    "    # store the total ratio of trend 1 of all the dataframes\n",
    "    total = 0\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # get the number of values where trend = 1\n",
    "        trend_1 = _dfs[symbol]['trend'].value_counts()[1]\n",
    "        \n",
    "        # get the total number of rows in the dataframe\n",
    "        row_num = _dfs[symbol].shape[0]\n",
    "        \n",
    "        # percentage of 'trend up' to the whole column\n",
    "        trend_1_ratio = trend_1/row_num\n",
    "        \n",
    "        # print the ratio to the screen\n",
    "        print(f\"The Trend up ratio of {symbol} is: {trend_1_ratio}\")\n",
    "        \n",
    "        # add the ratio to total\n",
    "        total += trend_1_ratio\n",
    "        \n",
    "    # get the average trend up ratio\n",
    "    average = total / len(_dfs.keys())\n",
    "    \n",
    "    # print the average ratio\n",
    "    print(f\"The Average Trend up ratio is: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e349d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trend up ratio of PFE is: 0.5177453027139874\n",
      "The Trend up ratio of ROP is: 0.578838174273859\n",
      "The Trend up ratio of XYL is: 0.5912863070539419\n",
      "The Trend up ratio of CPAY is: 0.556935817805383\n",
      "The Trend up ratio of INCY is: 0.463768115942029\n",
      "The Average Trend up ratio is: 0.5417147435578401\n"
     ]
    }
   ],
   "source": [
    "# get the trend up ratio of all the dataframes in the dictionary to get a sense of how balanced the data is\n",
    "calculate_data_balance(full_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b329ce",
   "metadata": {},
   "source": [
    "Based on these results, the data appears well-balanced.\n",
    "\n",
    "#### Common Sense Baseline\n",
    "\n",
    "Establishing a common sense baseline is crucial in machine learning to ensure that our model performs at least as well as basic logical assumptions. In stock prediction, a reasonable baseline assumes that the future trend will mirror the current trend; if a stock is rising, it’s expected to continue rising, and vice versa. This baseline is realistic and aligns with principles from behavioral finance[12], however, it's outside the scope of this project.\n",
    "\n",
    "To acheive that we will create calculate_common_sense_baseline (7), a function that calculates this common sense score for each stock in the dictionary, as well as the average overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20b789f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_common_sense_baseline(_dfs):\n",
    "    \n",
    "    # store the total common_sense_score for all the dataframes\n",
    "    total = 0\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # since the common sense will be to assume the trend next is going to be the same as the trend now, we will shift the trend\n",
    "        # forward by one, this will give us a column that matches the common sense assumption we set\n",
    "        common_sense = _dfs[symbol]['trend'].shift(1)\n",
    "\n",
    "        # measure the average of when the common sense (naive) prediction matches the actual 'trend'\n",
    "        common_sense_score = (common_sense == _dfs[symbol]['trend']).mean()\n",
    "        \n",
    "        # print the score to the screen\n",
    "        print(f\"The common sense score of {symbol} is: {common_sense_score}\")\n",
    "        \n",
    "        # add the score to the total\n",
    "        total += common_sense_score\n",
    "       \n",
    "    \n",
    "    # get the average score\n",
    "    average = total / len(_dfs.keys())\n",
    "    \n",
    "    # print the average score\n",
    "    print(f\"The Average common sense score is: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c24c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common sense score of PFE is: 0.534446764091858\n",
      "The common sense score of ROP is: 0.520746887966805\n",
      "The common sense score of XYL is: 0.5124481327800829\n",
      "The common sense score of CPAY is: 0.505175983436853\n",
      "The common sense score of INCY is: 0.494824016563147\n",
      "The Average common sense score is: 0.5135283569677492\n"
     ]
    }
   ],
   "source": [
    "# get the common sense baseline for each dataframe in the dictionary\n",
    "calculate_common_sense_baseline(full_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c982032",
   "metadata": {},
   "source": [
    "## Prepare the data for training\n",
    "\n",
    "#### Features scaling\n",
    "\n",
    "To prepare the data for training, we first need to scale the features to a specific range. This scaling process enhances the model's ability to learn patterns effectively and efficiently.\n",
    "\n",
    "We'll test three different scalers from the scikit-learn library: MinMaxScaler, StandardScaler, and RobustScaler.\n",
    "\n",
    "1. MinMaxScaler Transform features by scaling each feature to a given range. (0-1 in our case) [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html]\n",
    "2. StandardScaler Standardize features by removing the mean and scaling to unit variance. [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html]\n",
    "3. RobustScaler Scale features using statistics that are robust to outliers by removing the median and scaling the data according to the quantile range. [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html]\n",
    "\n",
    "Given the continuous pattern shifts in financial data, no single scaler is guaranteed to consistently outperform the others. Therefore, we'll implement apply_scaler(8), a function that applies any given scaler to a set of features. This approach allows us to experiment with different scalers to achieve the best improvement to model performance.\n",
    "\n",
    "It's important to note that apply_scaler scales the training set independently from the test set to prevent data leakage, thereby reducing the risk of overfitting.\n",
    "\n",
    "#### Data Sequences\n",
    "\n",
    "To predict the stock trend for the next timestep, we look back at the last x timesteps and make a prediction based on that sequence. This requires reshaping the data into sequences of x timesteps, a necessary step for RNNs, which require input data in the shape of (samples, timesteps, and number of features). We achieve this by using the create_seqs(9) function, which takes features and the number of timesteps of each sequence and then reshapes the data based on the given timesteps.\n",
    "\n",
    "#### Train-Validation-Test Split\n",
    "\n",
    "We'll split the data into 70% for training, 20% for validation, and 10% for testing. This 70-20-10 split maximizes model accuracy while reducing the likelihood of overfitting that might occur with different ratios. \n",
    "\n",
    "create_train_vald_test_sets(11) function is where all these three processes will be applied in order to the data, making it ready for training.\n",
    "\n",
    "To apply these steps on each dataframe in the dictionary we will use the prepare_data_to_train(12), a function that converts the dictionary of dataframes into a dictionary of train-validation-test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c34a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to apply a given scaler to the features\n",
    "def apply_scaler(scaler, features):\n",
    "    \n",
    "    # set the training and test ratio to be 70-30\n",
    "    training_ratio = int(len(features) * 0.7)\n",
    "\n",
    "    # devide the feature set into training and test set\n",
    "    X_train, X_test = features[:training_ratio], features[training_ratio:]\n",
    "    \n",
    "    # apply a scaler on the training and test sets in isolation so we don't allow the test set to influence the scaling process, which reduces the likelihood of overfitting \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # concat the two scaled sets into one\n",
    "    X = np.concatenate((X_train_scaled, X_test_scaled), axis=0)\n",
    "\n",
    "    # return the scaled features\n",
    "    return X\n",
    "\n",
    "\n",
    "# source of isnpiration: https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4 [13]\n",
    "# create a function to reshape X and y into sequences of x timesteps\n",
    "def create_seqs(features, target, num_rows):\n",
    "    # create 2 empty lists to store the newly shaped features and target lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate over the features\n",
    "    for i in range(len(features) - num_rows):\n",
    "        # create indexes of the start and end of each sequence\n",
    "        seq_s = i\n",
    "        seq_e = i + num_rows\n",
    "        \n",
    "        # the ith sequence will be a slice of the features between the indexes, create it and add it to X\n",
    "        xi = features[seq_s : seq_e]\n",
    "        X.append(xi)\n",
    "        \n",
    "        # do the same for the target and add it to y\n",
    "        yi = target[seq_e]\n",
    "        y.append(yi)\n",
    "    \n",
    "    # return the X and y as numpy arraies\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# create a function to convert a dataframe into training, validation and test sets\n",
    "def create_train_vald_test_sets(_df, scaler, target=\"classification\", timesteps=6):\n",
    "\n",
    "    # reset the index\n",
    "    _df.reset_index(inplace = True)\n",
    "    \n",
    "    # drop the Date column as it's not necessary for now\n",
    "    _df.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "    # set the features set\n",
    "    X = _df.iloc[:, :-2]\n",
    "    \n",
    "    # set the target \n",
    "    if (target == \"classification\"):\n",
    "        # trend is the target for classification\n",
    "        y = _df.iloc[:, -1]\n",
    "    else:\n",
    "        # next_close is the target for regression\n",
    "        y = _df.iloc[:, -2]\n",
    "\n",
    "    # apply a scaler on the features set\n",
    "    X = apply_scaler(scaler, X)\n",
    "    \n",
    "    # create sequences\n",
    "    X_seq, y_seq = create_seqs(X, y, timesteps)\n",
    "    \n",
    "    # source of inspiration: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical [14]\n",
    "    # use to_categorical from tf to converts the target (trend) to binary class matrix, this will help us assign confidences to the classification prediction\n",
    "    if (target == \"classification\"):\n",
    "        y_seq = to_categorical(y_seq)\n",
    "\n",
    "    # devide the data into a training set and a test set in 70-30 ratio\n",
    "    training_ratio = int(len(X) * 0.7)\n",
    "    \n",
    "    # add a vaidation ratio at 20% of the data, this will leave 10% as test\n",
    "    validation_ratio = int(len(X) * 0.2)\n",
    "    \n",
    "    X_train, X_vald, X_test = X_seq[:training_ratio], X_seq[training_ratio:training_ratio + validation_ratio], X_seq[training_ratio + validation_ratio:]\n",
    "    y_train, y_vald, y_test = y_seq[:training_ratio], y_seq[training_ratio:training_ratio + validation_ratio], y_seq[training_ratio + validation_ratio:]\n",
    "\n",
    "#     X_train, X_test = X_seq[:training_ratio], X_seq[training_ratio:]\n",
    "#     y_train, y_test = y_seq[:training_ratio], y_seq[training_ratio:]\n",
    "\n",
    "    # return the sets and the last_date\n",
    "    return X_train, X_vald, X_test, y_train, y_vald, y_test\n",
    "\n",
    "\n",
    "# create a function that takes a dict of dataframes, and return a dict of training, validation and testing datasets\n",
    "def prepare_data_to_train(dfs_dict, scaler, target, timesteps):\n",
    "    \n",
    "    # create a dict of dicts to store training, validation and test sets for each stock\n",
    "    sets_dict = {}\n",
    "    \n",
    "    # iterate over each dataframe in the dictionary\n",
    "    for symbol in dfs_dict.keys():\n",
    "        \n",
    "        # convert the dataframe into training, validation and test sets\n",
    "        X_train, X_vald, X_test, y_train, y_vald, y_test = create_train_vald_test_sets(dfs_dict[symbol].copy(deep=True), scaler, target, timesteps)\n",
    "        \n",
    "        # create a dict of the sets and add it to the sets_dict\n",
    "        sets_dict[symbol] = {\n",
    "            'X_train': X_train, 'X_vald': X_vald, 'X_test': X_test, \n",
    "            'y_train': y_train, 'y_vald': y_vald, 'y_test': y_test\n",
    "        }\n",
    "    \n",
    "    # return the sets\n",
    "    return sets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4010026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a MinMaxScaler instance for a range between 0 and 1\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# initialize a StandardScaler instance\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# initialize a RobustScaler instance\n",
    "robust_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e983a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the 3 steps of preparing data for training on each dataframe in the dictionary\n",
    "target = 'classification' # set the target\n",
    "timesteps = 10 # set the timesteps\n",
    "data_sets = prepare_data_to_train(full_dfs.copy(), min_max_scaler, target, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50ba6560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335, 10, 72)\n",
      "(95, 10, 72)\n",
      "(39, 10, 72)\n"
     ]
    }
   ],
   "source": [
    "print(data_sets['PFE']['X_train'].shape)\n",
    "print(data_sets['PFE']['X_vald'].shape)\n",
    "print(data_sets['PFE']['X_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d9a65",
   "metadata": {},
   "source": [
    "## Models archive\n",
    "\n",
    "For this section, we created create_models_archive(13), a function to train and save individual models for each dataset within a dictionary. This setup is crucial for scenarios where multiple stock choices are analyzed and assessed simultaneously. The function accepts a dictionary of datasets and a model creation function or class instance, then trains the model on each dataset separately. It stores the trained models, evaluation data, and hyperparameters (if applicable) in a dictionary associated with each stock.\n",
    "\n",
    "This implementation supports classification and regression-type models, so we can easily switch between the two prediction approaches. \n",
    "\n",
    "Additionally, this function incorporates EarlyStopping[] and ReduceLROnPlateau[] callbacks to enhance the training process. These callbacks help prevent overfitting by stopping training early if the model's performance stops improving and reducing the learning rate when necessary.\n",
    "\n",
    "[source: 7.2. Inspecting and monitoring deep-learning models using Keras callba- acks and TensorBoard, \n",
    "source: EarlyStopping, https://keras.io/api/callbacks/early_stopping/]\n",
    "\n",
    "[source: ReduceLROnPlateau, https://keras.io/api/callbacks/reduce_lr_on_plateau/]\n",
    "\n",
    "Optionally, it enables hyperparameter optimization during training, so if a hyperparameter tuner is passed, the function will fine-tune the model's hyperparameters to achieve the best possible performance before saving the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c04d6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of inspiration: Introduction to the Keras Tuner, https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "# create a function to train static and hyperparameters optimized models on multiple datasets and archive them\n",
    "def create_models_archive(_create_model, _datasets_dict, _model_type, _tuner=None, _epochs=50, _model_name='model', _project_name='proj'):\n",
    "    \n",
    "    # get time before the training\n",
    "    start = get_time()\n",
    "    \n",
    "    # create the models archive dictionary\n",
    "    archive = {}\n",
    "    \n",
    "    # iterate over the symbols in the dictionary\n",
    "    for symbol in _datasets_dict.keys():\n",
    "        \n",
    "        # initiate a dict for the symbol\n",
    "        archive[symbol] = {}\n",
    "        \n",
    "        # setup the data to be passed to the model\n",
    "        X_train, y_train = _datasets_dict[symbol]['X_train'], _datasets_dict[symbol]['y_train']\n",
    "        X_vald, y_vald = _datasets_dict[symbol]['X_vald'], _datasets_dict[symbol]['y_vald']\n",
    "        X_test, y_test = _datasets_dict[symbol]['X_test'], _datasets_dict[symbol]['y_test']\n",
    "\n",
    "        # source: 7.2. Inspecting and monitoring deep-learning models using Keras callba- acks and TensorBoard\n",
    "        # source: EarlyStopping, https://keras.io/api/callbacks/early_stopping/\n",
    "        # Stop training when a monitored metric has stopped improving.\n",
    "        # monitor: Quantity to be monitored.\n",
    "        # min_delta: Minimum change in the monitored quantity to qualify as an improvement (we will use the default value)\n",
    "        # patience: Number of epochs with no improvement after which training will be stopped.\n",
    "        stop_early = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=0, \n",
    "                                   patience=5)\n",
    "        \n",
    "        # source: ReduceLROnPlateau, https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "        # Reduce learning rate when a metric has stopped improving.\n",
    "        reduce_lr =  ReduceLROnPlateau(monitor='val_loss', \n",
    "                                       factor=0.1, \n",
    "                                       patience=10)\n",
    "        \n",
    "        # initialize the model\n",
    "        model = _create_model(X_train.shape)        \n",
    "        \n",
    "        # source: Hyperband Tuner, https://keras.io/api/keras_tuner/tuners/hyperband/\n",
    "        ### Instantiate the tuner and perform hypertuning\n",
    "        # pass the model which is an Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance)\n",
    "        # objective is the direction of the optimization\n",
    "        # max_epochs is the maximum number of epochs to train one model, it is recommended to set this to a value slightly higher than expected then use early stopping callback during training (we will use the default value)\n",
    "        # factor: the reduction factor for the number of epochs and number of models for each bracket (we will use the default value)\n",
    "        # hyperband_iterations: the number of times to iterate over the full Hyperband algorithm. One iteration will run approximately max_epochs * (math.log(max_epochs, factor) ** 2) cumulative epochs across all trials. (we will use the default value)\n",
    "        # we will set the seed to make our work easier to replicate by others \n",
    "        # directory is and project name is the path where it will store the trails data results, this will make it much faster to rerun the training process if we need to\n",
    "        if tuner != None:\n",
    "            tuner = _tuner(model, \n",
    "                        objective='val_accuracy', \n",
    "                        max_epochs=10, \n",
    "                        factor=3, \n",
    "                        hyperband_iterations=1, \n",
    "                        seed=101, \n",
    "                        directory='keras_tuner_models', \n",
    "                        project_name=f'{_project_name}/{symbol}')\n",
    "\n",
    "            # Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.\n",
    "            tuner.search(X_train, y_train, \n",
    "                         epochs=_epochs, \n",
    "                         validation_data=(X_vald, y_vald), \n",
    "                         callbacks=[stop_early, reduce_lr], \n",
    "                        verbose=1)\n",
    "\n",
    "            # source: The base Tuner class, https://keras.io/api/keras_tuner/tuners/base_tuner/\n",
    "            # get_best_hyperparameters Returns the best hyperparameters, as determined by the objective as a list sorted from the best to the worst.\n",
    "            # Get the optimal hyperparameters\n",
    "            best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "            # Display tuning results summary. prints a summary of the search results including the hyperparameter values and evaluation results for each trial.\n",
    "            results_summary = tuner.results_summary()\n",
    "        \n",
    "            # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "            model = tuner.hypermodel.build(best_hps)\n",
    "                    \n",
    "        \n",
    "        # fit the model\n",
    "        history = model.fit(X_train, y_train, \n",
    "                            epochs=_epochs, \n",
    "                            batch_size=32, \n",
    "                            validation_data=(X_vald, y_vald), \n",
    "                            callbacks=[stop_early, reduce_lr], \n",
    "                            verbose=1)\n",
    "        \n",
    "        # get the history of val_precision during training as a list\n",
    "        val_prec_per_epoch = history.history['val_accuracy']\n",
    "        \n",
    "        # get the index of the highest val_precision from this list, we will use this index to set the epochs values during the training\n",
    "        best_epoch = val_prec_per_epoch.index(max(val_prec_per_epoch)) + 1\n",
    "        print('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "        ### train the model based on the results of the hyperparameter optimization process \n",
    "        # Re-instantiate the hypermodel and train it with the optimal number of epochs from above.\n",
    "        if _tuner:\n",
    "            hypermodel = tuner.hypermodel.build(best_hps)\n",
    "        else:\n",
    "            hypermodel = _create_model(X_train.shape)\n",
    "\n",
    "        # Retrain the model\n",
    "        hypermodel_history = hypermodel.fit(X_train, y_train, \n",
    "                                            validation_data=(X_vald, y_vald), \n",
    "                                            epochs=best_epoch, \n",
    "                                            verbose=1)\n",
    "\n",
    "        # evaluate the model\n",
    "        model_evaluation = hypermodel.evaluate(X_test, y_test, verbose=0)\n",
    "            \n",
    "        # get predictions from the model given the test set\n",
    "        y_pred = hypermodel.predict(X_test)\n",
    "\n",
    "        # evaluate the model whether the model type is classification or regression\n",
    "        if _model_type == \"classification\":     \n",
    "            # convert the predictions and test set to be in the shape of a vector of labels\n",
    "            y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "            y_test_labels = np.argmax(y_test, axis=1)\n",
    "        else:\n",
    "            # get the R2 of the model\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # source of inspiration: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "        # save model to device\n",
    "        hypermodel.save(f'models/{_model_name}_{symbol}.keras')        \n",
    "        \n",
    "        # store the model in the associated symbol dict\n",
    "        archive[symbol]['model'] = load_model(f'models/{model_name}_{symbol}.keras')\n",
    "        \n",
    "        # evaluate the model on the test_set and store it in the associated symbol dict\n",
    "        archive[symbol]['evaluation'] = model_evaluation\n",
    "        \n",
    "        # store the best model hyperparameters in the associated symbol dict\n",
    "        archive[symbol]['hyperparameters'] = best_hps if _tuner else None\n",
    "        \n",
    "        # store the the best model training and validation accuracy history\n",
    "        archive[symbol]['training_history'] = hypermodel_history\n",
    "        \n",
    "        # store the the best model prediction labels and true labels\n",
    "        archive[symbol]['y_pred_labels'] = y_pred_labels if _model_type == \"classification\" else None\n",
    "        archive[symbol]['y_test_labels'] = y_test_labels if _model_type == \"classification\" else None\n",
    "        \n",
    "        # store the R2 score for regression model\n",
    "        archive[symbol]['r2'] = r2 if _model_type == \"regression\" else None\n",
    "        \n",
    "        # store the the tunning proccess results_summary\n",
    "        archive[symbol]['results_summary'] = results_summary if _tuner else None\n",
    "        \n",
    "        # get time after the training\n",
    "        end = get_time()\n",
    "        \n",
    "        # print the trainig duration\n",
    "        print(f\"training for the {symbol} model was done in: {end - start}\")\n",
    "        \n",
    "    return archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4a586",
   "metadata": {},
   "source": [
    "## Models Evaluation\n",
    "\n",
    "To evaluate the models in the created models archive we will create evaluate_models_archive(14), a function that takes the model archive and for each model in it: \n",
    "1. Calculate and print accuracy, precision, recall, F-score, and any other relevant evaluation metric.\n",
    "2. Create a confusion matrix.\n",
    "3. Plot Training vs. validation loss to assess model performance over epochs.\n",
    "\n",
    "Finally, the function calculates and prints the average metrics across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54bcb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to calculate the precision, recall, and accuracy of a model\n",
    "def calculate_precision_recall_fscore(y_test, y_pred):\n",
    "    # source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "    # get precision, recall, and fscore and return them\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    return precision, recall, fscore\n",
    "\n",
    "# create a function that produce a confusion matrix of a model\n",
    "def create_confusion_matrix(y_test, y_pred):\n",
    "    # source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(conf_mat)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "       \n",
    "# create a function that graph the traing vs validation loss\n",
    "def create_train_vald_graph(training_history): \n",
    "    # source of the code snippet[17]\n",
    "    # get the training and validation loss\n",
    "    loss = training_history['loss']\n",
    "    val_loss = training_history['val_loss']\n",
    "    \n",
    "    # we can get the number of epochs simply from the length of the loss list\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    \n",
    "    ### plot\n",
    "    plt.figure()\n",
    "    \n",
    "    # plot the training loss against the epochs\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    \n",
    "    # plot the validation loss against the epochs\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    \n",
    "    # add title and legend\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# create a function that evaluate a dictionary of models\n",
    "def evaluate_models_archive(_models_archive):\n",
    "    \n",
    "    # initialize the total metrics variables set them to 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_fscore = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    # iterate over the symbols of the dictionary\n",
    "    for symbol in _models_archive.keys():\n",
    "        \n",
    "        # get the model y_test and y_pred\n",
    "        y_test = _models_archive[symbol]['y_test_labels']\n",
    "        y_pred = _models_archive[symbol]['y_pred_labels']\n",
    "        \n",
    "        # classification model metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, fscore = calculate_precision_recall_fscore(y_test, y_pred)\n",
    "        \n",
    "        # print the metrics for each model\n",
    "        print(f\"The {symbol} Model Classification Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F-Score: {fscore}\")\n",
    "        print(\"--------------------------------------------------------------\")\n",
    "        \n",
    "        # add the scores for this model to the total scores\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_fscore += fscore\n",
    "        total_accuracy += accuracy\n",
    "        \n",
    "        # create confusion matrix\n",
    "        create_confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # plot training vs validation loss\n",
    "        create_train_vald_graph(_models_archive[symbol]['training_history'].history)\n",
    "        \n",
    "    # calculate average metrics\n",
    "    models_num = len(_models_archive.keys())\n",
    "    average_precision = total_precision / models_num\n",
    "    average_recall = total_recall / models_num\n",
    "    average_fscore = total_fscore / models_num\n",
    "    average_accuracy = total_accuracy / models_num\n",
    "\n",
    "    # print the average metrics\n",
    "    print(f\"Average Classification Metrics for All Models:\")\n",
    "    print(f\"Average Accuracy: {average_accuracy}\")\n",
    "    print(f\"Average Precision: {average_precision}\")\n",
    "    print(f\"Average Recall: {average_recall}\")\n",
    "    print(f\"Average F-Score: {average_fscore}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bc4e5",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d719749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
