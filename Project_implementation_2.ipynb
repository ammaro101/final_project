{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732cafee",
   "metadata": {},
   "source": [
    "# Project Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e33b7",
   "metadata": {},
   "source": [
    "## Install and import the required libraries\n",
    "\n",
    "<br>\n",
    "In the implementation part, we will start by importing the required libraries for our work. We will work mainly with yfinance for data collection, Pandas and Numpy for data processing, and TensorFlow for machine learning.\n",
    "\n",
    "<br>\n",
    "Other relevant libraries are keras_tuner for hyperparameter optimization, scikit-learn for data scaling and model evaluation, pandas-ta for calculating technical indicators based on the data from yfinance, and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcb0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Dependencies and import libraries\n",
    "# !pip install yfinance pandas numpy tensorflow scikit-learn pandas-ta matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d3e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "# import yfinance, our data source\n",
    "import yfinance as yf\n",
    "\n",
    "# import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# import from tensorflow\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Input, GRU, SeparableConv1D, BatchNormalization, MaxPooling1D, add, Layer, concatenate\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# import from keras_tuner\n",
    "from keras_tuner import HyperModel, Hyperband, Tuner, Oracle\n",
    "\n",
    "# import from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# https://pypi.org/project/pandas-ta/ (\"\"\"An easy to use Python 3 Pandas Extension with 130+ Technical Analysis Indicators. Can be called from a Pandas DataFrame or standalone\"\"\")\n",
    "# import pandas-ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# import matplotlib for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this library allow us to calculate how long a process would take \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40172c74",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "\n",
    "<br>\n",
    "In this implementation, we will work with 5 different stocks from the S&P500(1) list. The 5 stocks we will work with are chosen based on their ranking in this list from most valuable to least valuable, and each one is relatively distant from the other and belongs to a different industry. This will ensure a diverse sample and that our model evaluation results generalize relatively well, reducing the possibility of bias and overfitting.\n",
    "\n",
    "Check out our stock list for this project (2).\n",
    "\n",
    "<br>\n",
    "The yfinance API allows us to request the stock data for a company's given period and interval values. For the period value, we will set it to 10 years or max value which will be sufficient for all of our experiments, for the interval value however, which determines the frequency of the data rows, we will experiment with many options to see if our approach generalizes better with specific interval values as different intervals are relevant to other groups of financial analysts and traders in the real world, therefore we must try to create the best model relevant to each of these groups.\n",
    "\n",
    "That's why we will define a function that allows us to download any number of stock data at any period or interval, save the data as a CSV file to local storage, load it from storage, split it into different data frames based on the stock, and organize the data frames in a dictionary so it's easy to work with for the rest of the project.\n",
    "\n",
    "Check out the loadData function (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d04723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the stock symbols into a list\n",
    "symbols_list = ['PFE', 'ROP', 'XYL', 'CPAY', 'INCY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf2f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to load the data from source (yfinance API), and save it as a csv to local storage\n",
    "def loadData(symbols=symbols_list, period='10y', interval='1wk'):\n",
    "    \n",
    "    try:\n",
    "        # load the the dataframe from the csv file if it already exist\n",
    "        df = pd.read_csv(f'{period}_{interval}_stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "        \n",
    "        print(\"Data loaded from directory\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        # print a message stating the data does not already exists and need to be downloaded from yfinance\n",
    "        print(f\"There is no {period}_{interval}_stocks_data.csv. Data will be downloaded from yfinance.\")\n",
    "        \n",
    "        # download the data from source and store it in the stock_data variable which will hold the data as a pandas dataframe\n",
    "        stocks_data =  yf.download(symbols, period=period, interval=interval)\n",
    "\n",
    "        # reshape the dataframe as a multi-level index dataframe\n",
    "        stocks_data = stocks_data.stack()\n",
    "\n",
    "        # source: https://www.statology.org/pandas-change-column-names-to-lowercase/\n",
    "        # convert column names to lowercase\n",
    "        stocks_data.columns = stocks_data.columns.str.lower()\n",
    "\n",
    "        # save the dataframe to a csv file (Save the data to a CSV so we don't have to make any extra unnecessary requests to the API every time we reload the notebook)\n",
    "        stocks_data.to_csv(f'{period}_{interval}_stocks_data.csv', index=True)\n",
    "\n",
    "        # load the the dataframe from the csv file\n",
    "        df = pd.read_csv(f'{period}_{interval}_stocks_data.csv').set_index(['Date', 'Ticker'])\n",
    "\n",
    "    finally: \n",
    "        # create a dict to store the dataframe of each unique symbol where keys are symbol, values are dataframes\n",
    "        df_dict = {}\n",
    "\n",
    "        # iterate over the symbols\n",
    "        for symbol in symbols:\n",
    "\n",
    "            # source of inspiration https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html [11]\n",
    "            # extract the specific stock data at the 'Ticker' level of this multi index dataframe and save it as a dataframe\n",
    "            symbol_df = df.xs(symbol, axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "            # store the datafram into the df_dict\n",
    "            df_dict[symbol] = symbol_df\n",
    "\n",
    "        # return the dictionary\n",
    "        return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f0bb2",
   "metadata": {},
   "source": [
    "Load the data of the 5 selected stocks for the last 10 years on a weekly intrevals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bd077a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from directory\n"
     ]
    }
   ],
   "source": [
    "# load the stock data for the 5 companies into a dictionary\n",
    "dfs = loadData(symbols=symbols_list, period='10y', interval='1wk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dad2bf",
   "metadata": {},
   "source": [
    "## Perform simple exploritory data analysis\n",
    "\n",
    "<br> \n",
    "Now that we have a dictionary of dataframes, we can analyze the data and make some observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bad1f",
   "metadata": {},
   "source": [
    "1. We can get the shape of the data for any stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4dc955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: PFE, Shape: (523, 6) \n",
      "Symbol: ROP, Shape: (523, 6) \n",
      "Symbol: XYL, Shape: (523, 6) \n",
      "Symbol: CPAY, Shape: (523, 6) \n",
      "Symbol: INCY, Shape: (523, 6) \n"
     ]
    }
   ],
   "source": [
    "# the data shape\n",
    "for symbol in dfs.keys():\n",
    "    print(f\"Symbol: {symbol}, Shape: {dfs[symbol].shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1b77a",
   "metadata": {},
   "source": [
    "2. We can get the basic stats for any stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6283b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>5.230000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.870774</td>\n",
       "      <td>36.269160</td>\n",
       "      <td>37.066481</td>\n",
       "      <td>35.423588</td>\n",
       "      <td>36.250718</td>\n",
       "      <td>1.373323e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.594988</td>\n",
       "      <td>6.862873</td>\n",
       "      <td>7.114429</td>\n",
       "      <td>6.536369</td>\n",
       "      <td>6.851178</td>\n",
       "      <td>6.265050e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.923565</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>26.170000</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>25.580000</td>\n",
       "      <td>3.922725e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.773307</td>\n",
       "      <td>31.555978</td>\n",
       "      <td>32.129982</td>\n",
       "      <td>30.858634</td>\n",
       "      <td>31.555977</td>\n",
       "      <td>9.714320e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.344492</td>\n",
       "      <td>34.478176</td>\n",
       "      <td>34.914612</td>\n",
       "      <td>33.785580</td>\n",
       "      <td>34.487667</td>\n",
       "      <td>1.215046e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.093800</td>\n",
       "      <td>40.028976</td>\n",
       "      <td>40.682581</td>\n",
       "      <td>39.165085</td>\n",
       "      <td>40.033976</td>\n",
       "      <td>1.583856e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>52.740730</td>\n",
       "      <td>59.480000</td>\n",
       "      <td>61.709999</td>\n",
       "      <td>57.160000</td>\n",
       "      <td>60.599998</td>\n",
       "      <td>6.333997e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        adj close       close        high         low        open  \\\n",
       "count  523.000000  523.000000  523.000000  523.000000  523.000000   \n",
       "mean    29.870774   36.269160   37.066481   35.423588   36.250718   \n",
       "std      7.594988    6.862873    7.114429    6.536369    6.851178   \n",
       "min     17.923565   25.400000   26.170000   25.200001   25.580000   \n",
       "25%     23.773307   31.555978   32.129982   30.858634   31.555977   \n",
       "50%     28.344492   34.478176   34.914612   33.785580   34.487667   \n",
       "75%     33.093800   40.028976   40.682581   39.165085   40.033976   \n",
       "max     52.740730   59.480000   61.709999   57.160000   60.599998   \n",
       "\n",
       "             volume  \n",
       "count  5.230000e+02  \n",
       "mean   1.373323e+08  \n",
       "std    6.265050e+07  \n",
       "min    3.922725e+07  \n",
       "25%    9.714320e+07  \n",
       "50%    1.215046e+08  \n",
       "75%    1.583856e+08  \n",
       "max    6.333997e+08  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data basic stats\n",
    "dfs[\"PFE\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813bc76",
   "metadata": {},
   "source": [
    "3. We can check how many missing values each column have for a any stock dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5582917d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adj close    0\n",
       "close        0\n",
       "high         0\n",
       "low          0\n",
       "open         0\n",
       "volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many null values in each column\n",
    "dfs['PFE'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09b19d",
   "metadata": {},
   "source": [
    "#### Columns breakdown\n",
    "\n",
    "<br>\n",
    "Date: The index is the date on which the information on the rest of the columns takes place.\n",
    "\n",
    "<br>\n",
    "Adjusted close: is the closing price after adjustments for all applicable splits and dividend distributions which represents the true closing price.\n",
    "\n",
    "<br>\n",
    "Close: is the historical closing price of the stock.\n",
    "\n",
    "<br>\n",
    "High: is the highest point a stock has reached.\n",
    "\n",
    "<br>\n",
    "Low: is the lowest point of a stock.\n",
    "\n",
    "<br>\n",
    "Open: the opening price of the stock.\n",
    "\n",
    "<br>\n",
    "Volume: the volume of stocks traded in that timeframe.\n",
    "\n",
    "<br>\n",
    "Usually for this type of model we would only keep either Adjusted close or close, we are going to keep the adjusted close for now but it worth mentiong that most of the technical indicators we are utilizing are dependent on the none adjusted close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b6fd5",
   "metadata": {},
   "source": [
    "## Adding Targets\n",
    "\n",
    "<br>\n",
    "To predict stock trends based on past data, we'll create two new columns:\n",
    "\n",
    "- 'next_close': Represents the next closing price, serving as the target for the regression model.\n",
    "\n",
    "- 'trend': Indicates whether the next close is higher '1' or lower '0' than the current close, serving as the target for the classification model.\n",
    "\n",
    "So we will train the model on to the current closing price and make it predict the next closing price/trend for any given timestep.\n",
    "\n",
    "<br>\n",
    "To do that we define the add_targets(4) function which takes a data frame as input, adds the 'next_close' and 'trend' columns to it, and returns it as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3eb3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dataframe and create 'next_close' column based on its 'close' column\n",
    "def get_next_close(_df):\n",
    "    \n",
    "    # create the 'next_close' column to be equal to the next closing price\n",
    "    # this can be accomplished easily by shifting the close column backward by 1\n",
    "    return _df['close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['next_close'] > row['close']:\n",
    "        return 1\n",
    "    elif row['next_close'] < row['close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "# create a function that add the target columns to the dataframe\n",
    "def add_targets(_df):\n",
    "    \n",
    "    # add the next_close column to the dataframe\n",
    "    _df['next_close'] = get_next_close(_df)\n",
    "    \n",
    "    # add the trend column to the dataframe\n",
    "    _df['trend'] = _df.apply(assign_trend, axis=1)\n",
    "    \n",
    "    # drop the NaN values\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    # fix the 'trend' data type to be int\n",
    "    _df = _df.astype({'trend': int})\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418911ef",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "<br>\n",
    "Adding indicators to the dataframe is important for enhancing model performance and accuracy.\n",
    "\n",
    "<br>\n",
    "We can either manually calculate technical indicators, which is time-consuming and prone to errors, or we can utilize an existing library designed for this purpose. pandas-ta is a library that includes a wide set of technical indicators and is designed to work seamlessly with pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291c48d",
   "metadata": {},
   "source": [
    "To explore the available indicators in pandas-ta, you can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0aac4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list available technical indicators\n",
    "help(dfs['PFE'].ta.indicators())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc608a8d",
   "metadata": {},
   "source": [
    "For this project, we added a total of 66 technical indicators. Each feature is carefully selected based on the technical indicator's definition and description. \n",
    "\n",
    "Check the full list of the selected indicators and the implementation of the add_technical_indicators function which take a dataframe as input and add these indicators to it (5).\n",
    "\n",
    "We can also get detailed information on specific indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec24de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the MACD indicator\n",
    "help(ta.macd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd53854",
   "metadata": {},
   "source": [
    "Then, we’ll group the features into four categories:\n",
    "1. Base Features: Original features from yfinance (6 features).\n",
    "2. Technical Indicators based on Closing Price: (30 features).\n",
    "3. Technical Indicators based on Highs and Lows: (31 features).\n",
    "4. Technical Indicators based on Volume: (5 features).\n",
    "This grouping will enable us to create more sophisticated models, such as multi-output or inception models, which we will explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afe2c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def add_technical_indicators(_df):\n",
    "    \n",
    "    ##### indicators based on the closing price ##### index range: 6:36\n",
    "    # apply macd on the close column in a df and add it to the dataframe    \n",
    "    macd = ta.macd(_df['close'])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, 'macd', macd.iloc[:,0])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(7, 'macd_histogram', macd.iloc[:,1])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(8, 'macd_signal', macd.iloc[:,2])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df['close'])\n",
    "    _df.insert(9, 'rsi', rsi)\n",
    "\n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df['close'])\n",
    "    _df.insert(10, 'sma', sma)\n",
    "\n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df['close'])\n",
    "    _df.insert(11, 'ema', ema)\n",
    "    \n",
    "    ######## repeat the same proccess for all the technical indicators we want to include ##########\n",
    "    # bbands: A popular volatility indicator by John Bollinger.\n",
    "    bbands = ta.bbands(_df['close'])\n",
    "    _df.insert(12, 'bbands_lower', bbands.iloc[:,0])\n",
    "    _df.insert(13, 'bbands_mid', bbands.iloc[:,1])\n",
    "    _df.insert(14, 'bbands_upper', bbands.iloc[:,2])\n",
    "    _df.insert(15, 'bbands_bandwidth', bbands.iloc[:,3])\n",
    "    _df.insert(16, 'bbands_percent', bbands.iloc[:,4])\n",
    "    \n",
    "    # dema: The Double Exponential Moving Average attempts to a smoother average with less lag than the normal Exponential Moving Average (EMA).\n",
    "    dema = ta.dema(_df['close'])\n",
    "    _df.insert(17, 'dema', dema)\n",
    "    \n",
    "    # tema: A less laggy Exponential Moving Average.\n",
    "    tema = ta.tema(_df['close'])\n",
    "    _df.insert(18, 'tema', tema)\n",
    "\n",
    "    # roc: Rate of Change is an indicator is also referred to as Momentum. It is a pure momentum oscillator that measures the percent change in price with the previous price 'n' (or length) periods ago.\n",
    "    roc = ta.roc(_df['close'])\n",
    "    _df.insert(19, 'roc', roc)\n",
    "    \n",
    "    # mom: Momentum is an indicator used to measure a security's speed (or strength) of movement.  Or simply the change in price.\n",
    "    mom = ta.mom(_df['close'])\n",
    "    _df.insert(20, 'mom', mom)\n",
    "    \n",
    "    # kama: Developed by Perry Kaufman, Kaufman's Adaptive Moving Average (KAMA) is a moving average designed to account for market noise or volatility. KAMA will closely follow prices when the price swings are relatively small and the noise is low. KAMA will adjust when the price swings widen and follow prices from a greater distance. This trend-following indicator can be used to identify the overall trend, time turning points and filter price movements.\n",
    "    kama = ta.kama(_df['close'])\n",
    "    _df.insert(21, 'kama', kama)\n",
    "                       \n",
    "    # trix: is a momentum oscillator to identify divergences.\n",
    "    trix = ta.trix(_df['close'])\n",
    "    _df.insert(22, 'trix', trix.iloc[:,0])\n",
    "    _df.insert(23, 'trixs', trix.iloc[:,1])\n",
    "    \n",
    "    # hma: The Hull Exponential Moving Average attempts to reduce or remove lag in moving averages.\n",
    "    hma = ta.hma(_df['close'])\n",
    "    _df.insert(24, 'hma', hma)\n",
    "    \n",
    "    # alma: The ALMA moving average uses the curve of the Normal (Gauss) distribution, which can be shifted from 0 to 1. This allows regulating the smoothness and high sensitivity of the indicator. Sigma is another parameter that is responsible for the shape of the curve coefficients. This moving average reduces lag of the data in conjunction with smoothing to reduce noise.\n",
    "    alma = ta.alma(_df['close'])\n",
    "    _df.insert(25, 'alma', alma)\n",
    "    \n",
    "    # apo: The Absolute Price Oscillator is an indicator used to measure a security's momentum.  It is simply the difference of two Exponential Moving Averages (EMA) of two different periods. Note: APO and MACD lines are equivalent.\n",
    "    apo = ta.apo(_df['close'])\n",
    "    _df.insert(26, 'apo', apo)\n",
    "    \n",
    "    # cfo: The Forecast Oscillator calculates the percentage difference between the actualprice and the Time Series Forecast (the endpoint of a linear regression line).\n",
    "    cfo = ta.cfo(_df['close'])\n",
    "    _df.insert(27, 'cfo', cfo)\n",
    "    \n",
    "    # cg: The Center of Gravity Indicator by John Ehlers attempts to identify turning points while exhibiting zero lag and smoothing.\n",
    "    cg = ta.cg(_df['close'])\n",
    "    _df.insert(28, 'cg', cg)\n",
    "    \n",
    "    # cmo: Attempts to capture the momentum of an asset with overbought at 50 and oversold at -50.\n",
    "    cmo = ta.cmo(_df['close'])\n",
    "    _df.insert(29, 'cmo', cmo)\n",
    "    \n",
    "    # coppock: Coppock Curve (originally called the \"Trendex Model\") is a momentum indicator is designed for use on a monthly time scale.  Although designed for monthly use, a daily calculation over the same period can be made, converting the periods to 294-day and 231-day rate of changes, and a 210-day weighted moving average.\n",
    "    coppock = ta.coppock(_df['close'])\n",
    "    _df.insert(30, 'coppock', coppock)\n",
    "    \n",
    "    # cti: The Correlation Trend Indicator is an oscillator created by John Ehler in 2020. It assigns a value depending on how close prices in that range are to following a positively- or negatively-sloping straight line. Values range from -1 to 1. This is a wrapper for ta.linreg(close, r=True).\n",
    "    cti = ta.cti(_df['close'])\n",
    "    _df.insert(31, 'cti', cti)\n",
    "    \n",
    "    # decay: Creates a decay moving forward from prior signals like crosses. The default is \"linear\". Exponential is optional as \"exponential\" or \"exp\".\n",
    "    decay = ta.decay(_df['close'])\n",
    "    _df.insert(32, 'decay', decay)\n",
    "    \n",
    "    # decreasing: Returns True if the series is decreasing over a period, False otherwise. If the kwarg 'strict' is True, it returns True if it is continuously decreasing over the period. When using the kwarg 'asint', then it returns 1 for True or 0 for False.\n",
    "    decreasing = ta.decreasing(_df['close'])\n",
    "    _df.insert(33, 'decreasing', decreasing)\n",
    "    \n",
    "    # ebsw: This indicator measures market cycles and uses a low pass filter to remove noise. Its output is bound signal between -1 and 1 and the maximum length of a detected trend is limited by its length input.\n",
    "    ebsw = ta.ebsw(_df['close'])\n",
    "    _df.insert(34, 'ebsw', ebsw)\n",
    "    \n",
    "    # entropy: Introduced by Claude Shannon in 1948, entropy measures the unpredictability of the data, or equivalently, of its average information. A die has higher entropy (p=1/6) versus a coin (p=1/2).\n",
    "    entropy = ta.entropy(_df['close'])\n",
    "    _df.insert(35, 'entropy', entropy)\n",
    "    \n",
    "    \n",
    "    ##### indicators based on the high and lows of the price ##### range= 36:67\n",
    "    \n",
    "    # aberration: A volatility indicator\n",
    "    aberration = ta.aberration(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(36, 'aberration_zg', aberration.iloc[:,0])\n",
    "    _df.insert(37, 'aberration_sg', aberration.iloc[:,1])\n",
    "    _df.insert(38, 'aberration_xg', aberration.iloc[:,2])\n",
    "    _df.insert(39, 'aberration_atr', aberration.iloc[:,3])\n",
    "    \n",
    "    # adx:  Average Directional Movement is meant to quantify trend strength by measuring the amount of movement in a single direction.    \n",
    "    adx = ta.adx(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(40, 'adx_adx', adx.iloc[:,0])\n",
    "    _df.insert(41, 'adx_dmp', adx.iloc[:,1])\n",
    "    _df.insert(42, 'adx_dmn', adx.iloc[:,2])\n",
    "\n",
    "    # atr: Averge True Range is used to measure volatility, especially volatility caused by gaps or limit moves.\n",
    "    atr = ta.atr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(43, 'atr', atr)\n",
    "    \n",
    "    # stoch: The Stochastic Oscillator (STOCH) was developed by George Lane in the 1950's. He believed this indicator was a good way to measure momentum because changes in momentum precede changes in price.\n",
    "    stoch = ta.stoch(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(44, 'stoch_k', stoch.iloc[:,0])\n",
    "    _df.insert(45, 'stoch_d', stoch.iloc[:,1])\n",
    "    \n",
    "    # Supertrend: is an overlap indicator. It is used to help identify trend direction, setting stop loss, identify support and resistance, and/or generate buy & sell signals.\n",
    "    supertrend = ta.supertrend(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(46, 'supertrend_trend', supertrend.iloc[:,0])\n",
    "    _df.insert(47, 'supertrend_direction', supertrend.iloc[:,1])\n",
    "    \n",
    "    # cci: Commodity Channel Index is a momentum oscillator used to primarily identify overbought and oversold levels relative to a mean.\n",
    "    cci = ta.cci(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(48, 'cci', cci)\n",
    "    \n",
    "    # aroon: attempts to identify if a security is trending and how strong.\n",
    "    aroon = ta.aroon(_df['high'], _df['low'])\n",
    "    _df.insert(49, 'aroon_up', aroon.iloc[:,0])\n",
    "    _df.insert(50, 'aroon_down', aroon.iloc[:,1])\n",
    "    _df.insert(51, 'aroon_osc', aroon.iloc[:,2])\n",
    "    \n",
    "    # natr: Normalized Average True Range attempt to normalize the average true range.\n",
    "    natr = ta.natr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(52, 'natr', natr)\n",
    "    \n",
    "    # William's Percent R is a momentum oscillator similar to the RSI that attempts to identify overbought and oversold conditions.\n",
    "    willr = ta.willr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(53, 'willr', willr)\n",
    "    \n",
    "    # vortex: Two oscillators that capture positive and negative trend movement.\n",
    "    vortex = ta.vortex(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(54, 'vortex_vip', vortex.iloc[:,0])\n",
    "    _df.insert(55, 'vortex_vim', vortex.iloc[:,1])\n",
    "    \n",
    "    # hlc3: the average of high, low, and close prices\n",
    "    hlc3 = ta.hlc3(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(56, 'hlc3', hlc3)\n",
    "    \n",
    "    # ohlc4: the average of open, high, low, and close prices\n",
    "    ohlc4 = ta.ohlc4(_df['open'], _df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(57, 'ohlc4', ohlc4)\n",
    "    \n",
    "    # accbands: Acceleration Bands created by Price Headley plots upper and lower envelope bands around a simple moving average.\n",
    "    accbands = ta.accbands(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(58, 'accbands_lower', accbands.iloc[:,0])\n",
    "    _df.insert(59, 'accbands_mid', accbands.iloc[:,1])\n",
    "    _df.insert(60, 'accbands_upper', accbands.iloc[:,2])\n",
    "\n",
    "    # chop: The Choppiness Index was created by Australian commodity trader E.W. Dreiss and is designed to determine if the market is choppy (trading sideways) or not choppy (trading within a trend in either direction). Values closer to 100 implies the underlying is choppier whereas values closer to 0 implies the underlying is trending.\n",
    "    chop = ta.chop(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(61, 'chop', chop)\n",
    "    \n",
    "    # dm: The Directional Movement was developed by J. Welles Wilder in 1978 attempts to determine which direction the price of an asset is moving. It compares prior highs and lows to yield to two series +DM and -DM.\n",
    "    dm = ta.dm(_df['high'], _df['low'])\n",
    "    _df.insert(62, 'dm_positive', dm.iloc[:,0])\n",
    "    _df.insert(63, 'dm_negative', dm.iloc[:,1])\n",
    "\n",
    "    # donchian: Donchian Channels are used to measure volatility, similar to Bollinger Bands and Keltner Channels.\n",
    "    donchian = ta.donchian(_df['high'], _df['low'])\n",
    "    _df.insert(64, 'donchian_lower', donchian.iloc[:,0])\n",
    "    _df.insert(65, 'donchian_mid', donchian.iloc[:,1])\n",
    "    _df.insert(66, 'donchian_upper', donchian.iloc[:,2])\n",
    "    \n",
    "    \n",
    "    ##### indicators based on the volume of the price ##### range= 67:72\n",
    "    \n",
    "    # obv: On Balance Volume is a cumulative indicator to measure buying and selling pressure.\n",
    "    obv = ta.obv(_df['close'], _df['volume'])\n",
    "    _df.insert(67, 'obv', obv)\n",
    "    \n",
    "    # vwma: Volume Weighted Moving Average.\n",
    "    vwma = ta.vwma(_df['close'], _df['volume'])\n",
    "    _df.insert(68, 'vwma', vwma)\n",
    "    \n",
    "    # adosc: Accumulation/Distribution Oscillator indicator utilizes Accumulation/Distribution and treats it similarily to MACD or APO.\n",
    "    adosc = ta.adosc(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(69, 'adosc', adosc)\n",
    "    \n",
    "    # cmf: Chailin Money Flow measures the amount of money flow volume over a specific period in conjunction with Accumulation/Distribution.\n",
    "    cmf = ta.cmf(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(70, 'cmf', cmf)\n",
    "    \n",
    "    # efi: Elder's Force Index measures the power behind a price movement using price and volume as well as potential reversals and price corrections.\n",
    "    efi = ta.efi(_df['close'], _df['volume'])\n",
    "    _df.insert(71, 'efi', efi)\n",
    "\n",
    "\n",
    "    #### we can add more technical indicators if we want using the same process ####\n",
    "    \n",
    "    # remove the NaN values and return the new dataframe\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279c70d",
   "metadata": {},
   "source": [
    "Finally we will create add_targets_and_indicators (6), a helper functions to add the targets and indicators to all dataframes in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88b87d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dictionary of dataframes as input and add the targets and features to them \n",
    "def add_targets_and_indicators(_dfs):\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # copy the dataframe\n",
    "        _df = _dfs[symbol].copy(deep=True)\n",
    "        \n",
    "        # add target columns to the copied dataframe\n",
    "        _df = add_targets(_df)\n",
    "        \n",
    "        # add technical indicators to the copied dataframe\n",
    "        _df = add_technical_indicators(_df)\n",
    "        \n",
    "        # replace the original dataframe with the new dataframe\n",
    "        _dfs[symbol] = _df\n",
    "    \n",
    "    # return the new dataframes dictionary\n",
    "    return _dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0cab6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the targets and technical indicators to each dataframe in the dictionary\n",
    "full_dfs = add_targets_and_indicators(dfs.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f34bef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479, 74)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the new dataframes\n",
    "full_dfs['PFE'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c35f5",
   "metadata": {},
   "source": [
    "## Data balance and common sense baseline\n",
    "\n",
    "<br>\n",
    "With the target column added, we can now assess the balance of our dataset from a classification perspective. An unbalanced dataset may skew the model's predictions. The trend column indicates whether the stock will rise or fall. By calculating the ratio of (trend = 1) to the total number of samples, we can evaluate the data balance.\n",
    "\n",
    "To acomplish that, we'll create calculate_data_balance (6), a function that computes the ratio of trend = 1 for each individual dataframe in our dictionary and the overall ratio across all dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62c40636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to computes the ratio of trend = 1 for each individual dataframe in a dictionary \n",
    "def calculate_data_balance(_dfs):\n",
    "    \n",
    "    # store the total ratio of trend 1 of all the dataframes\n",
    "    total = 0\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # get the number of values where trend = 1\n",
    "        trend_1 = _dfs[symbol]['trend'].value_counts()[1]\n",
    "        \n",
    "        # get the total number of rows in the dataframe\n",
    "        row_num = _dfs[symbol].shape[0]\n",
    "        \n",
    "        # percentage of 'trend up' to the whole column\n",
    "        trend_1_ratio = trend_1/row_num\n",
    "        \n",
    "        # print the ratio to the screen\n",
    "        print(f\"The Trend up ratio of {symbol} is: {trend_1_ratio}\")\n",
    "        \n",
    "        # add the ratio to total\n",
    "        total += trend_1_ratio\n",
    "        \n",
    "    # get the average trend up ratio\n",
    "    average = total / len(_dfs.keys())\n",
    "    \n",
    "    # print the average ratio\n",
    "    print(f\"The Average Trend up ratio is: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99c6cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trend up ratio of PFE is: 0.5177453027139874\n",
      "The Trend up ratio of ROP is: 0.578838174273859\n",
      "The Trend up ratio of XYL is: 0.5912863070539419\n",
      "The Trend up ratio of CPAY is: 0.556935817805383\n",
      "The Trend up ratio of INCY is: 0.463768115942029\n",
      "The Average Trend up ratio is: 0.5417147435578401\n"
     ]
    }
   ],
   "source": [
    "# get the trend up ratio of all the dataframes in the dictionary to get a sense of how balanced the data is\n",
    "calculate_data_balance(full_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69dbef",
   "metadata": {},
   "source": [
    "Based on these results, the data appears well-balanced.\n",
    "\n",
    "#### Common Sense Baseline\n",
    "\n",
    "Establishing a common sense baseline is crucial in machine learning to ensure that our model performs at least as well as basic logical assumptions. In stock prediction, a reasonable baseline assumes that the future trend will mirror the current trend; if a stock is rising, it’s expected to continue rising, and vice versa. This baseline is realistic and aligns with principles from behavioral finance[12], however, it's outside the scope of this project.\n",
    "\n",
    "To acheive that we will create calculate_common_sense_baseline (7), a function that calculates this common sense score for each stock in the dictionary, as well as the average overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d9cf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_common_sense_baseline(_dfs):\n",
    "    \n",
    "    # store the total common_sense_score for all the dataframes\n",
    "    total = 0\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # since the common sense will be to assume the trend next is going to be the same as the trend now, we will shift the trend\n",
    "        # forward by one, this will give us a column that matches the common sense assumption we set\n",
    "        common_sense = _dfs[symbol]['trend'].shift(1)\n",
    "\n",
    "        # measure the average of when the common sense (naive) prediction matches the actual 'trend'\n",
    "        common_sense_score = (common_sense == _dfs[symbol]['trend']).mean()\n",
    "        \n",
    "        # print the score to the screen\n",
    "        print(f\"The common sense score of {symbol} is: {common_sense_score}\")\n",
    "        \n",
    "        # add the score to the total\n",
    "        total += common_sense_score\n",
    "       \n",
    "    \n",
    "    # get the average score\n",
    "    average = total / len(_dfs.keys())\n",
    "    \n",
    "    # print the average score\n",
    "    print(f\"The Average common sense score is: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d924e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common sense score of PFE is: 0.534446764091858\n",
      "The common sense score of ROP is: 0.520746887966805\n",
      "The common sense score of XYL is: 0.5124481327800829\n",
      "The common sense score of CPAY is: 0.505175983436853\n",
      "The common sense score of INCY is: 0.494824016563147\n",
      "The Average common sense score is: 0.5135283569677492\n"
     ]
    }
   ],
   "source": [
    "# get the common sense baseline for each dataframe in the dictionary\n",
    "calculate_common_sense_baseline(full_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa03dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
