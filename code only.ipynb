{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9239e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Project prototype (implementation)\n",
    "\n",
    "## Install Dependencies and import libraries\n",
    "\n",
    "# pip install pandas numpy yfinance pandas-ta scikit-learn tensorflow\n",
    "\n",
    "# https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "# import yfinance, our data source\n",
    "import yfinance as yf\n",
    "\n",
    "# https://pypi.org/project/pandas-ta/ (\"\"\"An easy to use Python 3 Pandas Extension with 130+ Technical Analysis Indicators. Can be called from a Pandas DataFrame or standalone\"\"\")\n",
    "# import pandas-ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import from tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Input, GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# insert the stock symbols into a list\n",
    "symbols_list = ['PFE', 'ROP', 'XYL', 'CPAY', 'INCY']\n",
    "\n",
    "# we will take the weekly data for the last 10 years\n",
    "# data_weekly = yf.download(symbols_list, period='10y', interval='1wk')\n",
    "\n",
    "## Format the data and save it as a CSV file\n",
    "\n",
    "# source of inspiration: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html[10]\n",
    "# Return a reshaped DataFrame having a multi-level inde\n",
    "# stacked_data_weekly = data_weekly.stack()\n",
    "# stacked_data_weekly\n",
    "\n",
    "# Save the data to a CSV so we don't have to make any extra unnecessary requests to the API every time we reload the notebook\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "# stacked_data_weekly.to_csv('stacked_data_weekly_1.csv', index=True)\n",
    "\n",
    "# load the the dataframe from the csv file\n",
    "df = pd.read_csv('stacked_data_weekly_1.csv').set_index([\"Date\", \"Ticker\"])\n",
    "\n",
    "# df.head(5)\n",
    "\n",
    "## Perform simple exploritory data analysis\n",
    "\n",
    "# how many null values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# the data shape\n",
    "df.shape\n",
    "\n",
    "# data basic stats\n",
    "df.describe()\n",
    "\n",
    "## Devide the data into five dataframes, one for each stock\n",
    "\n",
    "# source of inspiration https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html [11]\n",
    "# select specific stock data at the 'Ticker' level of this multi index dataframe\n",
    "df1 = df.xs('PFE', axis=0, level='Ticker', drop_level=True)\n",
    "df2 = df.xs('ROP', axis=0, level='Ticker', drop_level=True)\n",
    "df3 = df.xs('XYL', axis=0, level='Ticker', drop_level=True)\n",
    "df4 = df.xs('CPAY', axis=0, level='Ticker', drop_level=True)\n",
    "df5 = df.xs('INCY', axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "# disply the first dataframe\n",
    "df1.head(5)\n",
    "\n",
    "# show the new df shape\n",
    "df1.shape\n",
    "\n",
    "## Create the target of the model\n",
    "\n",
    "# copy the dateframe before modification so we don't get a warning from jupyter notebook\n",
    "df1 = df1.copy()\n",
    "\n",
    "# create the 'Next' column to be equal to the next closing price\n",
    "# this can be accomplished easily by shifting the close column backward by 1\n",
    "df1[\"Next\"] = df1['Close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['Next'] > row['Close']:\n",
    "        return 1\n",
    "    elif row['Next'] < row['Close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# create the 'Trend' column to be equal to the output of the 'assign_trend' function    \n",
    "df1['Trend'] = df1.apply(assign_trend, axis=1)\n",
    "\n",
    "# check out the results\n",
    "df1.head(5)\n",
    "\n",
    "# Check if the data is balanced\n",
    "\n",
    "# let's check the occurance of each value in the Trend column\n",
    "df1['Trend'].value_counts()\n",
    "\n",
    "df1['Trend'].value_counts()[1]\n",
    "\n",
    "# percentage of 'trend up' to the whole column\n",
    "df1['Trend'].value_counts()[1]/df1.shape[0]\n",
    "\n",
    "## Create a common sense baseline\n",
    "\n",
    "# this can be accomplished easily by shifting the close column forward by 1\n",
    "common_sense = df1['Trend'].shift(1)\n",
    "\n",
    "# measure the average of when the common sense (naive) prediction will match the actual 'Trend'\n",
    "(common_sense == df1['Trend']).mean()\n",
    "\n",
    "## Include the technical indicators\n",
    "\n",
    "#  we can easily check the available indicators in the pandas-ta library\n",
    "# help(df1.ta.indicators())\n",
    "\n",
    "#  we can also learn about any specific indicator like this\n",
    "# help(ta.macd)\n",
    "\n",
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def assign_TIs(_df):\n",
    "    # apply macd on the Close column in a df and add it to the dataframe    \n",
    "    mcda = ta.macd(_df[\"Close\"])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, \"MACD\", mcda[\"MACD_12_26_9\"])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(7, \"Signal\", mcda[\"MACD_12_26_9\"])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(8, \"Histogram\", mcda[\"MACD_12_26_9\"])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df[\"Close\"])\n",
    "    _df.insert(9, \"RSI\", rsi)\n",
    "    \n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df[\"Close\"])\n",
    "    _df.insert(10, \"SMA\", sma)\n",
    "    \n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df[\"Close\"])\n",
    "    _df.insert(11, \"EMA\", ema)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "# apply the function to the dataframe\n",
    "df1 = assign_TIs(df1)\n",
    "\n",
    "# drop the NaN values\n",
    "df1.dropna(inplace=True)\n",
    "\n",
    "# fix the 'Trend' data type to be int\n",
    "df1 = df1.astype({'Trend': int})\n",
    "\n",
    "# check the dataframe\n",
    "df1.head(5)\n",
    "\n",
    "# the shape of the data now\n",
    "df1.shape\n",
    "\n",
    "##  Prepare the data for training\n",
    "\n",
    "# reset the index\n",
    "df1.reset_index(inplace = True)\n",
    "\n",
    "# drop the Date column as it's not necessary for now\n",
    "df1.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "# df1.head(5)\n",
    "\n",
    "Create the features list, for now we will use every column except the last two.\n",
    "\n",
    "# The features list\n",
    "X1 = df1.iloc[:, :-2]\n",
    "\n",
    "X1.head(2)\n",
    "\n",
    "# Create the target, which is the 'Trend' column for now.\n",
    "\n",
    "# The Target (Trend for now)\n",
    "y1 = df1.iloc[:, -1]\n",
    "\n",
    "# initialize a MinMaxScaler instance for a range between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# pass the features to the scaler\n",
    "scaled_X1 = scaler.fit_transform(X1)\n",
    "\n",
    "# scaled_X1\n",
    "\n",
    "# source of isnpiration: https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4 [13]\n",
    "# create a function to reshape X and y into sequences of x timesteps\n",
    "def create_seqs(features, target, num_rows):\n",
    "    # create 2 empty lists to store the newly shaped features and target lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate over the features\n",
    "    for i in range(len(features) - num_rows):\n",
    "        # create indexes of the start and end of each sequence\n",
    "        seq_s = i\n",
    "        seq_e = i + num_rows\n",
    "        \n",
    "        # the ith sequence will be a slice of the features between the indexes, create it and add it to X\n",
    "        xi = features[seq_s : seq_e]\n",
    "        X.append(xi)\n",
    "        \n",
    "        # do the same for the target and add it to y\n",
    "        yi = target[seq_e]\n",
    "        y.append(yi)\n",
    "    \n",
    "    # return the X and y as numpy arraies\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "timesteps = 6\n",
    "X_seq1, y_seq1 = create_seqs(scaled_X1, y1, timesteps)\n",
    "\n",
    "# check the new shapes for the features and labels sets\n",
    "X_seq1.shape, y_seq1.shape\n",
    "\n",
    "# source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical [14]\n",
    "# use to_categorical from tf to converts the target (Trend) to binary class matrix\n",
    "y_seq1 = to_categorical(y_seq1)\n",
    "\n",
    "# Devide the data into a training set and a test set in 70-30 ratio\n",
    "\n",
    "#  sets the training test ratio to be 70-30\n",
    "training_ratio = int(len(X_seq1) * 0.7)\n",
    "\n",
    "# # split the data into training and test\n",
    "X1_train, X1_test = X_seq1[:training_ratio], X_seq1[training_ratio:]\n",
    "y1_train, y1_test = y_seq1[:training_ratio], y_seq1[training_ratio:]\n",
    "\n",
    "X1_train.shape, X1_test.shape\n",
    "\n",
    "## Create and train the baseline classification model\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(SimpleRNN(64, input_shape=(timesteps, X1_train.shape[2]), return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_model()\n",
    "\n",
    "# get the model weights before training\n",
    "# model1.get_weights()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "## Model evaluation and prototype conclusion\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# get the model summary\n",
    "# model1.summary()\n",
    "\n",
    "# get the model compile configurations\n",
    "# model1.get_compile_config()\n",
    "\n",
    "# get the model configurations after training\n",
    "# model1.get_config()\n",
    "\n",
    "# get the model weights after training\n",
    "# model1.get_weights()\n",
    "\n",
    "# Get a prediction from the model given the last 6 weeks. This is to simulate how a user would get a prediction from the model. The input will be the last entry in the test set.\n",
    "\n",
    "# reshape the input so it have the shape (1, 6, 12) which what the model expect as input\n",
    "_input = X1_test[-1].reshape(1, X1_test.shape[1], X1_test.shape[2])\n",
    "pred = model1.predict(_input)\n",
    "print(f\"Trend: {np.argmax(pred)}, \",f\"Confidence: {np.max(pred)}\")\n",
    "\n",
    "### LSTM\n",
    "\n",
    "# construct LSTM the model\n",
    "def create_LSTM_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    # input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # first dense layer\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    \n",
    "    # second dense layer\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_LSTM_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### GRU\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_GRU_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    # input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # first dense layer\n",
    "    model.add(GRU(64, \n",
    "                  dropout=0.1, \n",
    "                  recurrent_dropout=0.5, \n",
    "                  return_sequences=True))\n",
    "    \n",
    "    # second dense layer\n",
    "    model.add(GRU(64,\n",
    "                  dropout=0.1, \n",
    "                  recurrent_dropout=0.5))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_GRU_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Now le'ts create a regression version of all the 3 models we have so far.\n",
    "# First let's adjust the target column of the model, for refression the column we are trying to predict is the Next colum.\n",
    "\n",
    "# The Target (Next for now)\n",
    "y1_reg = df1.iloc[:, -2]\n",
    "\n",
    "# Create sequences\n",
    "timesteps = 6\n",
    "X_seq1_reg, y_seq1_reg = create_seqs(scaled_X1, y1_reg, timesteps)\n",
    "\n",
    "# split the data into training and test 70-30 ratio\n",
    "X1_train_reg, X1_test_reg = X_seq1_reg[:training_ratio], X_seq1_reg[training_ratio:]\n",
    "y1_train_reg, y1_test_reg = y_seq1_reg[:training_ratio], y_seq1_reg[training_ratio:]\n",
    "\n",
    "### SimpleRNN regression model\n",
    "\n",
    "# construct the model\n",
    "def create_SimpleRNN_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(SimpleRNN(64, return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_SimpleRNN_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### LSTM regression model\n",
    "\n",
    "# construct the model\n",
    "def create_LSTM_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(LSTM(64, return_sequences=True))    \n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_LSTM_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=300, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### GRU regression model\n",
    "\n",
    "# construct the model\n",
    "def create_GRU_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n",
    "    model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_GRU_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the average value for the Next closing price to judge weather the mae is acceptable or not \n",
    "df1['Next'].mean()\n",
    "\n",
    "## Hyperparameters optimization\n",
    "\n",
    "# !pip install scikeras\n",
    "\n",
    "# helper function to measure how long a process would take\n",
    "from datetime import datetime\n",
    "\n",
    "def get_time():\n",
    "    return datetime.now()\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import warnings\n",
    "\n",
    "\n",
    "# get the time before starting the process\n",
    "start = get_time()\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/72392579/scikeras-randomizedsearchcv-for-best-hyper-parameters [19]\n",
    "# construct the model\n",
    "def create_model(n_hidden = 1, n_neurons = 30, learning_rate=3e-3):\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create an input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # add a static first deep layer\n",
    "    model.add(SimpleRNN(n_neurons, return_sequences=True))\n",
    "\n",
    "    # add the other model deep layers dynamically\n",
    "    for layer in range(1, n_hidden):\n",
    "        model.add(SimpleRNN(n_neurons))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # create an Adam optimizer with a variable learning rate\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# define possible parameters (this is just a test for now and we will add more parameters for the final report)\n",
    "# we need to distingiush between the model building function input and the RandomizedSearchCV input, to do that we prefix the model input with model__\n",
    "# source: https://adriangb.com/scikeras/stable/notebooks/Basic_Usage.html (7.1 Special prefixes) [20]\n",
    "params = {\n",
    "    \"model__n_hidden\": [0, 1, 2, 3, 4, 5],\n",
    "    \"model__n_neurons\": [int(x) for x in np.arange(1, 128)],\n",
    "    \"model__learning_rate\": [1e-2, 1e-3, 1e-4], \n",
    "    'epochs': [10, 20, 30],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a keras classification model wrappers from the scikeras library which allow us to utilize the hyperparameter tunning functions from the scikit-learn library\n",
    "kerasWarp = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_simpleRnn_clas = RandomizedSearchCV(estimator=kerasWarp, param_distributions=params, n_iter=10, cv=None, random_state=101)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/40105796/turn-warning-off-in-a-cell-jupyter-notebook [21]\n",
    "# prevent FitFailedWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    # fit the RandomizedSearchCV models\n",
    "    random_simpleRnn_clas.fit(X1_train, y1_train, validation_split=0.2)\n",
    "\n",
    "# print results\n",
    "print(f\"RandomizedSearchCV best parameters: {random_simpleRnn_clas.best_params_}\")\n",
    "print(f\"RandomizedSearchCV best score: {random_simpleRnn_clas.best_score_}\")\n",
    "\n",
    "\n",
    "# get the time after finishing the process\n",
    "end = get_time()\n",
    "\n",
    "# print the duration\n",
    "print(f\"process finished in: {end - start}\")\n",
    "\n",
    "# retrieve the best model and refit on the training data to get the history\n",
    "best_model = random_simpleRnn_clas.best_estimator_.model_\n",
    "\n",
    "# evaluate the best model on the test data\n",
    "best_model.evaluate(X1_test, y1_test, verbose=1)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = best_model.predict(X1_test)\n",
    "\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# get a summary of the best model\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63e3db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"Next\"] = df1['Close'].shift(-1)\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Trend'] = (df1['Next'] > df1['Close']).astype(int)\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.dropna(inplace=True)\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['MACD'], _df['Signal'], _df['Histogram'] = mcda['MACD_12_26_9'], mcda['MACD_12_26_9'], mcda['MACD_12_26_9']\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['MACD'], _df['Signal'], _df['Histogram'] = mcda['MACD_12_26_9'], mcda['MACD_12_26_9'], mcda['MACD_12_26_9']\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['MACD'], _df['Signal'], _df['Histogram'] = mcda['MACD_12_26_9'], mcda['MACD_12_26_9'], mcda['MACD_12_26_9']\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['RSI'] = ta.rsi(_df[\"Close\"])\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['SMA'] = ta.sma(_df[\"Close\"])\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['EMA'] = ta.ema(_df[\"Close\"])\n",
      "C:\\Users\\ammaroAsus\\AppData\\Local\\Temp\\ipykernel_15576\\2434677341.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.drop(['Date'], axis=1, inplace=True)\n",
      "C:\\Users\\ammaroAsus\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 14 and 13 for '{{node sequential_1_1/simple_rnn_2_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1_1/simple_rnn_2_1/strided_slice_2, sequential_1_1/simple_rnn_2_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [?,14], [13,64].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(None, 14), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 64), dtype=float32)',)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 131\u001b[0m\n\u001b[0;32m    128\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(window\u001b[38;5;241m.\u001b[39mtrain, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39mwindow\u001b[38;5;241m.\u001b[39mval, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    134\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(window\u001b[38;5;241m.\u001b[39mtest, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 14 and 13 for '{{node sequential_1_1/simple_rnn_2_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1_1/simple_rnn_2_1/strided_slice_2, sequential_1_1/simple_rnn_2_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [?,14], [13,64].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(None, 14), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 64), dtype=float32)',)\n  • training=True"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas_ta as ta\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_csv('stacked_data_weekly_1.csv').set_index([\"Date\", \"Ticker\"])\n",
    "\n",
    "# Select specific stock data\n",
    "df1 = df.xs('PFE', axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "# Create the 'Next' column for the next closing price\n",
    "df1[\"Next\"] = df1['Close'].shift(-1)\n",
    "\n",
    "# Create the 'Trend' column\n",
    "df1['Trend'] = (df1['Next'] > df1['Close']).astype(int)\n",
    "\n",
    "# Drop NaN values\n",
    "df1.dropna(inplace=True)\n",
    "\n",
    "# Include technical indicators\n",
    "def assign_TIs(_df):\n",
    "    mcda = ta.macd(_df[\"Close\"])\n",
    "    _df['MACD'], _df['Signal'], _df['Histogram'] = mcda['MACD_12_26_9'], mcda['MACD_12_26_9'], mcda['MACD_12_26_9']\n",
    "    _df['RSI'] = ta.rsi(_df[\"Close\"])\n",
    "    _df['SMA'] = ta.sma(_df[\"Close\"])\n",
    "    _df['EMA'] = ta.ema(_df[\"Close\"])\n",
    "    return _df\n",
    "\n",
    "df1 = assign_TIs(df1)\n",
    "\n",
    "# Prepare the data for training\n",
    "df1.reset_index(inplace=True)\n",
    "df1.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X1 = df1.drop(columns=['Trend'])\n",
    "y1 = df1['Trend']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_X1 = scaler.fit_transform(X1)\n",
    "\n",
    "# Combine scaled features and target into one DataFrame for the WindowGenerator\n",
    "df_combined = pd.DataFrame(scaled_X1, columns=X1.columns)\n",
    "df_combined['Trend'] = y1.values\n",
    "\n",
    "# Define WindowGenerator class\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, label_width, shift, df, label_columns=None):\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "        self.df = df\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.df[:int(len(self.df) * 0.7)])\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.df[int(len(self.df) * 0.7):int(len(self.df) * 0.9)])\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.df[int(len(self.df) * 0.9):])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Total window size: {self.total_window_size}, Input indices: {self.input_indices}, Label indices: {self.label_indices}'\n",
    "\n",
    "# Separate features and target columns in df_combined\n",
    "label_columns = ['Trend']\n",
    "df_features = df_combined.drop(columns=label_columns)\n",
    "df\n",
    "\n",
    "# Instantiate WindowGenerator\n",
    "window = WindowGenerator(input_width=6, label_width=1, shift=1, df=df_combined, label_columns=['Trend'])\n",
    "\n",
    "# Define and compile the model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        SimpleRNN(64, input_shape=(6, X1.shape[1]), return_sequences=True),\n",
    "        SimpleRNN(64, return_sequences=False),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(window.train, epochs=50, validation_data=window.val, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(window.test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9647e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
