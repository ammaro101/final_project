{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9239e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Project prototype (implementation)\n",
    "\n",
    "## Install Dependencies and import libraries\n",
    "\n",
    "# pip install pandas numpy yfinance pandas-ta scikit-learn tensorflow\n",
    "\n",
    "# https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "# import yfinance, our data source\n",
    "import yfinance as yf\n",
    "\n",
    "# https://pypi.org/project/pandas-ta/ (\"\"\"An easy to use Python 3 Pandas Extension with 130+ Technical Analysis Indicators. Can be called from a Pandas DataFrame or standalone\"\"\")\n",
    "# import pandas-ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import from tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Input, GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# insert the stock symbols into a list\n",
    "symbols_list = ['PFE', 'ROP', 'XYL', 'CPAY', 'INCY']\n",
    "\n",
    "# we will take the weekly data for the last 10 years\n",
    "# data_weekly = yf.download(symbols_list, period='10y', interval='1wk')\n",
    "\n",
    "## Format the data and save it as a CSV file\n",
    "\n",
    "# source of inspiration: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html[10]\n",
    "# Return a reshaped DataFrame having a multi-level inde\n",
    "# stacked_data_weekly = data_weekly.stack()\n",
    "# stacked_data_weekly\n",
    "\n",
    "# Save the data to a CSV so we don't have to make any extra unnecessary requests to the API every time we reload the notebook\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "# stacked_data_weekly.to_csv('stacked_data_weekly_1.csv', index=True)\n",
    "\n",
    "# load the the dataframe from the csv file\n",
    "df = pd.read_csv('stacked_data_weekly_1.csv').set_index([\"Date\", \"Ticker\"])\n",
    "\n",
    "# df.head(5)\n",
    "\n",
    "## Perform simple exploritory data analysis\n",
    "\n",
    "# how many null values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# the data shape\n",
    "df.shape\n",
    "\n",
    "# data basic stats\n",
    "df.describe()\n",
    "\n",
    "## Devide the data into five dataframes, one for each stock\n",
    "\n",
    "# source of inspiration https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html [11]\n",
    "# select specific stock data at the 'Ticker' level of this multi index dataframe\n",
    "df1 = df.xs('PFE', axis=0, level='Ticker', drop_level=True)\n",
    "df2 = df.xs('ROP', axis=0, level='Ticker', drop_level=True)\n",
    "df3 = df.xs('XYL', axis=0, level='Ticker', drop_level=True)\n",
    "df4 = df.xs('CPAY', axis=0, level='Ticker', drop_level=True)\n",
    "df5 = df.xs('INCY', axis=0, level='Ticker', drop_level=True)\n",
    "\n",
    "# disply the first dataframe\n",
    "df1.head(5)\n",
    "\n",
    "# show the new df shape\n",
    "df1.shape\n",
    "\n",
    "## Create the target of the model\n",
    "\n",
    "# copy the dateframe before modification so we don't get a warning from jupyter notebook\n",
    "df1 = df1.copy()\n",
    "\n",
    "# create the 'Next' column to be equal to the next closing price\n",
    "# this can be accomplished easily by shifting the close column backward by 1\n",
    "df1[\"Next\"] = df1['Close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['Next'] > row['Close']:\n",
    "        return 1\n",
    "    elif row['Next'] < row['Close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# create the 'Trend' column to be equal to the output of the 'assign_trend' function    \n",
    "df1['Trend'] = df1.apply(assign_trend, axis=1)\n",
    "\n",
    "# check out the results\n",
    "df1.head(5)\n",
    "\n",
    "# Check if the data is balanced\n",
    "\n",
    "# let's check the occurance of each value in the Trend column\n",
    "df1['Trend'].value_counts()\n",
    "\n",
    "df1['Trend'].value_counts()[1]\n",
    "\n",
    "# percentage of 'trend up' to the whole column\n",
    "df1['Trend'].value_counts()[1]/df1.shape[0]\n",
    "\n",
    "## Create a common sense baseline\n",
    "\n",
    "# this can be accomplished easily by shifting the close column forward by 1\n",
    "common_sense = df1['Trend'].shift(1)\n",
    "\n",
    "# measure the average of when the common sense (naive) prediction will match the actual 'Trend'\n",
    "(common_sense == df1['Trend']).mean()\n",
    "\n",
    "## Include the technical indicators\n",
    "\n",
    "#  we can easily check the available indicators in the pandas-ta library\n",
    "# help(df1.ta.indicators())\n",
    "\n",
    "#  we can also learn about any specific indicator like this\n",
    "# help(ta.macd)\n",
    "\n",
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def assign_TIs(_df):\n",
    "    # apply macd on the Close column in a df and add it to the dataframe    \n",
    "    mcda = ta.macd(_df[\"Close\"])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, \"MACD\", mcda[\"MACD_12_26_9\"])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(7, \"Signal\", mcda[\"MACD_12_26_9\"])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(8, \"Histogram\", mcda[\"MACD_12_26_9\"])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df[\"Close\"])\n",
    "    _df.insert(9, \"RSI\", rsi)\n",
    "    \n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df[\"Close\"])\n",
    "    _df.insert(10, \"SMA\", sma)\n",
    "    \n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df[\"Close\"])\n",
    "    _df.insert(11, \"EMA\", ema)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "# apply the function to the dataframe\n",
    "df1 = assign_TIs(df1)\n",
    "\n",
    "# drop the NaN values\n",
    "df1.dropna(inplace=True)\n",
    "\n",
    "# fix the 'Trend' data type to be int\n",
    "df1 = df1.astype({'Trend': int})\n",
    "\n",
    "# check the dataframe\n",
    "df1.head(5)\n",
    "\n",
    "# the shape of the data now\n",
    "df1.shape\n",
    "\n",
    "##  Prepare the data for training\n",
    "\n",
    "# reset the index\n",
    "df1.reset_index(inplace = True)\n",
    "\n",
    "# drop the Date column as it's not necessary for now\n",
    "df1.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "# df1.head(5)\n",
    "\n",
    "Create the features list, for now we will use every column except the last two.\n",
    "\n",
    "# The features list\n",
    "X1 = df1.iloc[:, :-2]\n",
    "\n",
    "X1.head(2)\n",
    "\n",
    "# Create the target, which is the 'Trend' column for now.\n",
    "\n",
    "# The Target (Trend for now)\n",
    "y1 = df1.iloc[:, -1]\n",
    "\n",
    "# initialize a MinMaxScaler instance for a range between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# pass the features to the scaler\n",
    "scaled_X1 = scaler.fit_transform(X1)\n",
    "\n",
    "# scaled_X1\n",
    "\n",
    "# source of isnpiration: https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4 [13]\n",
    "# create a function to reshape X and y into sequences of x timesteps\n",
    "def create_seqs(features, target, num_rows):\n",
    "    # create 2 empty lists to store the newly shaped features and target lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate over the features\n",
    "    for i in range(len(features) - num_rows):\n",
    "        # create indexes of the start and end of each sequence\n",
    "        seq_s = i\n",
    "        seq_e = i + num_rows\n",
    "        \n",
    "        # the ith sequence will be a slice of the features between the indexes, create it and add it to X\n",
    "        xi = features[seq_s : seq_e]\n",
    "        X.append(xi)\n",
    "        \n",
    "        # do the same for the target and add it to y\n",
    "        yi = target[seq_e]\n",
    "        y.append(yi)\n",
    "    \n",
    "    # return the X and y as numpy arraies\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "timesteps = 6\n",
    "X_seq1, y_seq1 = create_seqs(scaled_X1, y1, timesteps)\n",
    "\n",
    "# check the new shapes for the features and labels sets\n",
    "X_seq1.shape, y_seq1.shape\n",
    "\n",
    "# source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical [14]\n",
    "# use to_categorical from tf to converts the target (Trend) to binary class matrix\n",
    "y_seq1 = to_categorical(y_seq1)\n",
    "\n",
    "# Devide the data into a training set and a test set in 70-30 ratio\n",
    "\n",
    "#  sets the training test ratio to be 70-30\n",
    "training_ratio = int(len(X_seq1) * 0.7)\n",
    "\n",
    "# # split the data into training and test\n",
    "X1_train, X1_test = X_seq1[:training_ratio], X_seq1[training_ratio:]\n",
    "y1_train, y1_test = y_seq1[:training_ratio], y_seq1[training_ratio:]\n",
    "\n",
    "X1_train.shape, X1_test.shape\n",
    "\n",
    "## Create and train the baseline classification model\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(SimpleRNN(64, input_shape=(timesteps, X1_train.shape[2]), return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_model()\n",
    "\n",
    "# get the model weights before training\n",
    "# model1.get_weights()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "## Model evaluation and prototype conclusion\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# get the model summary\n",
    "# model1.summary()\n",
    "\n",
    "# get the model compile configurations\n",
    "# model1.get_compile_config()\n",
    "\n",
    "# get the model configurations after training\n",
    "# model1.get_config()\n",
    "\n",
    "# get the model weights after training\n",
    "# model1.get_weights()\n",
    "\n",
    "# Get a prediction from the model given the last 6 weeks. This is to simulate how a user would get a prediction from the model. The input will be the last entry in the test set.\n",
    "\n",
    "# reshape the input so it have the shape (1, 6, 12) which what the model expect as input\n",
    "_input = X1_test[-1].reshape(1, X1_test.shape[1], X1_test.shape[2])\n",
    "pred = model1.predict(_input)\n",
    "print(f\"Trend: {np.argmax(pred)}, \",f\"Confidence: {np.max(pred)}\")\n",
    "\n",
    "### LSTM\n",
    "\n",
    "# construct LSTM the model\n",
    "def create_LSTM_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    # input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # first dense layer\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    \n",
    "    # second dense layer\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_LSTM_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### GRU\n",
    "\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python” chapter 6 [8]\n",
    "# construct the model\n",
    "def create_GRU_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    # input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # first dense layer\n",
    "    model.add(GRU(64, \n",
    "                  dropout=0.1, \n",
    "                  recurrent_dropout=0.5, \n",
    "                  return_sequences=True))\n",
    "    \n",
    "    # second dense layer\n",
    "    model.add(GRU(64,\n",
    "                  dropout=0.1, \n",
    "                  recurrent_dropout=0.5))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_GRU_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train, y1_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test, y1_test, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la [15]\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# source of inspiration: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html [16]\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Now le'ts create a regression version of all the 3 models we have so far.\n",
    "# First let's adjust the target column of the model, for refression the column we are trying to predict is the Next colum.\n",
    "\n",
    "# The Target (Next for now)\n",
    "y1_reg = df1.iloc[:, -2]\n",
    "\n",
    "# Create sequences\n",
    "timesteps = 6\n",
    "X_seq1_reg, y_seq1_reg = create_seqs(scaled_X1, y1_reg, timesteps)\n",
    "\n",
    "# split the data into training and test 70-30 ratio\n",
    "X1_train_reg, X1_test_reg = X_seq1_reg[:training_ratio], X_seq1_reg[training_ratio:]\n",
    "y1_train_reg, y1_test_reg = y_seq1_reg[:training_ratio], y_seq1_reg[training_ratio:]\n",
    "\n",
    "### SimpleRNN regression model\n",
    "\n",
    "# construct the model\n",
    "def create_SimpleRNN_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(SimpleRNN(64, return_sequences=True))\n",
    "    model.add(SimpleRNN(64, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_SimpleRNN_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### LSTM regression model\n",
    "\n",
    "# construct the model\n",
    "def create_LSTM_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(LSTM(64, return_sequences=True))    \n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_LSTM_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=300, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### GRU regression model\n",
    "\n",
    "# construct the model\n",
    "def create_GRU_Reg_model():\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the model layers\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n",
    "    model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# initialize the model\n",
    "model1 = create_GRU_Reg_model()\n",
    "\n",
    "# train the model\n",
    "history = model1.fit(X1_train_reg, y1_train_reg, validation_split=0.2, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# test the model accuracy\n",
    "model1.evaluate(X1_test_reg, y1_test_reg, verbose=2)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = model1.predict(X1_test_reg)\n",
    "\n",
    "# get the R2 of the model\n",
    "r2 = r2_score(y1_test_reg, y1_pred)\n",
    "print(f\"R2 score is: {r2}\")\n",
    "\n",
    "# source of the code snippet[17]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the average value for the Next closing price to judge weather the mae is acceptable or not \n",
    "df1['Next'].mean()\n",
    "\n",
    "## Hyperparameters optimization\n",
    "\n",
    "# !pip install scikeras\n",
    "\n",
    "# helper function to measure how long a process would take\n",
    "from datetime import datetime\n",
    "\n",
    "def get_time():\n",
    "    return datetime.now()\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import warnings\n",
    "\n",
    "\n",
    "# get the time before starting the process\n",
    "start = get_time()\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/72392579/scikeras-randomizedsearchcv-for-best-hyper-parameters [19]\n",
    "# construct the model\n",
    "def create_model(n_hidden = 1, n_neurons = 30, learning_rate=3e-3):\n",
    "    # initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create an input layer\n",
    "    model.add(Input(shape=(timesteps, X1_train.shape[2])))\n",
    "    \n",
    "    # add a static first deep layer\n",
    "    model.add(SimpleRNN(n_neurons, return_sequences=True))\n",
    "\n",
    "    # add the other model deep layers dynamically\n",
    "    for layer in range(1, n_hidden):\n",
    "        model.add(SimpleRNN(n_neurons))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # create an Adam optimizer with a variable learning rate\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# define possible parameters (this is just a test for now and we will add more parameters for the final report)\n",
    "# we need to distingiush between the model building function input and the RandomizedSearchCV input, to do that we prefix the model input with model__\n",
    "# source: https://adriangb.com/scikeras/stable/notebooks/Basic_Usage.html (7.1 Special prefixes) [20]\n",
    "params = {\n",
    "    \"model__n_hidden\": [0, 1, 2, 3, 4, 5],\n",
    "    \"model__n_neurons\": [int(x) for x in np.arange(1, 128)],\n",
    "    \"model__learning_rate\": [1e-2, 1e-3, 1e-4], \n",
    "    'epochs': [10, 20, 30],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a keras classification model wrappers from the scikeras library which allow us to utilize the hyperparameter tunning functions from the scikit-learn library\n",
    "kerasWarp = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_simpleRnn_clas = RandomizedSearchCV(estimator=kerasWarp, param_distributions=params, n_iter=10, cv=None, random_state=101)\n",
    "\n",
    "# source of inspiration: https://stackoverflow.com/questions/40105796/turn-warning-off-in-a-cell-jupyter-notebook [21]\n",
    "# prevent FitFailedWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    # fit the RandomizedSearchCV models\n",
    "    random_simpleRnn_clas.fit(X1_train, y1_train, validation_split=0.2)\n",
    "\n",
    "# print results\n",
    "print(f\"RandomizedSearchCV best parameters: {random_simpleRnn_clas.best_params_}\")\n",
    "print(f\"RandomizedSearchCV best score: {random_simpleRnn_clas.best_score_}\")\n",
    "\n",
    "\n",
    "# get the time after finishing the process\n",
    "end = get_time()\n",
    "\n",
    "# print the duration\n",
    "print(f\"process finished in: {end - start}\")\n",
    "\n",
    "# retrieve the best model and refit on the training data to get the history\n",
    "best_model = random_simpleRnn_clas.best_estimator_.model_\n",
    "\n",
    "# evaluate the best model on the test data\n",
    "best_model.evaluate(X1_test, y1_test, verbose=1)\n",
    "\n",
    "# get predictions from the model given the test set\n",
    "y1_pred = best_model.predict(X1_test)\n",
    "\n",
    "# convert the predictions and test set to be in the shape of a vector of labels\n",
    "y1_pred_labels = np.argmax(y1_pred, axis=1)\n",
    "y1_test_labels = np.argmax(y1_test, axis=1)\n",
    "\n",
    "# get precision, recall, and fscore\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y1_test_labels, y1_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y1_test_labels, y1_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(conf_mat)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# get a summary of the best model\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9647e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
